{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9730805,"sourceType":"datasetVersion","datasetId":5954892},{"sourceId":9730833,"sourceType":"datasetVersion","datasetId":5954911}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Using Gemini for Personalized Higher-Level Education Learning\n\nFor this Kaggle Challenge to highlight the capabilities of LLMs with very long context windows such as the Gemini models, I have created a learning assistant for Stanford CS229 Course. Education courses contain alot of information that students have to spend time going back through to identify the relevant content based on the topics they want to review. With Gemini's ability to handle vast amounts of information, students can ask questions spanning across multiple mediums — lecture videos, regular slides, annotated slides, midterm reviews, and the course textbook—leveraging Gemini's multimodal capabilities.\n\n**Education courses often overwhelm students with a large volume of content, requiring significant effort to locate and review relevant material. This solution eliminates that bottleneck, making it easier for students to interact with their course material in a meaningful and efficient way.**\n\n-----------------\n\n## **Use of Long Context Window**\nGemini's ability to handle 1 million tokens allows it to synthesize information across the entire semester, eliminating the need for fragmented queries and enabling cohesive responses\n\n-----------------\n\n## **Key Features & Value Proposition**:\n1) **Personalized Assistance**: Students can ask detailed questions about specific concepts, relationships between topics, or exam preparation strategies and receive tailored explanations.\n\n2) **Integrated Course Resources**: Gemini bridges insights across lectures, notes, slides, and textbooks, providing unified answers that save time and improve comprehension.\n\n3) **Flexible and Targeted Learning Support**: Students can catch up on missed lectures, compare annotated slides to regular ones, or focus on critical areas for exams.\nTopic-specific caches can be created to provide focused study guides for struggling students or for areas requiring additional review.\n\n---------------------\n\n## **The Impact of Gemini**\nBy providing access to an entire semester’s worth of material in a single cache, **Gemini can draw connections across different concepts, creating more meaningful and comprehensive answers**. This not only enhances the learning experience but also **demonstrates the cost-effectiveness of caching—students can reuse shared caches, reducing token costs while maintaining high-quality assistance**.\n\n---------------------\n\n## **Broad Applicability**\nWhile this notebook focuses on Stanford's CS229 course, this approach could be extended across K-12 education and other higher education disciplines — English, history, math, etc. As a supplemental learning assistant, Gemini offers an innovative solution for personalized, context-rich education support.\n\n------------------","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport google.generativeai as genai\nfrom dotenv import load_dotenv\nfrom google.generativeai import caching\nimport datetime\nimport pandas as pd\nimport os\nfrom io import BytesIO","metadata":{"execution":{"iopub.status.busy":"2024-11-29T18:18:06.711309Z","iopub.execute_input":"2024-11-29T18:18:06.711732Z","iopub.status.idle":"2024-11-29T18:18:06.720250Z","shell.execute_reply.started":"2024-11-29T18:18:06.711684Z","shell.execute_reply":"2024-11-29T18:18:06.719154Z"},"trusted":true},"outputs":[],"execution_count":91},{"cell_type":"markdown","source":"## Dataset Description\n\nFor this competition, I found the 2022 Stanford CS229 - Machine Learning course syllabus which had the link to all of the lecture slides, textbook and course lecture on Youtube. \n\n\n* A) Lecture Transcript: Using the Youtube API - I extracted the transcript of each lecture video and saved each as a text file: /kaggle/input/cs229-transcripts. \n* B) Lecture Notes: I downloaded all of the lectures notes (annotated and unannotated) & course textbook, which are publically avaiable, and saved the pdfs: /kaggle/input/cs299-notes\n* C) Midterm Review - link from syllabus\n* D) Course Textbook - link from syllabus\n\n\nReferences for sources:\n1. Youtube playlist: https://www.youtube.com/playlist?list=PLoROMvodv4rNyWOpJg_Yh4NSqI4Z4vOYy\n2. Course syallbus: https://docs.google.com/spreadsheets/d/18pHRegyB0XawIdbZbvkr8-jMfi_2ltHVYPjBEOim-6w/edit?pli=1&gid=0#gid=0","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ngoogle_api = user_secrets.get_secret(\"GOOGLE_API_KEY\")","metadata":{"execution":{"iopub.status.busy":"2024-11-29T18:18:09.509856Z","iopub.execute_input":"2024-11-29T18:18:09.510741Z","iopub.status.idle":"2024-11-29T18:18:10.384004Z","shell.execute_reply.started":"2024-11-29T18:18:09.510703Z","shell.execute_reply":"2024-11-29T18:18:10.383136Z"},"trusted":true},"outputs":[],"execution_count":92},{"cell_type":"code","source":"genai.configure(api_key=google_api)","metadata":{"execution":{"iopub.status.busy":"2024-11-29T18:18:10.385721Z","iopub.execute_input":"2024-11-29T18:18:10.386344Z","iopub.status.idle":"2024-11-29T18:18:10.391437Z","shell.execute_reply.started":"2024-11-29T18:18:10.386289Z","shell.execute_reply":"2024-11-29T18:18:10.390404Z"},"trusted":true},"outputs":[],"execution_count":93},{"cell_type":"code","source":"file_1 = genai.upload_file(path='/kaggle/input/cs229-transcripts/lecture_1_transcript.txt')\nfile_2 = genai.upload_file(path='/kaggle/input/cs229-transcripts/lecture_2_transcript.txt')\nfile_3 = genai.upload_file(path='/kaggle/input/cs229-transcripts/lecture_3_transcript.txt')\nfile_4 = genai.upload_file(path='/kaggle/input/cs229-transcripts/lecture_4_transcript.txt')\nfile_5 = genai.upload_file(path='/kaggle/input/cs229-transcripts/lecture_5_transcript.txt')\nfile_6 = genai.upload_file(path='/kaggle/input/cs229-transcripts/lecture_6_transcript.txt')\nfile_7 = genai.upload_file(path='/kaggle/input/cs229-transcripts/lecture_7_transcript.txt')\nfile_8 = genai.upload_file(path='/kaggle/input/cs229-transcripts/lecture_8_transcript.txt')\nfile_9 = genai.upload_file(path='/kaggle/input/cs299-notes/eval_slides.pdf')\nfile_10 = genai.upload_file(path='/kaggle/input/cs229-transcripts/lecture_9_transcript.txt')\nfile_11 = genai.upload_file(path='/kaggle/input/cs229-transcripts/lecture_10_transcript.txt')\nfile_12 = genai.upload_file(path='/kaggle/input/cs299-notes/bias_annotated.pdf')\nfile_13 = genai.upload_file(path='/kaggle/input/cs299-notes/ridge_annotated.pdf')\nfile_14 = genai.upload_file(path='/kaggle/input/cs299-notes/lasso_annotated.pdf')\nfile_15 = genai.upload_file(path='/kaggle/input/cs299-notes/midterm_review.pdf')\nfile_16 = genai.upload_file(path='/kaggle/input/cs299-notes/boosting.pdf')\nfile_17 = genai.upload_file(path='/kaggle/input/cs299-notes/decisiontrees_annotated.pdf')\nfile_18 = genai.upload_file(path='/kaggle/input/cs229-transcripts/lecture_12_transcript.txt')\nfile_19 = genai.upload_file(path='/kaggle/input/cs229-transcripts/lecture_13_transcript.txt')\nfile_20 = genai.upload_file(path='/kaggle/input/cs299-notes/em_annotated.pdf')\nfile_21 = genai.upload_file(path='/kaggle/input/cs299-notes/pca_annotated.pdf')\nfile_22 = genai.upload_file(path='/kaggle/input/cs229-transcripts/lecture_14_transcript.txt')\nfile_23 = genai.upload_file(path='/kaggle/input/cs229-transcripts/lecture_15_transcript.txt')\nfile_24 = genai.upload_file(path='/kaggle/input/cs229-transcripts/lecture_16_transcript.txt')\nfile_25 = genai.upload_file(path='/kaggle/input/cs299-notes/learning.pdf')\nfile_26 = genai.upload_file(path='/kaggle/input/cs229-transcripts/lecture_17_transcript.txt')\nfile_27 = genai.upload_file(path='/kaggle/input/cs229-transcripts/lecture_18_transcript.txt')\nfile_28 = genai.upload_file(path='/kaggle/input/cs229-transcripts/lecture_19_transcript.txt')\nfile_29 = genai.upload_file(path='/kaggle/input/cs299-notes/textbook.pdf')\nfile_30 = genai.upload_file(path='/kaggle/input/cs299-notes/fairness_annotated.pdf')\nfile_31 = genai.upload_file(path='/kaggle/input/cs299-notes/explainability_annotated.pdf')","metadata":{"execution":{"iopub.status.busy":"2024-11-29T18:18:11.611617Z","iopub.execute_input":"2024-11-29T18:18:11.612841Z","iopub.status.idle":"2024-11-29T18:19:10.283402Z","shell.execute_reply.started":"2024-11-29T18:18:11.612783Z","shell.execute_reply":"2024-11-29T18:19:10.282253Z"},"trusted":true},"outputs":[],"execution_count":94},{"cell_type":"code","source":"system_prompt = \"\"\"\nYou are an expert tutor specializing in machine learning, with comprehensive knowledge of the Stanford CS229 \"Introduction to Machine Learning\" course. You have access to all relevant materials, including:\n- Lecture cture notes for each session.\n- Transcripts of all recorded lectures.\n- The complete course textbook.\nYour role is to guide the user through the CS229 course material by:\n1. **Providing clear, detailed explanations** of key machine learning concepts and algorithms, from foundational topics like linear regression and classification to advanced areas such as support vector machines and unsupervised learning.\n2. **Connecting course concepts**, explaining how different topics (e.g., gradient descent, regularization) relate and build upon each other across lectures.\n3. **Summarizing lectures and sections**, highlighting major takeaways, essential equations, and conceptual insights.\n4. **Supporting exam preparation**, identifying high-impact topics, common pitfalls, and suggesting areas for further review.\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:19:10.285171Z","iopub.execute_input":"2024-11-29T18:19:10.285545Z","iopub.status.idle":"2024-11-29T18:19:10.290483Z","shell.execute_reply.started":"2024-11-29T18:19:10.285503Z","shell.execute_reply":"2024-11-29T18:19:10.289409Z"}},"outputs":[],"execution_count":95},{"cell_type":"code","source":"def generate_gemini_response(cache, question):\n    model = genai.GenerativeModel.from_cached_content(cached_content=cache)\n    response = model.generate_content(question)\n    return response.text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:19:10.292023Z","iopub.execute_input":"2024-11-29T18:19:10.292361Z","iopub.status.idle":"2024-11-29T18:19:10.308146Z","shell.execute_reply.started":"2024-11-29T18:19:10.292330Z","shell.execute_reply":"2024-11-29T18:19:10.307061Z"}},"outputs":[],"execution_count":96},{"cell_type":"markdown","source":"**NOTE**: The content across all course material exceeds the limit of 1 million so I split the material into first half and second half of the course","metadata":{}},{"cell_type":"code","source":"first_half_cache = caching.CachedContent.create(\n    model='models/gemini-1.5-flash-001',\n    display_name='first half of CS229 content',\n    system_instruction=(\n    system_prompt),\n    contents=[file_1, file_2, file_3, file_4, file_5, file_6, file_7, file_8, file_9, file_10, file_11, file_12, file_13, file_14, file_15, file_29],\n    ttl=datetime.timedelta(minutes=15)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:19:10.309872Z","iopub.execute_input":"2024-11-29T18:19:10.310236Z","iopub.status.idle":"2024-11-29T18:19:36.908130Z","shell.execute_reply.started":"2024-11-29T18:19:10.310178Z","shell.execute_reply":"2024-11-29T18:19:36.907187Z"}},"outputs":[],"execution_count":97},{"cell_type":"code","source":"second_half_cache = caching.CachedContent.create(\n    model='models/gemini-1.5-flash-001',\n    display_name='second half of CS229 content',\n    system_instruction=(\n    system_prompt),\n    contents=[file_16, file_17, file_18, file_19, file_20, file_21, file_22, file_23, file_24, file_25, file_26, file_27, file_28, file_30, file_31, file_29],\n    ttl=datetime.timedelta(minutes=15)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:52:33.443000Z","iopub.execute_input":"2024-11-29T17:52:33.443316Z","iopub.status.idle":"2024-11-29T17:53:10.070129Z","shell.execute_reply.started":"2024-11-29T17:52:33.443286Z","shell.execute_reply":"2024-11-29T17:53:10.069151Z"}},"outputs":[],"execution_count":79},{"cell_type":"markdown","source":"# 1. Lecture-Specific Queries\n\nStudents usually have lecture-specific questions to reinforce understanding of key topics or catch up on missed lectures.\n\nPurpose: Retrieve explanations, examples, and elaborations on concepts covered in a particular lecture.","metadata":{}},{"cell_type":"code","source":"response_1 = generate_gemini_response(second_half_cache, 'What are some key concepts covered in the KMeans lecture that are not covered in the notes? Be very specific in the points you generate.')\nprint(response_1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T02:04:11.919956Z","iopub.execute_input":"2024-11-29T02:04:11.920326Z","iopub.status.idle":"2024-11-29T02:05:33.116287Z","shell.execute_reply.started":"2024-11-29T02:04:11.920292Z","shell.execute_reply":"2024-11-29T02:05:33.115158Z"}},"outputs":[{"name":"stdout","text":"The KMeans lecture contains several concepts and explanations not explicitly detailed in the provided notes. Here are some key points:\n\n1. **The Squishiness of Unsupervised Learning:** The lecture emphasizes the inherent ambiguity and lack of a clearly defined \"right answer\" in unsupervised learning compared to supervised learning.  The notes primarily focus on the algorithm itself, but the lecture highlights the need for stronger assumptions and weaker guarantees in unsupervised settings.  The professor explicitly states that this used to be more disturbing but is now a common characteristic of much of modern AI.\n\n2. **Initialization Strategies (K-means++):** The lecture discusses the importance of initialization in KMeans and introduces the K-means++ algorithm. The notes mention K-means++ as a method to improve the approximation ratio but don't detail its mechanism. The lecture explains that K-means++ uses a density estimation to strategically place initial centroids, reducing the likelihood of getting stuck in poor local minima.  It highlights its adoption as the default in scikit-learn.\n\n3. **Choosing the Number of Clusters (K):**  The lecture extensively discusses the difficulty of selecting the optimal number of clusters (K). While the notes mention it's a modeling question, the lecture delves deeper into the trade-offs involved: choosing too few clusters results in information loss, while choosing too many leads to clusters that are no longer human-interpretable or meaningful.  The lecture stresses that K selection is a modeling decision, often relying on domain knowledge and not a purely algorithmic solution.\n\n4. **Practical Applications and Data Inspection:** The lecture highlights various practical applications of KMeans, such as finding hidden stratifications in data (e.g., identifying misclassified groups in supervised learning results) and exploring data to discover meaningful groupings.  The notes lack this discussion of practical use-cases.  The lecture stresses the role of data inspection and visualization to understand the results of the algorithm and evaluate its appropriateness to the specific data.\n\n5. **KMeans as Gradient Descent:** The lecture explains how the KMeans algorithm can be viewed as a type of gradient descent on a specific distance function, providing a mathematical framework for understanding its convergence. This connection isn't explicitly made in the provided notes.\n\nThese are specific points mentioned in the lecture audio and transcript that are absent or only briefly mentioned in the accompanying CS229 lecture notes.  They provide a more nuanced and practically oriented understanding of the KMeans algorithm.\n\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"response_2 = generate_gemini_response(first_half_cache, \"I didn't fully understand the concept of Laplace Smoothing discussed in Lecture 6. Provide more context on this topic, pull information from the textbook to supplement any explainations provided\")\nprint(response_2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T02:12:56.434971Z","iopub.execute_input":"2024-11-29T02:12:56.435351Z","iopub.status.idle":"2024-11-29T02:13:34.978682Z","shell.execute_reply.started":"2024-11-29T02:12:56.435316Z","shell.execute_reply":"2024-11-29T02:13:34.977428Z"}},"outputs":[{"name":"stdout","text":"Let's delve deeper into Laplace smoothing.  The concept arises in the context of naive Bayes, specifically when dealing with discrete features, as encountered in the spam classification example in Lecture 6. The core issue is that if a particular word (e.g., \"neurips\") never appears in your training set for a specific class (e.g., spam emails), then the maximum likelihood estimate for the probability of that word appearing in that class will be zero.  This leads to a problem because if you then encounter this word in a test email, the probability of the email belonging to that class becomes zero, regardless of other features.  Laplace smoothing offers a solution to this problem.\n\n\n**The Problem with Maximum Likelihood Estimates**\n\nThe naive Bayes classifier estimates probabilities based on observed frequencies in the training data.  This approach is known as *maximum likelihood estimation*.  The probability of feature *x<sub>j</sub>* given class *y* is estimated as:\n\nP(x<sub>j</sub> | y) =  (Number of times feature *x<sub>j</sub>* appears in class *y*) / (Total number of examples in class *y*)\n\n\nIf a feature never appears in a class, its estimated probability is zero. This can lead to *zero probability problems*. As demonstrated in the lecture, if a word never appears in spam emails in your training data, the model would incorrectly assign zero probability to any email containing that word being classified as spam.\n\n\n**Laplace Smoothing: Adding a Prior**\n\nLaplace smoothing introduces a *prior* probability to avoid these zero probability issues. It modifies the maximum likelihood estimate by adding a pseudocount to both the numerator and the denominator. For a word *x<sub>j</sub>* in class *y*, the formula becomes:\n\nP(x<sub>j</sub> | y) = (Number of times feature *x<sub>j</sub>* appears in class *y* + 1) / (Total number of examples in class *y* + |V|)\n\n\nWhere |V| is the size of the vocabulary (the total number of unique features/words).  Notice we add '1' to the numerator (the count of the feature) and |V| to the denominator (the total count for the class). This modification ensures that the estimated probability of any feature is never exactly zero.\n\nThe effect of Laplace smoothing is most noticeable when the training data is scarce. With a large amount of training data, the addition of 1 to the numerator and |V| to the denominator will have a minor impact on the estimated probability.  However, when dealing with limited data, the smoothing effect helps mitigate the influence of zero counts and leads to a more robust estimate that generalizes better to unseen data.\n\n**Textbook Connection**\n\nWhile the lecture notes provide a clear explanation, the CS229 textbook doesn't explicitly present the Laplace Smoothing formula in the exact same way as the lecture. However, the underlying concept is explained within the context of *maximum likelihood estimation* and the limitations of relying solely on observed frequencies.  The text highlights the issues of assigning zero probabilities, leading to unreliable predictions.  Laplace smoothing is presented as a way to address this limitation using prior knowledge (that is, assuming that unseen features will have a non-zero probability of occurring).  Therefore, while the formula isn't directly written out in the text in the same mathematical notation as the lecture notes, the core idea and motivation behind the method are consistent. The textbook focuses on the broader concepts of probability estimation, emphasizing the limitations of purely data-driven approaches and the importance of incorporating prior knowledge for better generalization.\n\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"## 2) Comparing Lecture Notes to Textbook\nStudents can identify additional insights shared in the lecture that are not explicitly documented in the textbook.","metadata":{}},{"cell_type":"code","source":"response_3 = gemini_response(first_half_cache, 'What is an example of something mentioned in the Neural Networks lecture that was not included in the textbook? Provide 2-3 specific examples')\nprint(response_3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T02:13:34.980357Z","iopub.execute_input":"2024-11-29T02:13:34.980700Z","iopub.status.idle":"2024-11-29T02:14:13.999856Z","shell.execute_reply.started":"2024-11-29T02:13:34.980665Z","shell.execute_reply":"2024-11-29T02:14:13.998691Z"}},"outputs":[{"name":"stdout","text":"The CS229 lecture notes and textbook cover similar ground regarding neural networks, but the lectures often include more up-to-date information, practical advice, and discussion of recent trends not present in the textbook.  Here are a few examples of topics from the Neural Networks lecture (Lecture 8) that are not explicitly detailed in the Andrew Ng CS229 textbook:\n\n1. **DALL-E 2 and similar large language models:** The lecture heavily emphasizes the impressive capabilities of large language models like DALL-E 2 for image generation and GPT-3 for text generation. While the textbook touches upon the general idea of neural networks and their applications, it doesn't specifically mention or detail these recent advancements in generative AI.  The lecture uses these examples to ground the discussion in the current state of the art, emphasizing the real-world impact of these models.\n\n2. **Discussion of different variants of SGD (Stochastic Gradient Descent):**  The lecture delves into the practical nuances of implementing SGD, discussing mini-batch gradient descent and comparing and contrasting it to standard SGD and batch gradient descent.  It highlights the importance of mini-batching for GPU parallelization, a critical aspect of modern deep learning that isn't extensively elaborated on in the textbook.  The lecture also touches on using SGD with and without replacement, which although conceptually implied in the textbook, is not explicitly examined.\n\n3. **The connection between Neural Networks and Kernel Methods:** The lecture draws a parallel between the final layer of a neural network (after the non-linear activations) and the kernel methods covered in Lecture 7.  This insightful comparison highlights how a neural network implicitly learns its own features (via the learned feature map represented in the hidden layers), contrasting it with the explicit feature selection and kernel definitions used in kernel methods. The textbook does not make this explicit connection.\n\nThe textbook provides a strong foundation, but the lectures provide a more contemporary perspective, incorporating developments and trends in the field since the textbook's publication.\n\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"response_4 = generate_gemini_response(second_half_cache, 'I am looking to further reinforce my understanding of decision trees. What are additional details covered about the different types of decision trees mentioned in the textbook that was not covered in the lecture?')\nprint(response_4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:53:30.790001Z","iopub.execute_input":"2024-11-29T17:53:30.791172Z","iopub.status.idle":"2024-11-29T17:55:28.397561Z","shell.execute_reply.started":"2024-11-29T17:53:30.791128Z","shell.execute_reply":"2024-11-29T17:55:28.396428Z"}},"outputs":[{"name":"stdout","text":"The lecture focused on the core concepts of decision trees and their implementation using a greedy approach. It demonstrated the process of selecting features to split on by calculating the classification error for each split. However, the textbook delves deeper into different aspects and variants of decision trees, including:\n\n* **ID3, C4.5, and CART:** The textbook discusses these popular algorithms in more detail, highlighting their differences in split criteria and handling of continuous attributes.  For instance, C4.5 uses the gain ratio to address the bias of ID3 towards features with many values, while CART employs the Gini index as a split criterion.\n\n* **Handling of Missing Values:** The textbook explores strategies for dealing with missing values in decision tree construction. This involves techniques like surrogate splits, where alternative features are used for splitting when the primary feature has missing values, and weighting based on the availability of data.\n\n* **Pruning Decision Trees:** Overfitting is a significant concern with decision trees, especially as they grow deeper. The textbook introduces pruning techniques like pre-pruning, which stops the tree growth based on certain criteria, and post-pruning, which trims the tree after construction by evaluating its performance on a separate validation set. This helps prevent overfitting and improve generalization.\n\n* **Multi-way Splits:** While the lecture focused on binary splits, the textbook mentions that decision trees can handle multi-way splits where a feature is divided into more than two categories. This can be beneficial for features with a large number of discrete values, but it increases the complexity of the tree structure.\n\nBy exploring these additional details, you can gain a more comprehensive understanding of how decision trees work and the various considerations involved in their construction and optimization. \n\n","output_type":"stream"}],"execution_count":80},{"cell_type":"markdown","source":"## 3) PowerPoint Slide Differences\nStudents can compare annotated slides to original slides to understand key takeaways and expanded explanations.","metadata":{}},{"cell_type":"code","source":"response_5 = generate_gemini_response(second_half_cache, 'What are the differences between the PCA original slides and the annotated content? Be specific in the type of differences you generate, do not include generic points like the annotated slides have more information')\nprint(response_5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:55:28.399106Z","iopub.execute_input":"2024-11-29T17:55:28.399522Z","iopub.status.idle":"2024-11-29T17:56:05.853479Z","shell.execute_reply.started":"2024-11-29T17:55:28.399489Z","shell.execute_reply":"2024-11-29T17:56:05.852410Z"}},"outputs":[{"name":"stdout","text":"The annotated content in the slides has more information and commentary on several key aspects of PCA, compared to the original slides. Specifically:\n\n**1.  Greater Emphasis on the Geometric Intuition**\n\n* **Projection:** The annotated slides emphasize the geometric interpretation of PCA, explicitly showing how a point in a high-dimensional space is projected onto the subspace defined by the principal components. This is done through diagrams and explanations of how the dot product is used to find the closest point to the subspace. \n* **Variance Maximization:** The annotated slides provide a more visual understanding of how PCA maximizes the variance of the projected data. This is illustrated through diagrams comparing different directions (vectors) and how they capture the variance of the data points.\n\n**2.  More Explicit Connections to Linear Algebra**\n\n* **Eigenvectors:** The annotated content clarifies the relationship between PCA, covariance matrices, and eigenvectors. It explains how minimizing reconstruction error is equivalent to finding the eigenvectors of the covariance matrix with the smallest eigenvalues. This makes the connection to linear algebra more concrete.\n* **Orthogonality:**  The annotated slides emphasize the importance of orthogonality in PCA, explicitly stating that the principal components are orthogonal unit vectors. This reinforces the idea that PCA finds a new, orthogonal basis for the data.\n\n**3.  More Contextualized Examples**\n\n* **Car Example:** The annotated slides use a more relatable example involving cars and their attributes (highway miles per gallon and city miles per gallon) to help visualize the concept of PCA. This makes the concept more tangible and easier to grasp.\n* **Hybrid/SUV Example:** The annotated slides add a layer to the car example by introducing the concept of hybrids and SUVs, further highlighting how PCA can identify clusters and explain variations within the data.\n\n**4.  Clarification of Mathematical Details**\n\n* **Covariance Matrix:** The annotated content provides a more detailed explanation of the covariance matrix and how it relates to PCA. It explicitly states that the covariance matrix is symmetric and positive semi-definite, which is crucial for understanding the eigen decomposition.\n* **Eigenvalue Decomposition:** The annotated slides emphasize the importance of the eigenvalue decomposition in PCA, clarifying that it is used to find the principal components. It also explains how the eigenvalues are ordered and their significance in understanding the variance captured by each component.\n\n**5.  Addressing Common Misconceptions**\n\n* **Initialization:** The annotated content explicitly addresses the question of how PCA is initialized, pointing out that the initial guess doesn't have a significant impact on the final result. \n* **Choice of K:** The annotated slides provide a more practical approach to choosing the number of principal components (K). It emphasizes that the goal is to select the smallest K that captures a significant amount of the variance in the data. It also discusses the potential issues of overfitting and how to avoid them.\n\nThese additions help to provide a more comprehensive and insightful understanding of PCA, making the concepts easier to grasp and connect with real-world applications. \n\n","output_type":"stream"}],"execution_count":81},{"cell_type":"markdown","source":"## 4) Study Guide and Practice Question Generation\nStudents can generate custom study questions and answers for exam preparation based on the cached content.","metadata":{}},{"cell_type":"code","source":"response_6a = generate_gemini_response(first_half_cache, 'Generate 3-5 questions using the concepts covered across lectures. Ask questions that require students to think critically about different concepts and that would be beneficial for them to review prior to the final exam. Provide the question followed by the answer.')\nprint(response_6a)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:20:32.108531Z","iopub.execute_input":"2024-11-29T18:20:32.108905Z","iopub.status.idle":"2024-11-29T18:21:04.449779Z","shell.execute_reply.started":"2024-11-29T18:20:32.108871Z","shell.execute_reply":"2024-11-29T18:21:04.448766Z"}},"outputs":[{"name":"stdout","text":"Here are a few questions that require students to think critically about different concepts and that would be beneficial for them to review prior to the final exam.\n\n**Question 1:** When would you prefer to use a generative learning algorithm over a discriminative learning algorithm, and what are the potential drawbacks of each approach?\n\n**Answer:** \n\nYou would prefer a generative learning algorithm over a discriminative learning algorithm when you have strong prior knowledge about the distribution of your data, especially if this knowledge can be captured in a relatively simple probabilistic model. Generative algorithms, such as Gaussian Discriminant Analysis (GDA) and Naive Bayes, model the joint distribution of both the inputs (X) and the labels (Y), which allows them to leverage this prior knowledge to make more accurate predictions. \n\nHowever, the drawback is that if your assumptions about the data are incorrect, then a generative model can perform worse than a discriminative model. Discriminative models, such as logistic regression, make fewer assumptions about the data and are therefore more robust to incorrect modeling assumptions.\n\n**Question 2:** Explain the concept of the bias-variance tradeoff. What are the key factors that contribute to high bias and high variance, and how can we mitigate these problems?\n\n**Answer:** The bias-variance tradeoff is a fundamental concept in machine learning that describes the relationship between the complexity of a model and its ability to generalize to unseen data. \n\n* High bias occurs when the model is too simple and cannot capture the underlying patterns in the data, leading to underfitting. This can be mitigated by increasing the model complexity, such as using a higher-order polynomial for regression or adding more hidden layers to a neural network.\n* High variance occurs when the model is too complex and overfits the training data, leading to poor generalization. This can be mitigated by reducing model complexity, such as using a simpler model or applying regularization techniques such as L2 regularization or dropout.\n\n**Question 3:** Explain the concept of the kernel trick, and how it allows us to efficiently learn non-linear models. Why is the kernel trick less popular in modern machine learning?\n\n**Answer:** \n\nThe kernel trick is a technique that allows us to implicitly learn non-linear models by transforming our data into a higher-dimensional feature space, where a linear model can then be learned. This is done by defining a kernel function that measures the similarity between data points in the original space, without explicitly computing the feature mapping into the higher-dimensional space. \n\nWhile the kernel trick can be very effective for certain types of problems, it has several drawbacks that have limited its popularity in modern machine learning. First, the kernel trick requires us to design a kernel function that accurately captures the underlying relationships in the data. This can be difficult, and there is no universal kernel function that works well for all problems. Second, the kernel trick can be computationally expensive, especially when the number of data points is large. \n\nModern machine learning techniques, such as deep learning, have largely overcome these limitations by automatically learning complex non-linear feature representations from the data itself, which has been more efficient and effective for a wide range of problems.\n\n**Question 4:** Describe how backpropagation works to compute the gradient of the loss function for a neural network. Explain the importance of using vectorization in implementing backpropagation.\n\n**Answer:** \n\nBackpropagation is a recursive algorithm used to calculate the gradient of the loss function with respect to the parameters of a neural network. It works by traversing the network in reverse order, starting from the output layer and moving backward through the hidden layers to the input layer. The key idea is to use the chain rule of calculus to calculate the derivative of the loss function with respect to each weight and bias in the network, using the derivatives of the activation functions at each layer. \n\nVectorization is critical for implementing backpropagation efficiently because it allows us to represent and compute gradients using matrix operations. Instead of iterating over individual weights and biases, we can use matrix multiplication to compute the gradients for entire layers of the network, which significantly speeds up the training process, making deep learning practical for real-world applications.\n","output_type":"stream"}],"execution_count":98},{"cell_type":"code","source":"response_6b = generate_gemini_response(second_half_cache, 'Generate 3-5 questions using the concepts covered across lectures. Ask questions that require students to think critically about different concepts and that would be beneficial for them to review prior to the final exam. Provide the question followed by the answer.')\nprint(response_6b)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:04:28.290846Z","iopub.execute_input":"2024-11-29T18:04:28.291269Z","iopub.status.idle":"2024-11-29T18:06:29.525613Z","shell.execute_reply.started":"2024-11-29T18:04:28.291194Z","shell.execute_reply":"2024-11-29T18:06:29.524533Z"}},"outputs":[{"name":"stdout","text":"Here are some questions that would be beneficial for students to review prior to the final exam. \n\n**Question 1:** Describe the difference between a supervised learning problem and an unsupervised learning problem. How do the goals differ? Give an example of each.\n\n**Answer 1:**  Supervised learning involves having labeled data, where we know the target variable (y) for each input (x). The goal is to learn a model that can accurately predict y from x. \nUnsupervised learning, on the other hand, involves data without labels. The goal is to uncover hidden structure or patterns within the data, such as finding clusters, identifying key dimensions of variation, or learning representations of the data. \n\n**Question 2:** How does the concept of latent variables play a role in the Expectation Maximization (EM) algorithm? Give an example of a latent variable and explain how it is used within the EM algorithm.\n\n**Answer 2:** Latent variables represent unobserved or hidden factors that influence the observed data in unsupervised learning.  They are typically estimated during the E-step of the EM algorithm, where we calculate the probability of each data point belonging to a specific cluster (or source) based on the current model parameters. In the M-step, those estimated probabilities are then used to update the model parameters (like cluster centers, variances, and mixing weights) to improve the model's fit to the data.\n\n**Question 3:** Describe the concept of the bias-variance tradeoff. How does it relate to the model complexity? What are some methods for mitigating the bias-variance tradeoff? \n\n**Answer 3:** The bias-variance tradeoff is a fundamental concept in machine learning that describes the balance between two sources of error in a model: \n- **Bias:** The error due to the model's inability to capture the true underlying relationship between inputs and outputs. Simple models tend to have higher bias. \n- **Variance:** The error due to the model's sensitivity to the specific training data. Complex models tend to have higher variance. \nThe goal is to find a model that minimizes the total error, which is a combination of bias and variance. \nMethods for mitigating the bias-variance tradeoff include:\n- **Regularization:**  Adding a penalty term to the loss function that discourages overly complex models.\n- **Model selection:** Choosing a model complexity that balances bias and variance.\n- **Cross-validation:** Evaluating the model's performance on unseen data to estimate its generalization error.\n\n**Question 4:** Explain the difference between PCA and ICA. When would you choose one over the other?\n\n**Answer 4:** Both PCA and ICA are dimensionality reduction techniques, but they differ in their underlying assumptions and goals:\n\n- **PCA (Principal Component Analysis):** PCA seeks to find the directions (principal components) that capture the maximum variance in the data. It assumes that the data has a linear structure and focuses on preserving as much information as possible while reducing the dimensionality.  \n- **ICA (Independent Component Analysis):** ICA aims to recover the independent sources that contribute to the observed data. It assumes that the data is generated by a linear combination of independent sources, and seeks to unmix those sources. \n\nChoose PCA when you want to reduce the dimensionality while preserving as much variance in the data as possible. Use ICA when you want to identify the independent components that generated the data. \n\n**Question 5:** Briefly describe the concept of self-supervised learning and explain the idea behind it. Give an example of how self-supervised learning is used in a real-world application. \n\n**Answer 5:** Self-supervised learning involves training a model on unlabeled data using a “self-supervision” signal generated from the data itself.  The goal is to learn representations or features from the data that are useful for various downstream tasks. For example, a language model like BERT is trained by predicting masked words in a sentence, using the context provided by the other words in the sentence. This is a self-supervision signal extracted from the text itself. The learned representations from this pretraining can then be adapted to perform various downstream tasks, such as question answering, text summarization, or machine translation, often achieving impressive performance even with limited labeled data. \n\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"response_7 = generate_gemini_response(first_half_cache, \"Using the midterm review as a guide, generate 4-5 practice questions that cover concepts across lectures, with a particular focus on SVMs and Decision Trees. The questions should require critical thinking and help prepare for the final exam. Include questions that explore the relationships between these topics and other course concepts. Provide detailed answers for each question. Use the midterm review as a guide to determine the types of questions that are relevant.\")\nprint(response_7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:22:41.389005Z","iopub.execute_input":"2024-11-29T17:22:41.389790Z","iopub.status.idle":"2024-11-29T17:24:00.459766Z","shell.execute_reply.started":"2024-11-29T17:22:41.389749Z","shell.execute_reply":"2024-11-29T17:24:00.458405Z"}},"outputs":[{"name":"stdout","text":"Here are 4-5 practice questions that cover concepts across lectures, with a particular focus on SVMs and Decision Trees, that you can use to prepare for the final exam:\n\n**Question 1:**\n\n**Explain the concept of a “margin” in Support Vector Machines (SVMs). How does the margin relate to the idea of a “separating hyperplane” and the concept of “overfitting?”**\n\n**Answer:**\n\nIn SVMs, the margin refers to the distance between the decision boundary (separating hyperplane) and the closest data points. A larger margin indicates that the classifier is more confident in its predictions, as data points are further away from the decision boundary. This concept is related to the idea of overfitting because a model with a large margin is less likely to overfit to the training data.\n\nHere's why:\n\n* **Overfitting:** Overfitting occurs when a model learns the training data too well, including noise and spurious patterns. This results in poor generalization to unseen data.\n* **Large Margin:** A large margin implies the model is more robust to noise and outliers in the training data. By maximizing the margin, SVMs aim to find a decision boundary that is as far away from the data points as possible, thus reducing the risk of overfitting.\n\n**Question 2:**\n\n**Compare and contrast SVMs with Logistic Regression. Discuss their advantages and disadvantages, focusing on model complexity and generalization performance.**\n\n**Answer:**\n\nSVMs and Logistic Regression are both supervised learning algorithms used for classification, but they differ in their underlying assumptions, model complexity, and generalization performance:\n\n**SVMs:**\n\n* **Model Complexity:** SVMs are generally considered more complex than Logistic Regression. They aim to maximize the margin between the classes, which can lead to more complex decision boundaries, especially in high-dimensional spaces.\n* **Generalization Performance:** SVMs often exhibit good generalization performance, particularly when dealing with high-dimensional data. Their focus on maximizing the margin helps to create more robust models that are less prone to overfitting.\n\n**Logistic Regression:**\n\n* **Model Complexity:** Logistic Regression is typically simpler than SVMs. It models the probability of a data point belonging to a particular class using a linear function.\n* **Generalization Performance:** Logistic Regression generally performs well, especially when the data is linearly separable. It can be more sensitive to overfitting than SVMs, particularly when dealing with complex datasets or high-dimensional spaces.\n\n**Key Differences:**\n\n* **Assumptions:** SVMs implicitly assume that the data is linearly separable, while Logistic Regression does not.\n* **Decision Boundary:** SVMs aim for a maximum margin decision boundary, while Logistic Regression uses a sigmoid function to model the probability of belonging to a class.\n\n**Question 3:**\n\n**Explain how Decision Trees work. How do they make predictions? What are the main advantages and disadvantages of using Decision Trees?**\n\n**Answer:**\n\nDecision Trees are non-parametric supervised learning algorithms that represent a series of decisions and their possible outcomes. They work by recursively partitioning the data based on the values of features, creating a tree-like structure.\n\n**Prediction Process:**\n\n* **Decision Nodes:** Each internal node in the tree represents a feature.\n* **Splitting Rules:**  Decision nodes have splitting rules that dictate which branch to follow based on the value of the feature.\n* **Leaf Nodes:** The tree's terminal nodes, called leaf nodes, represent the predicted output or class label.\n\n**Advantages:**\n\n* **Easy to Understand:** Decision Trees are intuitive and easy to interpret, even for non-technical audiences. \n* **Handle Non-Linear Relationships:** They can model non-linear relationships between features and target variables.\n* **Handle Missing Values:** They can handle missing values by considering only the available features during the splitting process.\n\n**Disadvantages:**\n\n* **Overfitting:** Decision Trees can overfit the training data if they are not pruned or regularized. They might create a very complex tree that is highly specific to the training data and does not generalize well. \n* **Instability:** Small changes in the training data can lead to significant changes in the tree structure, resulting in instability.\n* **Bias:** Decision Trees can exhibit bias towards features with a large number of splits, potentially neglecting important features with fewer splits.\n\n**Question 4:**\n\n**How can you use a kernel function to transform a non-linear problem into a linearly separable problem in SVM? Provide an example.**\n\n**Answer:**\n\nKernel functions allow SVMs to handle non-linearly separable data by implicitly transforming the original input space into a higher-dimensional feature space. This transformation makes the data linearly separable in the transformed space.\n\n**Example:**\n\nSuppose you have a dataset with two classes that are not linearly separable in a 2-dimensional space. \n\nUsing a polynomial kernel function, for example, a quadratic kernel (K(x,z) = (xTz+c)^2), the SVM would implicitly project the data points into a higher-dimensional space (in this case, a 3-dimensional space), where the data points become linearly separable.\n\n**Question 5:**\n\n**How does the \"curse of dimensionality\" impact Decision Tree construction? What techniques can you use to address this issue?**\n\n**Answer:**\n\nThe curse of dimensionality refers to the exponential increase in the number of possible splits as the number of features (dimensions) grows in a dataset. This significantly impacts Decision Tree construction because:\n\n* **Overfitting:** With a large number of features, Decision Trees can easily become very complex and overfit the training data.\n* **Computational Complexity:** The search for optimal splits becomes computationally expensive as the number of features increases.\n\n**Techniques to Address the Curse of Dimensionality:**\n\n* **Feature Selection:** Carefully selecting the most relevant features can reduce the number of dimensions, mitigating the curse of dimensionality.\n* **Pruning:** Pruning the Decision Tree by removing unnecessary branches can simplify the tree structure and improve generalization.\n* **Ensemble Methods:**  Using ensemble methods, such as Random Forests or Bagging, which combine multiple Decision Trees, can help reduce variance and overfitting.\n\nThese are just a few practice questions to get you started. You can find other relevant practice questions by reviewing the course notes, problem sets, and lecture recordings. Remember to focus on understanding the underlying principles and how they relate to different machine learning algorithms and techniques. \n\n","output_type":"stream"}],"execution_count":69},{"cell_type":"markdown","source":"## FUTURE IMPROVEMENTS\n------------------------------","metadata":{}},{"cell_type":"markdown","source":"**1. Singular Cache**: Collating both first and second half of the semester's material into one cache makes it easier for students to ask questions and not relying on knowing if content was convered in the first or second half. Due to 503 errors, I was unable to combine both into one cache & the cache token limit\n\n**2. Include Additional Course Materials**: If the cache window was longer, I would include historical course materials (additional midterm review and released midterm and final exams) which students could leverage to generate additional practice questions with to continue to reinforce concepts ahead of exams\n\n\n","metadata":{}}]}