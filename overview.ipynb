{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "For the Kaggle Challenge sponsored by Google, to highlight the capabilities of LLMs with very long context windows such as the Gemini models, I have created a semester study guide tool that allows users to ask questions about the course material across various mediums such as lecture videos, regular slides, annotated slides, midterm review and the textbook. Different caches can be created for diffrent combinations of material (content for Midterm 1, content for all the lectures so if students miss a class they can understand valuable points that were covered in the lecture that may not be in the notes, content curated on certain topics, etc.). There are endless possibilities of the different permutations and combinations of material that can be set up together. If certain students struggle on a set of topics, a cache can be created for that specific set of topics and the LLM can be topic focused as compared to a broad study guide. \n",
    "\n",
    "Having access to the entire semester's worth of material allows the LLM to draw connections between different concepts and provide more comprehensive answers which can serve as a very valuable resource in the education space and effectively showcases the benefits of caching and long context windows and the cost benefits of not having to pay for many tokens if students are using the same caches of content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/netraranga/Desktop/Projects/google_gemini/myenv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/netraranga/Desktop/Projects/google_gemini/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from google.generativeai import caching\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import os\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from io import BytesIO\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('/Users/netraranga/Desktop/Projects/.env')\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following datasets are the syllabus and transcripts of the lectures from the 2022 Fall Playlist. Due to copyright restrictions, the raw lecture videos are not available and I used the Youtube API to pull the transcripts of the lectures. If a professor or university provided permission to use the raw lecture videos, those could have been used instead. \n",
    "\n",
    "The functions below are used to write the transcripts to individual text files and to merge the regular lecture slides and annotated slides into one file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_transcripts_to_files(youtube_df)\n",
    "\n",
    "combined_annotated_slides = []\n",
    "for file_path in os.listdir('/Users/netraranga/Desktop/Projects/google_gemini/docs'):\n",
    "      if 'annotated' in file_path:\n",
    "            combined_annotated_slides.append(file_path.split('_')[0])\n",
    "\n",
    "output_files = merge_annotated_slides(combined_annotated_slides) #Get list of files that need to be consolidated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_df = pd.read_csv('youtube_playlist_contents.csv') #pull in transcript content \n",
    "youtube_df['Lecture'] = youtube_df.index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are two functions that are used to write the transcripts to individual text files and to merge the regular lecture slides and annotated slides into one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_cache(list_lectures):\n",
    "    index_vals = len(list_lectures) + 1\n",
    "    list_files = []\n",
    "    for i in range(1, index_vals + 1):\n",
    "        lecture_file = f'/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_{i}_transcript.txt'\n",
    "        file_name = f'lecture_{i}'\n",
    "        file_name = genai.upload_file(path=lecture_file)\n",
    "        list_files.append(file_name)\n",
    "    return list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_files = create_context_cache([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_1 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_1_transcript.txt'\n",
    "file_1 = genai.upload_file(path=lecture_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Order of files - use only lectures and annotated slides\n",
    "lecture_1 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_1_transcript.txt'\n",
    "file_1 = genai.upload_file(path=lecture_1)\n",
    "\n",
    "lecture_2 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_2_transcript.txt'\n",
    "file_2 = genai.upload_file(path=lecture_2)\n",
    "\n",
    "# lin_alg_notes = '/Users/netraranga/Desktop/Projects/google_gemini/docs/linalg_notes.pdf'\n",
    "# file_3 = genai.upload_file(path=lin_alg_notes)\n",
    "\n",
    "# lin_alg_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs/linalg_slides.pdf'\n",
    "# file_3_1 = genai.upload_file(path=lin_alg_slides)\n",
    "\n",
    "lecture_3 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_3_transcript.txt'\n",
    "file_4 = genai.upload_file(path=lecture_3)\n",
    "\n",
    "lecture_4 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_4_transcript.txt'\n",
    "file_5 = genai.upload_file(path=lecture_4)\n",
    "\n",
    "# probs_notes = '/Users/netraranga/Desktop/Projects/google_gemini/docs/prob_notes.pdf'\n",
    "# file_6 = genai.upload_file(path=probs_notes)\n",
    "\n",
    "# probs_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs/prob_slides.pdf'\n",
    "# file_6_1 = genai.upload_file(path=probs_slides)\n",
    "\n",
    "lecture_5 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_5_transcript.txt'\n",
    "file_7 = genai.upload_file(path=lecture_5)\n",
    "\n",
    "lecture_6 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_6_transcript.txt'\n",
    "file_8 = genai.upload_file(path=lecture_6)\n",
    "\n",
    "# numpy_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs/numpy_slides.pdf'\n",
    "# file_9 = genai.upload_file(path=numpy_slides)\n",
    "\n",
    "lecture_7 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_7_transcript.txt'\n",
    "file_10 = genai.upload_file(path=lecture_7)\n",
    "\n",
    "lecture_8 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_8_transcript.txt'\n",
    "file_11 = genai.upload_file(path=lecture_8)\n",
    "\n",
    "eval_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs/original_pdfs/eval_slides.pdf'\n",
    "file_12 = genai.upload_file(path=eval_slides)\n",
    "\n",
    "lecture_9 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_9_transcript.txt'\n",
    "file_13 = genai.upload_file(path=lecture_9)\n",
    "\n",
    "lecture_10 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_10_transcript.txt'\n",
    "file_14 = genai.upload_file(path=lecture_10)\n",
    "\n",
    "bias_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/bias_annotated.pdf'\n",
    "file_15 = genai.upload_file(path=bias_slides)\n",
    "\n",
    "ridge_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs/original_pdfs/ridge_annotated.pdf'\n",
    "file_16 = genai.upload_file(path=ridge_slides)\n",
    "\n",
    "lasso_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs/original_pdfs/lasso_annotated.pdf'\n",
    "file_17 = genai.upload_file(path=lasso_slides)\n",
    "\n",
    "midterm_review = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/midterm_review.pdf'\n",
    "file_18 = genai.upload_file(path=midterm_review)\n",
    "\n",
    "lecture_11 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_11_transcript.txt'\n",
    "file_19 = genai.upload_file(path=lecture_11)\n",
    "\n",
    "boosting_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/boosting.pdf'\n",
    "file_20 = genai.upload_file(path=boosting_slides)\n",
    "\n",
    "decision_trees_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/decisiontrees_annotated.pdf'\n",
    "file_21 = genai.upload_file(path=decision_trees_slides)\n",
    "\n",
    "# decision_trees_overfitting = '/Users/netraranga/Desktop/Projects/google_gemini/docs/decisiontrees_overfitting.pdf'\n",
    "# file_22 = genai.upload_file(path=decision_trees_overfitting)\n",
    "\n",
    "lecture_12 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_12_transcript.txt'\n",
    "file_23 = genai.upload_file(path=lecture_12)\n",
    "\n",
    "lecture_13 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_13_transcript.txt'\n",
    "file_24 = genai.upload_file(path=lecture_13)\n",
    "\n",
    "kmeans_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/kmeans_annotated.pdf'\n",
    "file_25 = genai.upload_file(path=kmeans_slides)\n",
    "\n",
    "em_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/em_annotated.pdf'\n",
    "file_26 = genai.upload_file(path=em_slides)\n",
    "\n",
    "pca_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/pca_annotated.pdf'\n",
    "file_27 = genai.upload_file(path=pca_slides)\n",
    "\n",
    "lecture_14 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_14_transcript.txt'\n",
    "file_28 = genai.upload_file(path=lecture_14)\n",
    "\n",
    "lecture_15 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_15_transcript.txt'\n",
    "file_29 = genai.upload_file(path=lecture_15)\n",
    "\n",
    "# ml_advice = '/Users/netraranga/Desktop/Projects/google_gemini/docs/ml_advice.pdf'\n",
    "# file_30 = genai.upload_file(path=ml_advice)\n",
    "\n",
    "lecture_16 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_16_transcript.txt'\n",
    "file_31 = genai.upload_file(path=lecture_16)\n",
    "\n",
    "learning_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/learning.pdf'\n",
    "file_32 = genai.upload_file(path=learning_slides)\n",
    "\n",
    "lecture_17 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_17_transcript.txt'\n",
    "file_33 = genai.upload_file(path=lecture_17)\n",
    "\n",
    "lecture_18 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_18_transcript.txt'\n",
    "file_34 = genai.upload_file(path=lecture_18)\n",
    "\n",
    "lecture_19 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_19_transcript.txt'\n",
    "file_35 = genai.upload_file(path=lecture_19)\n",
    "\n",
    "fairness_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/fairness_annotated.pdf'\n",
    "file_36 = genai.upload_file(path=fairness_slides)\n",
    "\n",
    "privacy_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/privacy_annotated.pdf'\n",
    "file_37 = genai.upload_file(path=privacy_slides)\n",
    "\n",
    "explanation_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/explainability_annotated.pdf'\n",
    "file_38 = genai.upload_file(path=explanation_slides)\n",
    "\n",
    "textbook = '/Users/netraranga/Desktop/Projects/google_gemini/docs/textbook.pdf'\n",
    "file_39 = genai.upload_file(path=textbook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### System Prompt\n",
    "system_prompt = \"\"\"You are an expert tutor specializing in machine learning, with comprehensive knowledge of the Stanford CS229 \"Introduction to Machine Learning\" course. You have access to all relevant materials, including:\n",
    "- Annotated and regular lecture notes for each session.\n",
    "- Transcripts of all recorded lectures.\n",
    "- The complete course textbook.\n",
    "Your role is to guide the user through the CS229 course material by:\n",
    "1. **Providing clear, detailed explanations** of key machine learning concepts and algorithms, from foundational topics like linear regression and classification to advanced areas such as support vector machines and unsupervised learning.\n",
    "2. **Connecting course concepts**, explaining how different topics (e.g., gradient descent, regularization) relate and build upon each other across lectures.\n",
    "3. **Summarizing lectures and sections**, highlighting major takeaways, essential equations, and conceptual insights.\n",
    "4. **Supporting exam preparation**, identifying high-impact topics, common pitfalls, and suggesting areas for further review.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture 1: The phrase \"machine learning\" was first introduced in 1959 by Arthur Samuel, who defined it as giving computers the ability to learn without being explicitly programmed.\n",
      "\n",
      "Lecture 2:  Zillow attempted to use machine learning to predict house prices and flip houses, ultimately losing a significant amount of money, while Blackstone successfully profited from a similar venture.  This highlights the challenges and potential rewards in applying machine learning to real-world problems.\n",
      "\n",
      "Lecture 3:  Newton's method, while incredibly fast for converging to a solution, is computationally expensive for high-dimensional problems due to the need to compute the Hessian matrix, making it less practical for many modern machine learning applications.\n",
      "\n",
      "Lecture 4: Many common probability distributions (Bernoulli, Gaussian, Poisson, Gamma, etc.) belong to the exponential family, a fact that simplifies inference and learning through a unified framework.\n",
      "\n",
      "Lecture 5: Even with simplifying assumptions like using a Gaussian distribution to model features, the decision boundary in Gaussian Discriminant Analysis (GDA) is still fundamentally different from that of logistic regression, due to the different ways in which the parameters are learned and the underlying probabilistic models differ.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textbook_cache = create_cache(name='first_5_lectures', contents=cache_files)\n",
    "response_1 = gemini_response(textbook_cache, 'Give me an interesting fact from each lecture. Provide the output in the following format: Lecture 1: Fact 1. Lecture 2: Fact 2. Lecture 3: Fact 3. Lecture 4: Fact 4. Lecture 5: Fact 5.')\n",
    "print(response_1.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Queries from certain lectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Stanford CS229 course, \"Introduction to Machine Learning,\" provides a foundational understanding of the field.  The instructors emphasize that while introductory, the course is mathematically intensive and requires a solid background in probability, linear algebra, and programming (especially Python and NumPy).  They strongly recommend students have experience with at least two of these three areas.\n",
      "\n",
      "**Main Topics:**\n",
      "\n",
      "The course covers a broad range of machine learning topics, organized roughly into these areas:\n",
      "\n",
      "1. **Supervised Learning:** This forms a large portion of the course.  It focuses on:\n",
      "    * **Regression:** Predicting continuous values (e.g., house prices).  Linear and polynomial regression are key examples.\n",
      "    * **Classification:** Predicting categorical values (e.g., classifying images, identifying spam).  The course will cover various classification algorithms.  The concept of features (inputs) and labels (outputs) is central here.  High-dimensional features and feature selection are also discussed.\n",
      "    * **Infinite-dimensional features:**  A more advanced topic exploring how to handle an effectively infinite number of features in models.\n",
      "\n",
      "2. **Unsupervised Learning:** This section explores learning from unlabeled data to discover structure. Key algorithms and concepts include:\n",
      "    * **Clustering:** Grouping similar data points (e.g., customer segmentation, gene clustering).  Algorithms like k-means and Gaussian mixture models will be covered.\n",
      "    * **Dimensionality Reduction:** Techniques to represent data in lower dimensions while preserving important information.  Topic modeling (e.g., discovering topics in a corpus of documents) is an application example.\n",
      "    * **Word Embeddings:** Representing words as vectors, capturing semantic relationships between words (like those used in large language models).\n",
      "\n",
      "3. **Reinforcement Learning:** This section covers learning to make sequential decisions, focusing on:\n",
      "    * **Sequential Decision Making:**  Making decisions with long-term consequences (e.g., controlling robots, playing games).  The iterative process of trial-and-error, feedback collection, and strategy improvement is emphasized.\n",
      "    * **Delayed Rewards:**  Dealing with situations where the consequences of actions are not immediately apparent.\n",
      "\n",
      "4. **Other Topics:**\n",
      "    * **Deep Learning Basics:**  An introduction to the fundamental concepts and techniques of deep learning, a powerful subset of machine learning.\n",
      "    * **Learning Theory:**  Understanding the theoretical foundations of machine learning, including model selection and generalization.\n",
      "    * **Practical Aspects of Model Tuning:**  Guidance on making effective choices during model training and deployment.\n",
      "    * **Robustness and Fairness in Machine Learning:** A guest lecture will address ethical considerations and societal impacts of machine learning models.  This includes discussions about mitigating bias and ensuring reliable performance.\n",
      "\n",
      "\n",
      "**Assignments and Exams:**\n",
      "\n",
      "* **Four Homeworks:** These assignments will combine mathematical derivations with practical programming components using Python and NumPy.\n",
      "* **One Midterm Exam:**  This assesses understanding of the core concepts and algorithms covered in the first half of the course.\n",
      "* **One Final Project:** Students work in groups of one to three to apply machine learning to a problem of their choice.  There are *no* late days allowed for this project.\n",
      "* **TA Lectures and Discussion Sections:** Optional supplementary sessions covering review material, advanced topics, and interactive problem-solving to support homework completion.\n",
      "\n",
      "\n",
      "The course website is a crucial resource, containing detailed logistical information, the syllabus, lecture notes, and guidelines for the final project.  The instructors strongly encourage students to use this resource first when encountering questions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textbook_cache = create_cache(name='course_overview', contents=file_1)\n",
    "response_1 = gemini_response(textbook_cache, 'Give me an overview of what this course is about, the main topics that will be covered over the semester, and the homework assignments and exams that will be given.')\n",
    "print(response_1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_cache() got an unexpected keyword argument 'contents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m textbook \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/netraranga/Desktop/Projects/google_gemini/docs/textbook.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m file_39 \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mupload_file(path\u001b[38;5;241m=\u001b[39mtextbook)\n\u001b[0;32m----> 3\u001b[0m textbook_cache \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_39\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m response_1 \u001b[38;5;241m=\u001b[39m gemini_response(textbook_cache, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGive me an overview of L1 and L2 regularization and how they are used in machine learning.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: create_cache() got an unexpected keyword argument 'contents'"
     ]
    }
   ],
   "source": [
    "textbook = '/Users/netraranga/Desktop/Projects/google_gemini/docs/textbook.pdf'\n",
    "file_39 = genai.upload_file(path=textbook)\n",
    "textbook_cache = create_cache(name='textbook', contents=file_39)\n",
    "response_1 = gemini_response(textbook_cache, 'Give me an overview of L1 and L2 regularization and how they are used in machine learning.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_cache = create_cache(name='lecture_notes', contents=[file_1, file_2, file_4, file_5, file_7, file_8, file_10, file_11, file_12, file_13, file_14, file_15, file_16, file_17, file_18, file_19, file_20, file_21, file_23, file_24, file_25, file_26, file_27, file_28, file_29, file_31, file_32, file_33, file_34, file_35, file_39])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_2 = gemini_response(lecture_cache, 'What is an examle of something mentioned in the Neural Networks lecture that wasn not included in the textbook? Provide 2-3 specific examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Neural Networks lectures in CS229 contain several concepts and details not explicitly covered in the textbook. Here are a few examples:\n",
      "\n",
      "1. **Different Variants of Gradient Descent in Deep Learning:** While the textbook covers gradient descent, the lectures delve into the specifics of stochastic gradient descent (SGD) and mini-batch gradient descent, including practical considerations like the choice of batch size (often determined empirically by the maximum batch size your GPU memory can handle) and the fact that smaller batch sizes often lead to better performance but at the cost of increased noise.  The textbook doesn't give the same level of practical algorithmic detail.\n",
      "\n",
      "2. **ReLU and Other Activation Functions:** Although the textbook might mention activation functions in general, the lectures focus specifically on the ReLU (Rectified Linear Unit) activation function, its properties (non-linearity), its use in neural networks, and its widespread popularity in deep learning.  The lectures also discuss other activation functions like sigmoid and tanh, comparing and contrasting their properties and suitability for different applications; this level of comparative analysis isn't present in the textbook's more concise overview.\n",
      "\n",
      "3. **Connection Between Neural Networks and Kernel Methods:** The lectures draw a direct parallel between neural networks and kernel methods. The idea that the penultimate layer of a neural network can be viewed as a learned feature map (Φβ(x)), creating a structure similar to the feature map used in kernel methods (Φ(x)), is explained in detail. This connection, highlighting the learned versus handcrafted feature aspect, is not explicitly elaborated in the textbook.\n",
      "\n",
      "\n",
      "These examples show how the lectures often build upon the textbook's foundational material by providing specific algorithmic details, implementation advice, and connections between different machine learning approaches.  They enhance the understanding of practical aspects not always emphasized in the theoretical treatment of the textbook.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response_2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CS229 lecture on KMeans includes several important points and nuanced discussions not explicitly detailed in the accompanying notes.  Here are some key concepts that are discussed in the lecture but receive less attention or are absent from the notes:\n",
      "\n",
      "\n",
      "1. **The Squishiness of Unsupervised Learning and the Role of Assumptions:** The lecture emphasizes the inherent ambiguity and difficulty in unsupervised learning compared to supervised learning.  It highlights that unsupervised methods necessitate stronger assumptions about the underlying data structure (e.g., the existence of K clusters) and accept weaker guarantees (e.g., convergence to a local rather than a global optimum). This philosophical point about the trade-off between stronger assumptions and weaker guarantees isn't explicitly stated in the notes, which focus more on the algorithmic details.\n",
      "\n",
      "2. **Initialization Strategies and KMeans++:** The lecture introduces the importance of initialization in KMeans. While the notes mention random initialization, they lack the discussion of the limitations of this approach, particularly the possibility of getting stuck in poor local minima due to bad initial centroid placement. The lecture introduces KMeans++, highlighting its improved approximation ratio and its role as the default initialization method in scikit-learn,  providing a more robust approach than simply running KMeans multiple times with random initialization.  This sophisticated algorithm and its advantages are not covered in the provided notes.\n",
      "\n",
      "3. **The Difficulty of Choosing K:** The lecture explicitly addresses the challenging task of selecting the optimal number of clusters (K).  The notes primarily focus on the algorithm assuming K is known. However, the lecture acknowledges that there is no single \"right\" answer for K, that it's fundamentally a modeling choice, and that the user may need to leverage domain knowledge or evaluate results for different K values to determine the most meaningful clustering.\n",
      "\n",
      "4. **Intuitive Explanation of KMeans Convergence:** The lecture provides a more intuitive explanation for why the KMeans algorithm converges.  The notes state convergence but lack the detailed explanation of how the potential function (sum of squared distances from points to their cluster centers) monotonically decreases with each iteration, preventing oscillations and ensuring convergence to a (possibly local) minimum.  The lecture connects this convergence to the concept of gradient descent on the potential function, offering a deeper understanding than just stating convergence.\n",
      "\n",
      "5. **Comparison to Other Clustering Approaches:** The lecture briefly touches upon the limitations of KMeans due to its assumption of spherical, equally sized clusters and mentions other approaches, such as the application of k-means after performing dimensionality reduction (e.g., PCA or embeddings) for data which are not spherically symmetric,  or the merging of clusters to improve clustering results. This discussion on the limitations of the basic algorithm and the existence of more sophisticated techniques is absent from the notes.\n",
      "\n",
      "In summary, while the notes provide the core KMeans algorithm, the lecture expands upon these core aspects by offering crucial insights into the theoretical challenges and practical considerations involved in unsupervised learning and the implementation of KMeans.  The focus shifts from purely algorithmic to a more nuanced discussion of assumptions, limitations, and interpretational subtleties of the method.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_3 = gemini_response(lecture_cache, 'What are some key concepts covered in the KMeans lecture that are not covered in the notes? Be very specific in the points you generate.')\n",
    "print(response_3.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca_comparison = genai.upload_file(path='/Users/netraranga/Desktop/Projects/google_gemini/docs/consolidated/combined_pca_slides.pdf')\n",
    "pca_cache = create_cache(name='pca', contents=pca_comparison)\n",
    "response_pca= gemini_response(pca_cache, 'What are the differences between the PCA original slides and the annotated slides? Be specific in the type of differences you generate, do not include generic points like the annotated slides have more information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between the original and annotated PCA slides lies primarily in the addition of handwritten notes and diagrams on the annotated version.  These additions clarify concepts and calculations presented in the original slides. Let's break down the specific types of annotations:\n",
      "\n",
      "1. **Elaboration on Mathematical Concepts:** The annotated slides contain additional mathematical steps, derivations, and explanations of equations related to reconstruction error, covariance matrices, and the relationship between PCA and eigenvectors.  For example, the derivation of the reconstruction error is expanded, showing intermediate steps and clarifying how minimizing this error leads to the selection of eigenvectors.\n",
      "\n",
      "2. **Visual Aids and Interpretations:**  Handwritten diagrams and annotations are added to existing figures to illustrate the projection process, the reconstruction error visually, and the choice of optimal projection vectors.  These additions offer a more intuitive understanding of how the algorithm works geometrically.\n",
      "\n",
      "3. **Conceptual Clarifications:**  The annotator adds notes explaining the meaning and implications of key concepts like \"intrinsic dimensionality,\" the selection of principal components based on eigenvalues, and the interpretation of eigenvectors in the context of the data.  For instance, the slides illustrating word embeddings are enhanced with labels that group words by semantic categories (e.g., \"names,\" \"months,\" \"states/cities\").\n",
      "\n",
      "4. **Algorithm Steps and Flow:** The process of PCA, especially the steps in the algorithm, is broken down further and explained in more detail using handwritten annotations to better guide understanding of the flow.\n",
      "\n",
      "5. **Practical Considerations and Alternative Approaches:** The annotated slides touch upon practical issues like the computational challenges of dealing with high-dimensional covariance matrices, and introduce Singular Value Decomposition (SVD) as an efficient alternative.\n",
      "\n",
      "\n",
      "In short, while the original slides provide a structured overview of PCA, the annotations significantly enhance the understanding by providing detailed mathematical support, geometrical interpretations, conceptual clarifications, and practical implementation tips. They transform the slides from a presentation of results into a step-by-step tutorial.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response_pca.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in caching.CachedContent.list():\n",
    "  print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in caching.CachedContent.list():\n",
    "#   print(c) #Slide 1 to 15 are 166273 tokens\n",
    "  #Slide 1 to 38 are 500798\n",
    "\n",
    "for c in caching.CachedContent.list():\n",
    "    c.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO ODO\n",
    "#-combines all of the slide contents into one file so it passes the cache min size limit\n",
    "#Determine with chatgpt what are good questions - study guides on certain lectures and concepts\n",
    "#Identify the differece between annotated notes and regular notes\n",
    "#Create a study guide that is grounded in the lecture nad pulls additional key concepts from the notes\n",
    "#Generate some python questions for certain lectures for the application piece \n",
    "#Watch a certain video and see if the LLM can retrieve the specific fact or instnce referenced in the video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
