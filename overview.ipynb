{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "For the Kaggle Challenge sponsored by Google, to highlight the capabilities of LLMs with very long context windows such as the Gemini models, I have created a semester study guide tool that allows users to ask questions about the course material across various mediums such as lecture videos, regular slides, annotated slides, midterm review and the textbook. Different caches can be created for diffrent combinations of material (content for Midterm 1, content for all the lectures so if students miss a class they can understand valuable points that were covered in the lecture that may not be in the notes, content curated on certain topics, etc.). There are endless possibilities of the different permutations and combinations of material that can be set up together. If certain students struggle on a set of topics, a cache can be created for that specific set of topics and the LLM can be topic focused as compared to a broad study guide. \n",
    "\n",
    "Having access to the entire semester's worth of material allows the LLM to draw connections between different concepts and provide more comprehensive answers which can serve as a very valuable resource in the education space and effectively showcases the benefits of caching and long context windows and the cost benefits of not having to pay for many tokens if students are using the same caches of content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from google.generativeai import caching\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import os\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from io import BytesIO\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('/Users/netraranga/Desktop/Projects/.env')\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following datasets are the syllabus and transcripts of the lectures from the 2022 Fall Playlist. Due to copyright restrictions, the raw lecture videos are not available and I used the Youtube API to pull the transcripts of the lectures. If a professor or university provided permission to use the raw lecture videos, those could have been used instead. \n",
    "\n",
    "The functions below are used to write the transcripts to individual text files and to merge the regular lecture slides and annotated slides into one file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_transcripts_to_files(youtube_df)\n",
    "\n",
    "combined_annotated_slides = []\n",
    "for file_path in os.listdir('/Users/netraranga/Desktop/Projects/google_gemini/docs'):\n",
    "      if 'annotated' in file_path:\n",
    "            combined_annotated_slides.append(file_path.split('_')[0])\n",
    "\n",
    "output_files = merge_annotated_slides(combined_annotated_slides) #Get list of files that need to be consolidated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_df = pd.read_csv('youtube_playlist_contents.csv') #pull in transcript content \n",
    "youtube_df['Lecture'] = youtube_df.index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are functions used to create caches for different lecture combinations and to generate responses from Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_cache_lectures(list_lectures):\n",
    "    index_vals = len(list_lectures) + 1\n",
    "    list_files = []\n",
    "    for i in range(1, index_vals):\n",
    "        lecture_file = f'/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_{i}_transcript.txt'\n",
    "        file_name = f'lecture_{i}'\n",
    "        file_name = genai.upload_file(path=lecture_file)\n",
    "        list_files.append(file_name)\n",
    "    return list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gemini_response(cache, prompt):\n",
    "    response = gemini_response(cache, prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create cache for various lectures combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_files = create_context_cache_lectures([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "### System Prompt\n",
    "system_prompt = \"\"\"You are an expert tutor specializing in machine learning, with comprehensive knowledge of the Stanford CS229 \"Introduction to Machine Learning\" course. You have access to all relevant materials, including:\n",
    "- Annotated and regular lecture notes for each session.\n",
    "- Transcripts of all recorded lectures.\n",
    "- The complete course textbook.\n",
    "Your role is to guide the user through the CS229 course material by:\n",
    "1. **Providing clear, detailed explanations** of key machine learning concepts and algorithms, from foundational topics like linear regression and classification to advanced areas such as support vector machines and unsupervised learning.\n",
    "2. **Connecting course concepts**, explaining how different topics (e.g., gradient descent, regularization) relate and build upon each other across lectures.\n",
    "3. **Summarizing lectures and sections**, highlighting major takeaways, essential equations, and conceptual insights.\n",
    "4. **Supporting exam preparation**, identifying high-impact topics, common pitfalls, and suggesting areas for further review.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture 1: The phrase \"machine learning\" was first introduced in 1959 by Arthur Samuel, who defined it as giving computers the ability to learn without being explicitly programmed.\n",
      "\n",
      "Lecture 2:  Zillow, a major real estate company, attempted to use machine learning to predict house prices and flip properties, but ultimately lost a significant amount of money in the process.  This highlights the real-world challenges and risks involved in applying machine learning, even to seemingly straightforward problems.\n",
      "\n",
      "Lecture 3:  The sigmoid function, commonly used in logistic regression, is a smooth, non-linear function that maps values to the range (0,1), making it suitable for representing probabilities and improving the performance of gradient descent optimization.\n",
      "\n",
      "Lecture 4: Many common probability distributions (Bernoulli, Gaussian, Poisson, Gamma, Exponential, Laplace) belong to the exponential family, a fact that simplifies inference and learning significantly.\n",
      "\n",
      "Lecture 5:  In Gaussian Discriminant Analysis (GDA), if the covariance matrices for different classes are assumed to be equal, the resulting decision boundary between classes is linear.  This simplifies calculations and provides an analytical solution for the maximum likelihood estimates of the parameters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textbook_cache = create_cache(name='first_5_lectures', contents=cache_files)\n",
    "response_1 = generate_gemini_response(textbook_cache, 'Give me an interesting fact from each lecture. Provide the output in the following format: Lecture 1: Fact 1. Lecture 2: Fact 2. Lecture 3: Fact 3. Lecture 4: Fact 4. Lecture 5: Fact 5.')\n",
    "print(response_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache for entire semester worth of content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_1_transcript.txt',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_2_transcript.txt',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_3_transcript.txt',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_4_transcript.txt',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_5_transcript.txt',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_6_transcript.txt',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_7_transcript.txt',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_8_transcript.txt',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/original_pdfs/eval_slides.pdf',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_9_transcript.txt',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_10_transcript.txt',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/original_pdfs/bias_annotated.pdf',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/original_pdfs/ridge_annotated.pdf',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/original_pdfs/lasso_annotated.pdf',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/original_pdfs/midterm_review.pdf',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_11_transcript.txt',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/boosting.pdf',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/decisiontrees_annotated.pdf',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_12_transcript.txt',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_13_transcript.txt',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/kmeans_annotated.pdf',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/em_annotated.pdf',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/pca_annotated.pdf',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_14_transcript.txt',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_15_transcript.txt',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_16_transcript.txt',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/learning.pdf',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_17_transcript.txt',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_18_transcript.txt',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_19_transcript.txt',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/fairness_annotated.pdf',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/privacy_annotated.pdf',\n",
    "    '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/explainability_annotated.pdf'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_semester_content(content, cache_name):\n",
    "    semester_cache = []\n",
    "    for file_path in content:\n",
    "        file_name = genai.upload_file(path=file_path)\n",
    "        semester_cache.append(file_name)\n",
    "    final_textbook_cache = create_cache(name=cache_name, contents=semester_cache)\n",
    "    return final_textbook_cache\n",
    "    #return semester_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResumableUploadError",
     "evalue": "<HttpError 503 \"Service Unavailable\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResumableUploadError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# semester_cache = \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m midterm_review_cache \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_context_semester_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmidterm_review_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[136], line 4\u001b[0m, in \u001b[0;36mcreate_context_semester_content\u001b[0;34m(content, cache_name)\u001b[0m\n\u001b[1;32m      2\u001b[0m semester_cache \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m content:\n\u001b[0;32m----> 4\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m \u001b[43mgenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     semester_cache\u001b[38;5;241m.\u001b[39mappend(file_name)\n\u001b[1;32m      6\u001b[0m final_textbook_cache \u001b[38;5;241m=\u001b[39m create_cache(name\u001b[38;5;241m=\u001b[39mcache_name, contents\u001b[38;5;241m=\u001b[39msemester_cache)\n",
      "File \u001b[0;32m~/Desktop/Projects/google_gemini/myenv/lib/python3.9/site-packages/google/generativeai/files.py:85\u001b[0m, in \u001b[0;36mupload_file\u001b[0;34m(path, mime_type, name, display_name, resumable)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[1;32m     83\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 85\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmime_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmime_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresumable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresumable\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file_types\u001b[38;5;241m.\u001b[39mFile(response)\n",
      "File \u001b[0;32m~/Desktop/Projects/google_gemini/myenv/lib/python3.9/site-packages/google/generativeai/client.py:121\u001b[0m, in \u001b[0;36mFileServiceClient.create_file\u001b[0;34m(self, path, mime_type, name, display_name, resumable, metadata)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m metadata:\n\u001b[1;32m    120\u001b[0m     request\u001b[38;5;241m.\u001b[39mheaders[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m--> 121\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_file({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[0;32m~/Desktop/Projects/google_gemini/myenv/lib/python3.9/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/google_gemini/myenv/lib/python3.9/site-packages/googleapiclient/http.py:902\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    900\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         _, body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_retries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m body\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# Non-resumable case.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Projects/google_gemini/myenv/lib/python3.9/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/google_gemini/myenv/lib/python3.9/site-packages/googleapiclient/http.py:1022\u001b[0m, in \u001b[0;36mHttpRequest.next_chunk\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresumable_uri \u001b[38;5;241m=\u001b[39m resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1022\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ResumableUploadError(resp, content)\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_error_state:\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;66;03m# If we are in an error state then query the server for current state of\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;66;03m# the upload by sending an empty PUT and reading the 'range' header in\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;66;03m# the response.\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m     headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Range\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes */\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m size, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "\u001b[0;31mResumableUploadError\u001b[0m: <HttpError 503 \"Service Unavailable\">"
     ]
    }
   ],
   "source": [
    "# semester_cache = \n",
    "midterm_review_cache = create_context_semester_content(all_files, 'midterm_review_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResumableUploadError",
     "evalue": "<HttpError 503 when requesting None returned \"Service Unavailable\". Details: \"Service Unavailable\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResumableUploadError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m file_16 \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mupload_file(path\u001b[38;5;241m=\u001b[39mridge_slides)\n\u001b[1;32m     56\u001b[0m lasso_slides \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/netraranga/Desktop/Projects/google_gemini/docs/original_pdfs/lasso_annotated.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 57\u001b[0m file_17 \u001b[38;5;241m=\u001b[39m \u001b[43mgenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlasso_slides\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m midterm_review \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/midterm_review.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     60\u001b[0m file_18 \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mupload_file(path\u001b[38;5;241m=\u001b[39mmidterm_review)\n",
      "File \u001b[0;32m~/Desktop/Projects/google_gemini/myenv/lib/python3.9/site-packages/google/generativeai/files.py:85\u001b[0m, in \u001b[0;36mupload_file\u001b[0;34m(path, mime_type, name, display_name, resumable)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[1;32m     83\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 85\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmime_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmime_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresumable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresumable\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file_types\u001b[38;5;241m.\u001b[39mFile(response)\n",
      "File \u001b[0;32m~/Desktop/Projects/google_gemini/myenv/lib/python3.9/site-packages/google/generativeai/client.py:121\u001b[0m, in \u001b[0;36mFileServiceClient.create_file\u001b[0;34m(self, path, mime_type, name, display_name, resumable, metadata)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m metadata:\n\u001b[1;32m    120\u001b[0m     request\u001b[38;5;241m.\u001b[39mheaders[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m--> 121\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_file({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[0;32m~/Desktop/Projects/google_gemini/myenv/lib/python3.9/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/google_gemini/myenv/lib/python3.9/site-packages/googleapiclient/http.py:902\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    900\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         _, body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_retries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m body\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# Non-resumable case.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Projects/google_gemini/myenv/lib/python3.9/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/google_gemini/myenv/lib/python3.9/site-packages/googleapiclient/http.py:1022\u001b[0m, in \u001b[0;36mHttpRequest.next_chunk\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresumable_uri \u001b[38;5;241m=\u001b[39m resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1022\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ResumableUploadError(resp, content)\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_error_state:\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;66;03m# If we are in an error state then query the server for current state of\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;66;03m# the upload by sending an empty PUT and reading the 'range' header in\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;66;03m# the response.\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m     headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Range\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes */\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m size, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "\u001b[0;31mResumableUploadError\u001b[0m: <HttpError 503 when requesting None returned \"Service Unavailable\". Details: \"Service Unavailable\">"
     ]
    }
   ],
   "source": [
    "###Order of files - use only lectures and annotated slides\n",
    "lecture_1 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_1_transcript.txt'\n",
    "file_1 = genai.upload_file(path=lecture_1)\n",
    "\n",
    "lecture_2 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_2_transcript.txt'\n",
    "file_2 = genai.upload_file(path=lecture_2)\n",
    "\n",
    "# lin_alg_notes = '/Users/netraranga/Desktop/Projects/google_gemini/docs/linalg_notes.pdf'\n",
    "# file_3 = genai.upload_file(path=lin_alg_notes)\n",
    "\n",
    "# lin_alg_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs/linalg_slides.pdf'\n",
    "# file_3_1 = genai.upload_file(path=lin_alg_slides)\n",
    "\n",
    "lecture_3 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_3_transcript.txt'\n",
    "file_4 = genai.upload_file(path=lecture_3)\n",
    "\n",
    "lecture_4 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_4_transcript.txt'\n",
    "file_5 = genai.upload_file(path=lecture_4)\n",
    "\n",
    "# probs_notes = '/Users/netraranga/Desktop/Projects/google_gemini/docs/prob_notes.pdf'\n",
    "# file_6 = genai.upload_file(path=probs_notes)\n",
    "\n",
    "# probs_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs/prob_slides.pdf'\n",
    "# file_6_1 = genai.upload_file(path=probs_slides)\n",
    "\n",
    "lecture_5 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_5_transcript.txt'\n",
    "file_7 = genai.upload_file(path=lecture_5)\n",
    "\n",
    "lecture_6 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_6_transcript.txt'\n",
    "file_8 = genai.upload_file(path=lecture_6)\n",
    "\n",
    "# numpy_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs/numpy_slides.pdf'\n",
    "# file_9 = genai.upload_file(path=numpy_slides)\n",
    "\n",
    "lecture_7 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_7_transcript.txt'\n",
    "file_10 = genai.upload_file(path=lecture_7)\n",
    "\n",
    "lecture_8 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_8_transcript.txt'\n",
    "file_11 = genai.upload_file(path=lecture_8)\n",
    "\n",
    "eval_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs/original_pdfs/eval_slides.pdf'\n",
    "file_12 = genai.upload_file(path=eval_slides)\n",
    "\n",
    "lecture_9 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_9_transcript.txt'\n",
    "file_13 = genai.upload_file(path=lecture_9)\n",
    "\n",
    "lecture_10 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_10_transcript.txt'\n",
    "file_14 = genai.upload_file(path=lecture_10)\n",
    "\n",
    "bias_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/bias_annotated.pdf'\n",
    "file_15 = genai.upload_file(path=bias_slides)\n",
    "\n",
    "ridge_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs/original_pdfs/ridge_annotated.pdf'\n",
    "file_16 = genai.upload_file(path=ridge_slides)\n",
    "\n",
    "lasso_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs/original_pdfs/lasso_annotated.pdf'\n",
    "file_17 = genai.upload_file(path=lasso_slides)\n",
    "\n",
    "midterm_review = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/midterm_review.pdf'\n",
    "file_18 = genai.upload_file(path=midterm_review)\n",
    "\n",
    "lecture_11 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_11_transcript.txt'\n",
    "file_19 = genai.upload_file(path=lecture_11)\n",
    "\n",
    "boosting_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/boosting.pdf'\n",
    "file_20 = genai.upload_file(path=boosting_slides)\n",
    "\n",
    "decision_trees_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/decisiontrees_annotated.pdf'\n",
    "file_21 = genai.upload_file(path=decision_trees_slides)\n",
    "\n",
    "# decision_trees_overfitting = '/Users/netraranga/Desktop/Projects/google_gemini/docs/decisiontrees_overfitting.pdf'\n",
    "# file_22 = genai.upload_file(path=decision_trees_overfitting)\n",
    "\n",
    "lecture_12 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_12_transcript.txt'\n",
    "file_23 = genai.upload_file(path=lecture_12)\n",
    "\n",
    "lecture_13 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_13_transcript.txt'\n",
    "file_24 = genai.upload_file(path=lecture_13)\n",
    "\n",
    "kmeans_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/kmeans_annotated.pdf'\n",
    "file_25 = genai.upload_file(path=kmeans_slides)\n",
    "\n",
    "em_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/em_annotated.pdf'\n",
    "file_26 = genai.upload_file(path=em_slides)\n",
    "\n",
    "pca_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/pca_annotated.pdf'\n",
    "file_27 = genai.upload_file(path=pca_slides)\n",
    "\n",
    "lecture_14 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_14_transcript.txt'\n",
    "file_28 = genai.upload_file(path=lecture_14)\n",
    "\n",
    "lecture_15 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_15_transcript.txt'\n",
    "file_29 = genai.upload_file(path=lecture_15)\n",
    "\n",
    "# ml_advice = '/Users/netraranga/Desktop/Projects/google_gemini/docs/ml_advice.pdf'\n",
    "# file_30 = genai.upload_file(path=ml_advice)\n",
    "\n",
    "lecture_16 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_16_transcript.txt'\n",
    "file_31 = genai.upload_file(path=lecture_16)\n",
    "\n",
    "learning_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/learning.pdf'\n",
    "file_32 = genai.upload_file(path=learning_slides)\n",
    "\n",
    "lecture_17 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_17_transcript.txt'\n",
    "file_33 = genai.upload_file(path=lecture_17)\n",
    "\n",
    "lecture_18 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_18_transcript.txt'\n",
    "file_34 = genai.upload_file(path=lecture_18)\n",
    "\n",
    "lecture_19 = '/Users/netraranga/Desktop/Projects/google_gemini/docs/transcripts/lecture_19_transcript.txt'\n",
    "file_35 = genai.upload_file(path=lecture_19)\n",
    "\n",
    "fairness_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/fairness_annotated.pdf'\n",
    "file_36 = genai.upload_file(path=fairness_slides)\n",
    "\n",
    "privacy_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/privacy_annotated.pdf'\n",
    "file_37 = genai.upload_file(path=privacy_slides)\n",
    "\n",
    "explanation_slides = '/Users/netraranga/Desktop/Projects/google_gemini/docs//original_pdfs/explainability_annotated.pdf'\n",
    "file_38 = genai.upload_file(path=explanation_slides)\n",
    "\n",
    "textbook = '/Users/netraranga/Desktop/Projects/google_gemini/docs/textbook.pdf'\n",
    "file_39 = genai.upload_file(path=textbook)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Queries from certain lectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_cache = create_cache(name='lecture_notes', contents=[file_1, file_2, file_4, file_5, file_7, file_8, file_10, file_11, file_12, file_13, file_14, file_15, file_16, file_17, file_18, file_19, file_20, file_21, file_23, file_24, file_25, file_26, file_27, file_28, file_29, file_31, file_32, file_33, file_34, file_35, file_39])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Neural Networks lectures in CS229 go beyond the textbook in several ways, incorporating more recent advancements and practical considerations. Here are a few examples of topics discussed in the lectures but not extensively covered in the textbook:\n",
      "\n",
      "1. **Specific Activation Functions and their Properties:** While the textbook introduces the concept of activation functions, the lectures delve into the specifics of various activation functions beyond the sigmoid and tanh.  The ReLU (Rectified Linear Unit) function and its variants (Leaky ReLU) are emphasized due to their prevalence and effectiveness in modern deep learning.  The lectures discuss their non-linearity, computational efficiency, and the impact of their choice on gradient calculations and optimization.  The textbook doesn't provide this level of detail on the properties and practical implications of specific activation functions.\n",
      "\n",
      "2. **Mini-Batch Gradient Descent and its Hardware Relation:** The lectures explicitly discuss mini-batch gradient descent, explaining its practical importance in deep learning due to its suitability for parallelization on GPUs.  The optimization speedups achievable through mini-batch processing, enabled by modern hardware capabilities, are highlighted.  The textbook might mention mini-batch, but it doesn't focus on its hardware implications and the practical trade-offs between batch size and computational efficiency.\n",
      "\n",
      "3. **Connection Between Neural Networks and Kernel Methods:**  The lectures draw a connection between the final layer of a neural network and kernel methods.  They explain that the penultimate layer of a neural network can be viewed as defining a learned feature map, similar to the fixed feature map in kernel methods.  This comparison highlights how neural networks offer greater flexibility by learning optimal features from data, whereas kernel methods rely on pre-defined kernels. The textbook may touch on kernel methods but doesn't explicitly make this connection to the internal workings of neural networks.\n",
      "\n",
      "\n",
      "In summary, the CS229 Neural Networks lectures emphasize the practical aspects of deep learning, focusing on the algorithmic choices and hardware considerations that are essential for implementing and scaling modern neural networks.  These practical details and connections to other machine learning concepts are often less thoroughly explored in the textbook.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_1 = generate_gemini_response(semester_cache, 'What is an example of something mentioned in the Neural Networks lecture that wasn not included in the textbook? Provide 2-3 specific examples')\n",
    "print(response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some questions that cover multiple lectures and require critical thinking, suitable for CS229 final exam review:\n",
      "\n",
      "**Question 1:**\n",
      "\n",
      "**Explain the key differences between discriminative and generative models in the context of supervised learning.  Illustrate these differences with examples from the course, including the assumptions made by each type of model, and discuss the advantages and disadvantages of each approach.**\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "Discriminative models directly model the conditional probability P(y|x), focusing on the decision boundary between classes.  They aim to predict y given x without explicitly modeling the underlying data distribution. Examples include logistic regression and support vector machines.  Their advantages include often higher accuracy in practice when the assumptions are reasonable and computational efficiency. However, discriminative models do not model p(x) and have limited ability to generate new samples or handle situations with missing data.\n",
      "\n",
      "\n",
      "Generative models, conversely, model the joint probability P(x,y) by modeling both P(x|y) and P(y).  From this, P(y|x) can be inferred using Bayes' theorem. Examples include Gaussian Discriminant Analysis (GDA) and Naive Bayes. Generative models have the advantage of being able to generate new data samples and better handle missing data. They also incorporate prior knowledge about the data distribution (e.g., assuming Gaussian distribution in GDA). A disadvantage is that they may require stronger assumptions about data distribution and may not achieve the same level of accuracy as discriminative models, especially when those stronger assumptions are incorrect.  Furthermore, they can be computationally more expensive to train than discriminative models.\n",
      "\n",
      "\n",
      "**Question 2:**\n",
      "\n",
      "**Compare and contrast the algorithms used to solve linear regression and logistic regression. Discuss the advantages and disadvantages of batch gradient descent, stochastic gradient descent, and Newton's method in these contexts.  Relate this to the concept of mini-batch gradient descent.**\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "Both linear regression (for continuous y) and logistic regression (for binary y) aim to find optimal parameters (θ) to minimize a cost function. However, they use different cost functions (mean squared error for linear regression, log-likelihood for logistic regression).\n",
      "\n",
      "*   **Batch Gradient Descent:** Computes the gradient using the entire dataset for each update. It's accurate but slow, especially for large datasets. Its advantages include guaranteed convergence (for convex cost functions) and simpler implementation.  Disadvantages are slower convergence and inability to process data in mini-batches or online.\n",
      "\n",
      "*   **Stochastic Gradient Descent (SGD):** Uses only one data point at a time for each update.  It's fast but noisy, leading to less precise but faster convergence.  Advantages include faster convergence and suitability for mini-batching and online learning. Disadvantages include noisy updates that may lead to oscillations and slower convergence than batch GD for very well-behaved data.\n",
      "\n",
      "*   **Mini-batch Gradient Descent:** A compromise between batch GD and SGD, using a small batch of data points for each update. It balances speed and accuracy by exploiting parallelism in hardware (like GPUs).  Advantages are faster convergence, and robustness to noisy updates and effective use of parallel computing. Disadvantages can still involve slower convergence than SGD if the batch sizes are very large and more complex implementation than SGD.\n",
      "\n",
      "*   **Newton's method:** Uses second-order derivatives (Hessian) to approximate the cost function locally using a quadratic function. It converges very fast but is computationally expensive, requiring the inversion of a large Hessian matrix for each iteration and its calculation can be prohibitive in higher dimensions. Advantages include very fast convergence when applicable. Disadvantages include high computational cost for each iteration and applicability is restricted to continuously twice-differentiable functions.  It's not generally used for large-scale machine learning problems.\n",
      "\n",
      "\n",
      "**Question 3:**\n",
      "\n",
      "**Explain the concept of the exponential family of distributions. Give three examples of distributions belonging to the exponential family and show how each satisfies the canonical form of the exponential family (highlighting relevant parameters and terms).  Explain how this concept is used in Generalized Linear Models (GLMs).**\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "The exponential family is a broad class of probability distributions that can be expressed in a canonical form:\n",
      "\n",
      "p(y; η) = b(y) exp(ηT(y) − a(η))\n",
      "\n",
      "where:\n",
      "\n",
      "*   η is the natural parameter.\n",
      "*   T(y) is the sufficient statistic (often T(y) = y).\n",
      "*   a(η) is the log-partition function, ensuring the distribution integrates/sums to 1.\n",
      "*   b(y) is the base measure.\n",
      "\n",
      "Examples:\n",
      "\n",
      "1.  **Bernoulli:** p(y; φ) = φy(1 − φ)1−y, where  φ is the probability of success.  In exponential family form: b(y) = 1, T(y) = y, η = log(φ/(1 − φ)), a(η) = log(1 + eη).\n",
      "2.  **Gaussian:** p(y; μ) = (1/√(2π))exp(−(y − μ)²/2), where μ is the mean and variance is 1. In exponential family form: b(y) = (1/√(2π))exp(−y²/2), T(y) = y, η = μ, a(η) = η²/2.\n",
      "3.  **Poisson:** p(y; λ) = (λy/y!)e−λ, where λ is the rate parameter. In exponential family form: b(y) = 1/y!, T(y) = y, η = log λ, a(η) = eη.\n",
      "\n",
      "\n",
      "GLMs use the exponential family by assuming the conditional distribution of the response variable y given the predictors x follows a distribution from the exponential family. The natural parameter η is modeled as a linear function of the predictors: η = θTx.  This framework provides a unified approach for various regression and classification problems by using different link functions connecting the natural parameter to the mean of the response distribution.\n",
      "\n",
      "\n",
      "**Question 4:**\n",
      "\n",
      "**Describe the bias-variance tradeoff in machine learning.  Explain how model complexity relates to bias and variance.  Illustrate the concept using examples and diagrams.  How does regularization address the bias-variance tradeoff?  Discuss the \"double descent\" phenomenon and its implications.**\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "The bias-variance tradeoff describes the balance between model underfitting (high bias) and overfitting (high variance).\n",
      "\n",
      "*   **Bias:** Represents the error introduced by approximating a complex real-world problem using a simpler model. High bias indicates the model is too simplistic and fails to capture the underlying patterns in the data.  A low-complexity model (e.g., linear regression on highly non-linear data) will have high bias.\n",
      "\n",
      "*   **Variance:** Represents the error introduced by the model's sensitivity to small fluctuations in the training data.  High variance indicates the model is too complex and overfits the training data, performing poorly on unseen data. A high-complexity model (e.g., high-degree polynomial on limited data) will have high variance.\n",
      "\n",
      "\n",
      "Model complexity (e.g., number of parameters, model degrees of freedom) affects the trade-off:\n",
      "\n",
      "*   Low complexity: High bias, low variance.\n",
      "*   High complexity: Low bias, high variance.\n",
      "\n",
      "\n",
      "The optimal model has a balance between bias and variance, minimizing the total prediction error. Regularization techniques (like L1 and L2 regularization) add penalty terms to the cost function, pushing towards simpler models and reducing variance (overfitting) at the cost of potentially increasing bias (underfitting) slightly.\n",
      "\n",
      "\n",
      "The \"double descent\" phenomenon is a recent observation where, as model complexity increases beyond the point of overfitting, the test error might decrease again. This contrasts with the classic U-shaped bias-variance curve.  The double descent phenomenon highlights the limitations of the classical bias-variance trade-off framework, particularly in the overparameterized regime (where the number of parameters exceeds the number of data points) where implicit regularization effects in the optimization process can lead to unexpected generalization improvements.\n",
      "\n",
      "\n",
      "**Question 5:**\n",
      "\n",
      "**Explain the concept of self-supervised learning.  Contrast it with supervised and unsupervised learning. Detail two self-supervised learning approaches, one for images and one for text, focusing on the pretext tasks, loss functions, and how the pretrained models are adapted for downstream tasks (including linear probing and fine-tuning).**\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "Self-supervised learning leverages unlabeled data to learn useful representations without explicit human-provided labels. It differs from:\n",
      "\n",
      "*   **Supervised Learning:** Requires labeled data (x, y pairs) to train the model for direct prediction of y from x.\n",
      "*   **Unsupervised Learning:** Uses unlabeled data (only x) to discover underlying structure or patterns without a specific prediction goal.\n",
      "\n",
      "\n",
      "Self-supervised learning constructs a *pretext task*—a proxy task derived from the input data itself—to generate pseudo-labels for training.  After pretraining, the learned representations are then adapted for downstream tasks.\n",
      "\n",
      "\n",
      "**Image Example (Contrastive Learning):**\n",
      "\n",
      "*   **Pretext Task:** Create pairs of augmented versions of the same image (positive pairs) and pairs of augmented versions of different images (negative pairs).  The loss function (e.g., SimCLR loss) encourages similar representations for positive pairs and dissimilar representations for negative pairs.\n",
      "*   **Downstream Task Adaptation:**  The pretrained feature extractor (the layers before the final classification layer in the neural network) is used, and either a linear probe (training only a new classifier on top) or fine-tuning (training the entire network with the pretrained weights as initialization) is used for downstream tasks like image classification or object detection.\n",
      "\n",
      "**Text Example (Next Word Prediction):**\n",
      "\n",
      "*   **Pretext Task:** Predict the next word in a sequence of words. This is a language modeling task. The input is a sequence of words (x1,...,xt), and the \"pseudo-label\" is the next word (xt+1). Loss functions are typically cross-entropy based.\n",
      "*   **Downstream Task Adaptation:** The pretrained word embeddings and language model layers (e.g., Transformer) are used, with either a linear probe (adding a classifier on top of the model's representation of the text) or fine-tuning (training the whole language model with the pretrained weights as initialization) is used for downstream tasks such as sentiment analysis, question answering, or machine translation.\n",
      "\n",
      "\n",
      "In both cases, the pretext task enables learning of general representations, which can then be efficiently adapted to various downstream tasks by utilizing either linear probing (faster, less accurate) or fine-tuning (slower, potentially more accurate).\n"
     ]
    }
   ],
   "source": [
    "response_3 = gemini_response(lecture_cache, 'Generate 3-5 questions using the concepts covered across lectures. Ask questions that require students to think critically about different concepts and that would be beneificial for them to review prior to the final exam. Provide the question followed by the answer.')\n",
    "print(response_3.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lecture on KMeans included several important concepts and nuanced points not explicitly detailed in the provided notes.  Here are some specific examples:\n",
      "\n",
      "1. **The impact of initialization on KMeans performance:** While the notes mention initialization, the lecture emphasized the significance of choosing good initial cluster centers. The instructor highlighted how poor initialization (placing centers far from data clusters) can lead to suboptimal solutions, resulting in the algorithm getting \"stuck\" in a poor local minimum.  The lecture contrasted this with smarter initialization techniques like k-means++, which, through density estimation, helps spread initial centers, improving the likelihood of finding a good solution.  The notes don't delve into this comparative analysis of different initialization strategies and their impact on the final result's quality.\n",
      "\n",
      "\n",
      "2. **The non-guaranteed convergence to a global minimum:** The notes state that the KMeans algorithm converges, but the lecture explicitly addressed the fact that this convergence is not guaranteed to be a *global* minimum.  The instructor illustrated how KMeans can get trapped in local minima, particularly if initial cluster assignments are poorly chosen or the data has disparate cluster sizes or significant overlap between clusters. The lecture discussed the computational complexity of finding the global minimum (NP-hard), explaining why a heuristic algorithm like KMeans is necessary, and why local convergence is often the best achievable result. The notes lack this explicit discussion of the limitations of KMeans in achieving global optimality.\n",
      "\n",
      "\n",
      "3. **Practical considerations for choosing the number of clusters (K):** The notes briefly mention the parameter K (the number of clusters), but the lecture spent considerable time discussing the challenges of choosing an appropriate value for K.  The instructor emphasized that there's no single \"right\" answer for K; it's a modeling decision. The choice of K depends on the data's structure and the goals of the clustering.  The lecture highlighted the tradeoffs: choosing too few clusters lumps together dissimilar data points, while choosing too many creates overly granular, less interpretable clusters.  The notes lack this qualitative discussion, presenting only a simple parameter without explicitly discussing its significance.\n",
      "\n",
      "\n",
      "4. **Soft versus hard cluster assignments:** Although the notes eventually introduce soft assignments in the context of Gaussian Mixture Models, the lecture contrasted this with the hard assignments used in the basic k-means algorithm. The difference was highlighted: k-means assigns each data point to precisely one cluster, whereas soft assignments assign probabilities of membership to multiple clusters. The notes do not emphasize this distinction or contrast the two assignment strategies' characteristics.\n",
      "\n",
      "\n",
      "5. **Use of k-means for data exploration:**  The lecture highlighted that, beyond its use as a standalone clustering algorithm, k-means serves as a valuable tool for exploratory data analysis. By running k-means on a dataset, one can gain insights into data structure, identify potential outliers, and visualize clusters to understand data groupings before applying more complex modeling techniques. This important use-case of KMeans for initial data exploration and visualization is largely absent from the notes.\n",
      "\n",
      "In summary, while the lecture notes provide a mathematical foundation for KMeans, the lecture significantly expanded upon this by discussing the practical considerations, limitations, and nuances of the algorithm's application, which are not comprehensively covered in the provided notes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_4 = gemini_response(lecture_cache, 'What are some key concepts covered in the KMeans lecture that are not covered in the notes? Be very specific in the points you generate.')\n",
    "print(response_4.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca_comparison = genai.upload_file(path='/Users/netraranga/Desktop/Projects/google_gemini/docs/consolidated/combined_pca_slides.pdf')\n",
    "pca_cache = create_cache(name='pca', contents=pca_comparison)\n",
    "response_pca= gemini_response(pca_cache, 'What are the differences between the PCA original slides and the annotated slides? Be specific in the type of differences you generate, do not include generic points like the annotated slides have more information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The differences between the original and annotated PCA slides from Stanford CS229 lie primarily in the addition of handwritten annotations on the latter.  These annotations offer further explanation and context, clarifying various aspects of the PCA algorithm and its applications. Here's a breakdown of the specific types of added information:\n",
      "\n",
      "1. **Elaboration on Applications:** The annotated slides provide more detail on the practical use cases of PCA.  For instance, the word embedding example is enriched with labels highlighting clusters representing \"names,\" \"months,\" \"states/cities,\" and \"middle initials,\"  providing a more concrete understanding of how PCA organizes high-dimensional data in this context.\n",
      "\n",
      "2. **Mathematical Details and Derivations:** Several slides include handwritten steps clarifying mathematical computations.  This includes detailing the reconstruction error calculation, explicitly showing how it relates to the covariance matrix and eigenvectors.  The annotations add intermediate steps and explanations to make the derivations clearer.\n",
      "\n",
      "3. **Conceptual Clarifications:**  Handwritten notes are added to explain key concepts. For example, diagrams illustrate the projection process more intuitively by showing how data points are projected onto the principal components and the reconstruction error minimized.  Additional comments explain the concept of choosing the \"best\" projection vector and the relationship to reconstruction error.\n",
      "\n",
      "4. **Algorithm Clarification:**  The annotations provide insights into the steps of the PCA algorithm. They offer additional descriptions of the recentering step and the role of the covariance matrix.  Furthermore, explanations of SVD’s role in efficient computation are also presented.\n",
      "\n",
      "5. **Relationship between Concepts:** Annotations link different concepts together, for example, explicitly drawing the connection between minimizing reconstruction error and finding the eigenvectors corresponding to the smallest eigenvalues.\n",
      "\n",
      "In short, the annotations do not simply add information but rather contextualize the information already presented on the original slides, making the material more understandable and providing a deeper grasp of the underlying concepts and calculations within the PCA algorithm.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response_pca.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in caching.CachedContent.list():\n",
    "  #c.delete()\n",
    "  print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in caching.CachedContent.list():\n",
    "#   print(c) #Slide 1 to 15 are 166273 tokens\n",
    "  #Slide 1 to 38 are 500798\n",
    "\n",
    "for c in caching.CachedContent.list():\n",
    "    c.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO ODO\n",
    "#-combines all of the slide contents into one file so it passes the cache min size limit\n",
    "#Determine with chatgpt what are good questions - study guides on certain lectures and concepts\n",
    "#Identify the differece between annotated notes and regular notes\n",
    "#Create a study guide that is grounded in the lecture nad pulls additional key concepts from the notes\n",
    "#Generate some python questions for certain lectures for the application piece \n",
    "#Watch a certain video and see if the LLM can retrieve the specific fact or instnce referenced in the video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
