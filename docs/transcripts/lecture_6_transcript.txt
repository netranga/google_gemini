Stanford CS229 Machine Learning I Naive Bayes, Laplace Smoothing I 2022 I Lecture 6

so I guess uh last time we talked about um gaussian discrimination of analysis I'll have a very brief review and um and talk about some of the remaining points that I didn't get time to mention last time and then I'm going to move on to another case about spam filtering where we're gonna have uh discrete acts instead of continuous acts so last time we talked about gaussian discriminal analysis and the general idea is that you first model P of x given y and p of Y and what we do is that we say p of x given Y is a gaussian for y is 1 or y0 okay I guess I need to remember right bigger so x given Y is equals to zero is from some gaussian distribution with mu zero and covalent Sigma and x given Y is one is from some gaussian distribution between one and sigma and recall that we have this uh illustrative situations where you have some examples like this these are positive samples there's an active examples and you kind of believe that each of this subpopulation come from a gaussian distribution with different means but with the same covers so that's the methodology we have that's the starting point we have last time and then what we do is that we say after you have this probabilistic model we also we have this P of Y is 1 is equals to V after you have the probabilistic model then you can learn the probabilistic model from data by Noe so so we learn by maximize we take the arc Max over our parameters Phi Mu 0 mu 1 and sigma and we take the r The Arc Max of the the log likelihood you know it's the same as the likelihood you know Mac Arc Max of the log likelihood is the same as the max of the likelihood so I'm just writing the log likelihood which is the log of the probability um of x i given y i plus the sum of the log of the probability y under I guess you know technically under this parameter V mu 1 mu 2 Sigma and I guess technically I don't have to write video just because it doesn't depend on feet okay so that's the the methodology and then we skipped a lot of steps because these are homework questions um and and we told uh and they told you that the solution of this mle problem you can analytically solve it in this case because this objective function is nice enough it's a it's kind of like a quadratic function so you can solve the uh the maximization problem analytically and you get um some formula for uh these quantities right so the formula for Phi was something like um something of this form of this indicator function which is here basically the numerator here is the number of positive examples and divided by the total number of examples and we also have formulas for Mu 1 and mu 2 so mu 1 was something like the average of x i in the positive group in the positive examples so you take the average you divide by the total number of positive examples and you can also have mu zero and also Sigma so each of these has a formula which is a Formula or that depends on the data so this is how you learn the parameters uh from uh from the data and then we talk about given these parameters how do you do the test how do you do the the inference or how do you do the um the prediction right like when you get we already learned these parameters now you are given an example X how do you predict y using the parameters and given ducts right so and we said that the you're trying to compute the arc Max over the two choice of y's and you want to look at which choice of Y gives you the largest probability given X and given the parameter and these parameters are the the solutions you have computed from these formulas right so you compute which one oh sorry not bad this is one you complete which one has the largest probability and also we discussed that you know to know this actually you just have to you know in some sense you you have a decision boundary so uh the decision boundary between the two choices is those cases where is this uh those axes were that is where these two probabilities are exactly the same so where the P of Y is one given X is equal to P of y 0 given X is equal to a half right this is the decision boundary and we have computed the decision boundary which turns out to be something like a linear function so decision boundary turns out that this P of Y is equal to one given X this is equals to um this is equals to something like I guess maybe I'll just directly write out the decision boundary we found out that the decision boundary is the set of X such that uh Theta transpose X Plus Theta 0 is equals to zero where Theta and Theta zero are functions of the parameters that you have learned there are some like um specific formulas uh that um to describe this Theta instead of zero which are the homework questions but Theta says there are some functions of Phi Mu 0 mu 1 and sigma so that's why when you really make the prediction what you do is you say you find out this decision boundary this is the final of X such that Theta transpose X plus zero is equal to zero and then if this quantity is bigger than zero then you'd say it's positive and if this quantity is less than zero you say it's negative this is very quick you know Five Minutes review of the last lecture very very quick any questions if we have multiple labels yeah there will be a decision boundary for multiple labels so um and um you you basically just compute the decision boundary using this methodology you're gonna get a foreign I think if you have the same covariance the decision boundary is still linear um but if you have different models you may have different type of decisions you know it wouldn't be a half because you know you're gonna have multiple choices wise here right so so your decision module will be like uh I think I think it's going to be more complicated so you have to really decide you know when suppose this is like zero one two three or four right so then you really have to just really literally solve this right you have to know when is the case what is the region such that the maximizer of this is equals to say label two so that's that's gonna be something that describes by some linear region some kind of linear boundaries but it's going to be more of conflicted fall you know it could be depending on what probabilistic model you define right so I think if stigma are all the same the capital Sigma all the same I think it's going to be linear and if you have different segment even you have two classes but just take my one and sigma 2 then I know it's as a fact it's quadratic I saw some other questions as well any questions are welcome okay so this is just give you a quick overview a quick review of what we have we did last time um and and you will see that our new when we discussed the new problem we are going to have from similar methodology you define a probabilistic model you solve the mle you get some formula and then like you get some parameters and they use those parameters to predict what's the most likely wise okay so but before going to that I'm going to discuss one important thing which you know many people actually ask in the last lecture at the end of the last lecture they are great questions so the question I'm going to discuss next is that you know why this is different from what what we have seen in the first two weeks right so at the end of the day you have a decision boundary which is linear right so basically at least superficially it sounds like you are using a linear model to decide you are going to use this linear function to decide whether it's positive negative right you compare this with zero and if it's paused if large zero is positive otherwise is negative and this is the same as the the logistic regression that Chris talked about in the first two weeks so why these are why this is different uh from from the uh the so-called discriminative methods that Chris talk about so I think um here is the the way I think about this so um so so if you have GDA on the left and suppose you have like a logistic regression on the right um so first of all in terms of the Assumption they are different so for GDA I guess I wrote The Assumption there so you maybe I just wrote here again so this is gaussian and this is also gaussian something like this and you also have Y which is from Bernoulli and when you do logistic regression then you just literally say that P of Y is equal to one given X you your probabilistic model assumes that this has the form uh one over one plus e to the minus Theta transpose X and and recall that here when you write this in places we are saying that X zero is one right that's quiz assumption like in the first two weeks but suppose you don't have that assumption you're going to have another suppose X doesn't contain X zero then you need to have another set of zero something like this right if you contain the x0 then you are going to have a clean form but they are the same right because you know if suppose let's say for today let's say x uh you drop that convention so exciting contain x 0 then you you're gonna write it like this right and this exactly match the form here so so basically you can see that one thing is that for GDA you assume this bunch of things and you and you found out that it implies that y given X has this form right so recall that okay maybe I didn't write this but like this is equals to one over one plus e to the minus Theta transpose X plus zero right so let's say how do I write this so P of Y is equals 1 given X right so this is a this is a conclusion that we got from the probabilistic modeling right from our mathematical derivation right we conclude that y given X should have this form and in logistic regression you just directly assume it so in in GDA that's a conclusion in some sense right and and I guess you know we have to stress this many times so here you you you model The Joint probability distribution because given if you know x given Y and you know why then you also have the density of X comma y right but here you only always have y given X but you never have anything about X we only have the conditional probability so in some sense the kind of like the the main difference between these two is that on the left hand side you have more assumptions so I think this is the key you have stronger assumptions on the left hand side than on the right hand side and and in some sense the color of the journal um uh the general kind of like a phenomenal General principle about you know how do you model what what part of the world you want to model in your machine learning algorithm right so do you want to model both or you want to model only YG Max right so how do you decide when what how much you want to model the world so the the kind of the pros and cons of the the kind of the trade-off is that if you have more assumptions then typically if if this is also correct assumption if your assumption is correct then uh this pretty much implies you have better performance of course this is a not mathematical statement but I think it's kind of reasonably intuitive because if you have more assumption it means that you you are using your prior knowledge about the world right so here you are using a prior knowledge that x given Y is gaussian and if you use that prior knowledge typically if you use it correctly then you should have better performance like a like you know if I tell you everything then of course you have better performance but if I tell you a little more I suppose I'll tell you everything about mu and sigma and of course your performance is the best right if you tell you a little more about X then you should always have a little better performance so but the problem is with this is that so this is the the good thing about more assumptions but the risk is that you may have wrong assumptions right you might have make a wrong or approximate or kind of like not exactly correct assumptions right right so if you make wrong assumptions then make in most of the cases probably GDA will will be worse out for example what if the data are not really gaussian right so what if this doesn't look gaussian at all and you still make assumption that they are gaussian then you you're gonna have like a um um worse worse performance so and um another thing I want to stress is that even though superficially you see the form here is the same right so Theta transpose so when you make the GDA assumptions you get this y given X of this form but suppose you go through this left hand side and get this and you numerical you compute the source Theta so Theta instead of zero it would it wouldn't be the same Theta instead of zero um as you what you computed from logistic regression so you're gonna get a different side of like Theta and say zero so that's why it makes a difference so so this is a zero from Logistics regression is how do you get it you just directly fake your leading model using logistic regression that's how you gonna say that zero but if you go from here to here then you're gonna first learn the MU the sigma uh the MU 1 mu two a mu zero mu 1 Sigma and then you compute Theta using mu zero mu one Sigma so so that will result in a different set of theta instead of zero that's why it does make a difference um and whether it's better or not I think as I said basically depends on whether assumptions are are correct or how correct your assumptions are probably you never have can have like exactly correct assumption but maybe or something we all sometimes approximately correct then you should do GDA if your assumption is just completely wrong then you probably should do logistical question any questions so far um like we have a different data in Mortal cases so I will just be different again like is it because we're Computing in that equation of the out there directly and using those formulas and then you compute Theta as a function of mu and sigma right so so it's definitely a different process right you're not getting the Theta instead of zero in these two cases using the same process right so in one process it's kind of Securities right you first have to compute mu and then say it's data and the other case you just directly fit this using a numerical algorithm like a reading descent right is it possible that you can get a different decoration X but we have let's say the premise our shoes yep definitely you you may have if you change your assumption you may have a different equation for p of Magnum X and there's an actual interesting point I'm going to mention maybe maybe after I answer are there any other questions okay so then there's actually another interesting point I'm going to mention next is that even you change this actually it's possible you change the Assumption you still have the same formula but on the right hand side so so you can have both you can like if you change the Assumption maybe you still have the same formula maybe you still have a different formula so so this is a comfort case which I think that I think what I said is actually even more surprising right so you change the Assumption on the left hand side you still have the same y given X on the right hand side at least the same form so so this is a example so I guess here I'm looking at uh I think one dimensional so suppose X is one dimensional just you know it's not very important like exactly so so suppose you do x given Y is one is from the so-called poisson distribution um wait how do I personally distribution sorry and then you do this I must misspell this this works sorry um but you do this so you change it's no longer a gaussian it's um some other distribution and um oh I guess X is a is in because this is a personal description so it has to be integer something like integer um and and then you still have y is uh P of Y is one is equals to V right so we change our assumption and then this actually still implies that P of Y given X has this form one plus e to the minus Theta transpose X plus zero so still the form looks similar of course you know if you really numerical computers it would be a different state and say a zero because Theta and say zero here I'm just writing them as a as a generic variable right but actually they are they are they are they have meanings right Theta is a function of Lambda one and lambda zero and so that zero is also the function of unknown one and lambda zero so the form is still same um but you could have like um numerical if you if you use this model in terms of instead of GDA you're gonna get different state and zero so so a linear form doesn't necessarily mean everything right uh it also you know it also depends on how you learn this linear function so you have actually here we have already three ways to learn it where you can use this this model we can use the GDA we can use the largest equation they will all give you different Theta and Theta 0 as a as a final result and which one will be better I think you know if you compare GDA with this poisson version of the the standard floating algorithm then I guess the answer would be that you know probably depends most on whether which assumption is more correct more likely to be correct of course there's also some type thing here because here your your thing is like and but suppose you have a different model which also deal with like R like you know if we basically I'm saying like when you compare it okay forget about this axis in anything I suppose you don't care about that differences then which model will work better probably depends on whether your assumption is correct or not or which assumption is more likely to be correct and and when you compare the general following algorithm with the discriminant one I think it's the same thing right so if your generative assumption is more is likely to be correct then you should gain something from it otherwise maybe you should just use long distal question I think in some sense like at these days you know if you look at um um and also another thing is that um okay so um so and and also um maybe a little more General discussion is that um in some sense your model has two source of like a so in some sense they are like in some sense you have two sources of knowledge the model learns from two things in some sense so one thing is that your assumption and the other thing is you have data right so the assumptions means like how do I probabilistically model all of these qualities and data is really just what you see right so if you have more data of course it's good right if you have more assumption if the assumption is correct then that's good um but on the other hand suppose if for example you have already a lot of data then you have less need to use prior knowledge because your data already are very telling right so like the data is kind of sufficient for you to to kind of like extract whatever information you want then you don't really need to use prior knowledge because if you use prior knowledge you always have a risk to use it wrongly right so so basically in some sense I think the more than our trend is that you know like we're going to talk about the new artworks and deep learning um so in in two lectures the Modern Trend is that now we're in this setting that we have more and more data for many applications and and and then that's why the modern techniques you know like deep learning new artworks those techniques you know use fewer and fewer assumptions about our data so just because you know it's not really worth it right so like I have to think about how do I model my images right suppose you apply these two images right you have to have a model for X for the file image you know does it really work the effort to do that you know probably not you know of course it depends on cases but if you just want to First Cut results you don't have to model your X because modern X is very difficult you know it's very challenging and you may make mistakes so so you probably should just directly go for the more pull off like a discriminative analysis type of approach right so you just directly model y given XP accessor image right but in the in the um in some other applications for example if you think about medical applications right all or some of the other applications of machine learning where you don't have enough data in those cases I think still you have to kind of use as much prior knowledge as possible um and actually many times people even do even much more complicated modeling of your ex right so maybe you can use some other more advanced ways to model your ex because you know how does each chord of X what each coordinate of X means and you know what's their relationship and you put all of this prior knowledge into your modeling for x and then you get finally some wagon X using this machinery and then you predict and that's that's more likely to work so um yeah I guess that's the that's another thing is that if you use more assumptions then you are specializing to your application right that's another reason probably why in the modern time like people somehow don't do this kind of use these paranoids that often because I guess probably heard of me right works and one of the kind of magic about it is that it works for many many cases without much customization right if you use prior assumptions you'll use more assumptions you you are you are you have to kind of like use different assumptions for different applications because for different applications their data probably have different structures you so you have to do it one by one and that's actually what people do um a lot of times right so they look at their domain their questions and and study the structure um but these days you know like as you'll see like when people use device works because you have enough data and you just drop assumptions and you make a model kind of very general not very not specialized at all and you supply it to everything just without thinking much about what the data look like so uh yeah so I guess that's a you know you will talk more about new artworks but this is the a preview or kind of connections to what we're going to talk about next um another kind of like a big picture is that you know one of the reasons as I said you know in some sense I I was saying that you know this kind of GDA analysis is not used that often um at least not used as often as before right before you have to use these kind of things and now you can at least you have the choice to uh to try something like works and and because you have more data so um but still I think if you are able to model the ax you know in many cases you can do better so and also in some cases you just don't have why so even even for example when you have images right so sometimes you don't have why at all you just only have X and in those cases you just really have to model X because that's the only place where you can get information from it and another possibility another cases is um uh we are going to talk about languages right like uh especially like this lecture I'm going to talk about language as well but you know later when we're also going to talk about languages you know solving language problems with new artworks and they are you know you're just getting a kind of a um like a lot of like text from the from the internet right and there's no labels there's no way nobody's tell you like which like which web page is is about which right so you just have like raw attacks you don't only have acts and in those cases you really have to model X3 there's no way you can get around this so so so so still like modeling acts is is very important it's just like um for example it's less important for certain kind of applications because we have more data any questions thank you Okay cool so I guess so now I'm going to move on to um the next uh uh the next uh question so um I think the the question is this the so-called one on classification so you are trying to understand whether a piece of tax is a spam email or not right so like you have an email spam filter and you want to know whether your email is a spam or not and we're still going to do generative learning algorithm and and we're gonna have this great ax so this is another example how you do this challenge for learning algorithm your model X given Y and you execute this like a pipeline in some sense and and learn something out of it okay so um so the first thing so I'm going to you know get into a little more details you know how do we really approach this question so the first thing you probably have to what okay these are just examples um right so I guess um um so the first question uh to a first question we have to do to approach this is that how do you represent a text right attacks are symbols right like a ABCD right so um you need to make a numerical at least to to to to to to make the computer recognize them right so to in some in some naive way at least right so so the question first question is how do you change the tax to some you know acts maybe you call this feature right this is a um You can call this feature vector or or representation or something like that so you want to change this to X in some Dimension d e and then you model X so so first question is how to represent text so there are many ways to represent text so um so the way I'm going to tell you is a very naive way this is like a um a probably I shouldn't say naive like but this is like a very simple way like uh on this stage we're gonna have like a you know if you really deal with the taxi you're gonna use a more advanced deep learning based approach which we're going to cover a little bit in the in probably three or five weeks so um so this way so here the way that we do it is very simple so what you do is that um maybe I should have some so what you do is you say you have you first look at the vocabulary suppose you have a vocabulary of maybe 10K words so and suppose you say you list all of these words in your sequence based on alphabetical order I guess if if you open up a dictionary the first letter the first word is for probably always a I think the second you know are coming to some dictionary the second word is this word otherwalk I think it's a kind of animal um and the third one is uh the wolf I think it's another kind of animal something like this and then you list all the words you know and maybe at some point you're gonna have a word book and eventually the last word I think in many of the dictionary is this this thing I don't even know how to I don't even remember how to pronounce it I think I used to know when the first time I teach this lecture and then I forgot after a few years um anyway so you listed all of this and and then you say that um um you have suppose you have a piece of text right so maybe say suppose you have a sentence or maybe an email suppose this email just has one sentence uh something like I buy a book so you want to turn this into a vector and how do you turn it into a vector so the way that we do it here is that you turn it in a vector that is of dimension um this axis of Dimension d where let's say d is the size of the vocabulary T is equals to the number of words so and then on this Vector that represents um piece of uh this sentence is going to be a zero one vector so X is actually you know it's really from 0 1 to the power d there are only two choices and every entry so you have so many entries and what you do is you say if this word shows up in the sentence then you have entry one in this entry so the word a shows up in the sentence then I have one here and the artwork doesn't show up in that sense sometimes I have zero and then at some point you have this corresponding entry book the book shows up in the sentence I have one here and maybe somewhere I think there's a word probably I here in the list and then that one would also have a one that that word also corresponds to one and for all the other entries that all the other words that don't show up uh in the sentence you just fill in zero for the corresponding entries so and you call this x your representation or your feature Vector uh for uh for for this email for this sentence so basically technically I'm just going to say that x i is equals to one if and only if uh the ith word occurs in the email so there are actually many other ways to encode even before you know using deep learning techniques um but but this is probably one of the simplest one and you can see that this this representation of the sentence is in some sense like a super um I guess like simplistic because for example you don't care about the orders of the world right suppose you have another sentence I a book by right that sentence still has the same representation just exactly the same and it doesn't care about the frequency of the word for example suppose I have a sentence I buy a book and a a kind of pencil then of course the the representation will change but this two word a and a here you know will be kind of you will still have a work according one here it's because the word a shows up in this sentence but you don't care about how many times the word a shows up in the sentence so so you don't care about the frequency of the words in this sentence you just care about whether each word shows up so um and you know there are many other proper issues with this like uh what else um yeah I guess probably these two are the most important thing where you don't have to order and you don't have you don't care about the frequency um but that's what we're going to deal with because this is easy and you can um somewhere kind of like do all of the math uh with this kind of model Okay so now what's the next question the next question is that we need to build a generative model for X gaming y and we need to have a model for y and then we do mle and we solve the parameter so and so forth so [Applause] so basically I think I can just erase this and now I'm going to redefine experience right so that's what I'm gonna do okay so let's take um and how do you proceed so now because this x now is a binary Vector it's only taking 0 and 1. so you need a distribution that can generate binary vectors right so you cannot use gaussian here um and the kind of the techniques that we are using here is the so-called naive layers so what does this mean is that um this means that you just assume Max 1 up to XD or maybe I'll erase this you know are independent conditions on y so given y you just independently draw exponent after x d of course this is not realistic right this is definitely not exact you know how realistic it is I think that's subjective but at least this is not exactly how people generate emails right you are not saying that I'm going to generate a spam filter I first decide I'm going to generate generate a spamming email and the first thing I decide is I decide why and then after I decide why I just trying to reward randomly like like right that's probably not what how how people write spam emails right so and also that's not how people write the the usual like good emails so um but it turns out that many in many cases these kind of assumptions are pretty um already pretty good so in the homework actually um actually we haven't decided whether we are going to include that homework question but at least you know there are there are cases where um you can see this kind of things can be very effective right actually if you just really use this model even you make this kind of crazy assumption and you learn some parameters and use this model to expect on classified spams I think you're going to get more than 90 accuracy maybe these days you know as of like a 20 2012 maybe it's not that effective because all the the spammers they are they are adversarial right so they know what your prediction algorithm is they can change their algorithm to kind of fool you but but at least this is a reasonable one if you go back 10 years ago for sure like so um so so it's kind of interesting right so even you make you know obviously kind of not exactly the right assumption but sometimes you can still because the Assumption problems somewhat correct you know to some extent you can still get a useful kind of outcome from it and for our purpose I think here um I guess you know to some extent I'm not really that um I don't care that much about assumptions I think it's more like I'm trying to demonstrate the methodology um like how how do you like I just want to give a new example where you execute this probabilistic model this flow this this pipeline right um and show how how to solve them one example right so the question is how how does the text the lens of the text matter right so here um in so one interesting about this is that in this representation the lens of the text doesn't really matter you can encode any lens into a vector of Dimension d which is you know you can say this is a good thing or bad thing you know um um but but so so basically you can encode any lens um any sentence or any documents into a single vector and how do you how to decide which uh what is the window size you know here I don't think it matters that much maybe you just take entire email um yeah of course you know if you change the the the I think I think you should you should just include the entire email because that's the the unit you are working with right like for every email you are classifying whether it's a Spam or not you are not classifying it whether a sentence is of spam or not so that's why you treat an email as a single example okay all right x12x3. does that mean that like if you want like One X appears that tells us nothing about whether another X is likely to appears or does it mean that all X's are equally likely to appear like one y material um I think it's it's more about the it's more the first okay so so because I do like certainly not all the words are equally likely to appear right so I so basically I'm assuming that given y given you have decided that which whether this is spam email or not um every word you know is is uh is they are they are independent with each other but but they they may have different kind of probability [Applause] okay so I guess let me proceed so so what does this really mean okay now I'm going to do some math right so to kind of expand this and parametricize it so this really means that you have this P of X1 give an XT given y you write this as P of X1 given y times P of x d given y and now I I can I just need to parameterize each of this problem distribution by some parameter so and if you think about this what is this uh this is this is really just the distribution of Bernoulli distribution because X I can only have two Choice zero one so basically you just have to describe this probability by two numbers right I'm actually by one number right so so basically what you do is that you parametrize parameters of the model is you have fee on say j so there's some in the index which I'm going to explain in a moment you say this is x j is equals to one given Y is equal to one so basically for y is equal to one you're asking you know what's the probability of x j is one right and you you denote that probability as this and then once you have this you know the probability of x j is equals to zero given Y is 1 is going to be one minus v j given Y is one and this uh this um this thing is just a notation it's not like a um it's equally the same if I write J comma one uh I just like I write this subscript because it's a little bit more intuitive but it but I just I just need an index in some sense doesn't make sense so basically for every J I'm going to have a parameter for either J and and one I'm gonna have a parameter that's called Phi and this parameter is in 0 1. right so and and this parameter describe this distribution and for y is zero I'm also going to have a parameter so so I'm going to write this as J given y zero so this is the parameter for the distribution of x j given y zero yes it's between 0 and 1. this is between this is inter this is uh the bracket uh the hardback okay okay so so basically I'm saying that if I this with this parameter and this parameter I can describe all the all of this I can I can write out all of these numbers because I I describe all the quantities right because for example what is p of x j given actually 0 given y zero this is going to be equals to 1 minus V j y is zero okay so and then I also have need to have a a like something for this to parameterize the description of Y we call that we call this V before right in in the in the GDA case and not just for the for the sake of like a distinguishing it from the other fees I'm going to call it v y but this is the this serves as the same row as fee uh in before in a GDA case any questions foreign [Applause] of the parameters and you and this is the probability of the data given the parameters so what are the parameters the parameters of v y is one of the parameters and also all of these fees VJ given y 0 and v j given Y is one so basically I have V1 given y zero up to 3D given y 0 and then fee one given Y is one to Phi d y is y right these are all your parameters so on my likelihood you know here there are two um uh waste I can expand this so the first thing is that because by definition the likelihood is the product of the likelihood of each of the example because your all your examples are independent this is not naive place yet this is the examples I implement so you can just write this as probability of x i y i given all the parameters and here if you are careful then you know uh okay so it's basically all the time I guess all the parameters I'm going to write I think in my notes the notation is this so but I think this really just means this is uh just a convenience notation that denotes all of the parameters this is the same as this um okay and then you first you so so far is the same as the GDA and then you can also do the chain rule to make this x i given y i conditional parameters okay given the parameters and then times P of Y I gave me the parameters so when I use dot dot dots I just mean all the parameters of course sometimes you can drop some parameters because they are not they don't matter and then I'm going to again factorize in the dimension of X Y now I'm going to use my G like this naive face assumption right which is the factorization across the coordinates of X right we call that this is x sub I is the coordinate of X the superscript I is the iso example so so what I'm going to do is that I'm going to use that assumption for each of the examples so what I'm going to cut is uh I from 1 to n I'm going to put this in front just to make a simple make it easier um and if I'm careful I I only have to write Phi right here because the description y only depends on v y and then I'm going to factorize this thing across the coordinates so I'm going to have a product um across the corners have D coordinates and then I have P of x sub j i is the gist coordinate of the ice example given the label for that Y is example and all the parameters V j y just means I guess maybe I'll just uh so v j y this just means a shorthand for this family of parameters as a mix maybe I was just sorry this is a bad notation um so v j y just means a shorthand for this collection of parameters Okay so [Applause] um [Music] so you mean here um yeah that's right so right if you only look at Yep this J and this J yes you only care about the you just care about the j under there's the fixed j under the Y is 0 and Y Square that's great yeah right so so so we have two times that we factorize this probability so so the first time is here so here I'm using the fact that all the all the examples are independently sampled so that's why I I see the drawn probability is the overall examples is is the product of the probabilities of each of the example I know for every example um of course I first do the chain rule to get x given y That's and then I factorize this one into this product again and this is the level of the knife phase this is using native phase Okay cool so and then so I guess you can expect you know what we're going to do we're going to maximize this and we know that if you add maximize this it's the same as maximize suppose that's called this L right so max minus L this is the same as Arc Max of log l and log l is going to be a sum of the log of this probability so log l will be a sum you turn all of this to sum and you have to log in in front of the terms right so you have log p y I can v y Plus here we have a double sum sum over I from one to n sum over J from 1 to D and the log of this and then you analytically plug in all of this right so for example for this one you can you know what is this right this is equals to what is this this is equals to v y if y i is one if this is equal to one minus v y if Y is equal to zero right so and for each of this you can write them as a formula or for of the data and the and the parameters and then you you do the maximum likelihood so the maximum likelihood uh I guess I'll also just tell you the solution um so if you write look at the gradient of l is zero right this is the right this is the con it's a sufficient it's a necessary condition for for you you are being failed to be a maximizer okay I should write why and you compute this gradient and you solve this equation this is a family of equations and then this solving it gives um some formulas for the parameters on your recovering so the so the final solution will look like this v y is equals to um right this is pretty intuitive this is the fraction of positive examples actually it's the same formula as we have seen for the GDA and then fee j y is one this is the probability to see XJ is one given Y is one it turns out that this is also something simple so you look at maybe let me write down the formula and then interpret it so what is this numerator this is the number of occurrences of ice word right it's only one when the eighth word the ice example contains the J's word right so the J's word right this is x j i is one means that the J's word shows up in the ith example in and and you also require that the iso example is positive example so basically the sum is the total number of currencies of jth word in positive examples and and this is the number of positive examples so you can see that you know even though we have done a lot of like calculation and modeling identify the the formula is pretty intuitive you are in sometimes just counting it's kind of like something about counting right you're doing some statistics right you're counting how many times the jsword shows up in positive examples and you divide that by how many total parts of examples they are right so for example suppose like the word book shows up in 10 positive examples and they are they are like a medium positive samples that means this is five over a minute which kind of means that book doesn't seem to have much correlation with past examples if it is this number is smaller if this number is small it means that it's unlikely to see the J's word get impossible so when it's unlikely you know it's unlikely because partly because the data they don't show up so okay and for the negative examples is the same so you just the right to um it's kind of symmetric so feel of J given y 0 is equals to on something like I guess you can guess what the what it is right basically just change every the value of y to zero and any questions [Applause] okay so um okay so basically we are done with this um at least uh we are almost done there's one small thing that we have to address but in terms of the isometer parameter we are done so and we got a parameter another prediction time as before right so we'll do the prediction as as usual you want to compute P of Y is 1 given X right and if this number is larger than 0.5 then you say this is a positive example if this number is less than 0.5 you say this is an active example so we have to compute this and how do we compute this you know this is again as in our general methodology use the base rule and divide by P of x so this is still the same but there is one small caveat here which is that what if you have a zero divided by zero in situation so before we have gaussian the density no matter what you do the density is always non-zero at every places even though sometimes it could be very small the density could be very small but still your PX in all of these quantities are like all positive it's strictly positive at least you're gonna get the ratio here but here there might be some cases where your P of X is just literally zero and why that can happen I guess maybe let me just give you an example so so you may think that some example just never shows up just because of some um I guess let me show the exam cases right so suppose maybe let's say suppose so suppose um maybe the word artwork never appears in in changing side I never claim that this would mean that this would mean that P if you have a but your test example content continent so let's say Test example let's call it X contains it and now I'm going to claim that P of X will be considered as 0. um why I guess let's um some will simulate this algorithm and see what the fee will we're going to compute from this so otherwise hard work is the second word right so your so J is 2. so um and you know x2j X2 I is zero for every eye this is a mathematical translation of for every example the second word never shows up that's why the x2i is zero and when the X2 is 0 and you plug in it into this formula that tries to estimate this parameter suppose let's try to ask me the parameter V two Y is one right this Parliament intuity means that How likely the second word will show up in a positive example and recall that you know this formula is really about you know the total number of occurrences that is where shows up and divided by total number of positive examples so this will be zero because no occurrences of the second word divided by the total number of positive examples and this will be zero that's still fine um so far it's not a problem so zero divided by a positive number that's fine and the feed to y 0 is the same 0 over total number of negative examples this is also zero so so basically according to your uh estimate male estimate this this word otherwise just cannot show up at all right which makes sense because they didn't so it didn't show up in the tuning side you know you you just you know estimate you also say it shouldn't show up at all in this under this uh this set of parameters so but that's that's a problem because now if you compute P of x for example X it does contain the the second word then you're gonna write this as P of X so how to compute this you use the the total law of probability you say this is equals to case when Y is one plus the case when y0 and of course this is a positive number this is a positive number that's fun but this one is equals to maybe I'll just this one is equals to the using naive base this is equal to x j you know Y is one and you have some a product over J from J to d right and now I know that my X2 is one because this word does show up in this email and that means that P you're gonna have P of X2 equals to 1 given Y is one as the second term in this product right so you're going to have this term shows up in this product but this one is equals to Phi 2 Y is 1 which is equal to zero so just because of the second word you know according to your model is not supposed to show up but it does show up that means that this example just does have zero probability and your probabilistic model so that's why this is zero and for the same reason so this one is going to be equals to y0 and also you have this term P of X2 equals to 1 given y 0. and we call that under a probabilistic model you learned you just think this word cannot show up at all right the chance is zero so that's why the chance to see this word shows up is zero so this is zero so that's why this is also zero so because the zero terms eventually I guess if you because there's a zero here there's a zero here so you get zero just eventually so basically you conclude that this example which is n't supposed to show up like this example has zero probability under this probabilistic model that you you learn so and then that's a problem because now this P of X is a zero this is zero so you have something divided by zero actually this is also zero if you think about it because this is just the one term the numerator is just one term in the decomposition of PX when the numerator is just this term and a p x is the sum of the two terms so both of these the numerator and the denominator they are both zero and you have the zero divided by zero situation and now and what you do so this is a this is the issue and this is a reasonably realistic issue because sometimes you just see a new word in your test example right you haven't seen this word at all in the tuning site so on the way we deal with this is so-called uh LaPlace and smoothing in some sense this is a way to introduce a little bit prior so that you don't 100 trust your training site so so basically what we are seeing here is that you trust your trainings are just 100 like religiously in some sense like a like a like you uh you haven't seen any word the other the word artwork right in the tuning side you just and because of that you trust it you just say this word the students show up at all that's why if you see this if this word is in this ax right so then the probability of X is to zero so so what we are going to do is we're going to say okay just uh you know maybe we shouldn't trust the data exactly we're gonna allow it to allow any new word to show up a little bit with some small chance and and it's in some sense this is a local adjustment um um by using some prior knowledge which is called LaPlace foreign just to the best way to describe this method is start with something abstract um for the moment let's forget about that for the moment and just think about the abstract question suppose you have um so simple simple example maybe you can call this example or abstraction in some sense so suppose you think about you know your estimate um the bias of the bias of a chord right suppose you have a call and this coin is biased it's not 50 50. um so how do you know the the bias of the chord or in mathematical language it really means that you have a random variable Z which is from Bernoulli um where with a parameter fee and this fee is something unknown right it's not the half and you want to know what the fee is and you want to know it by looking at some data right so you draw a few copies from this distribution and you want to know what's the um uh what's the what was feed you want to ask me feedback um by using the data and you want to solve this problem right this is still a probabilistic model we can still do the same thing where you can write out a probabilistic you can write out the mle and you can write out the you can maximize the Moe right so let's uh maybe let's try to do this just uh so suppose you have like maybe n trials and let's call this you know Z1 Z2 and z and and each of these is either one or zero something like this right or maybe I can see my nose is called tail has so I'll just continue something like this so and you want to estimate what fee is and if you follow our general principle right we'll try to write out the likelihood then the likelihood what is the likelihood foreign so the likelihood of fee right of the parameter fee um is the chance to see this data set given the parameter fee so what's the chance to see this data set the chance to see the first one is one minus V right a chance to see the second one is is one um I guess tail means I think I need to Define Teo means what tail means zero let's say has means one right so so the chance to and you have a uh the the the probability to see one is Phi and probability c0 is one minus V so that's why the probability of to see the first example is one minus V and then you time one minus FIFA second example when you multiply a bench and then you multiply fee at the end right this is the left node and and if you organize this right you count how many one minus V they are how many speeds they are this will be one minus V to the power of the number of uh uh tails and times V to pause actually if you really look at this example you will see that the formulas are very very similar actually this example is a toy case for that in some sense um okay so you get this and then you can um take the arc Max of this like log likelihood you take the arc Max of the log likelihood so the arc Max uh maybe let's call this IO fee so the arc Max of log of LV if you solve it you are going to get the following so it's going to be the number of has over the number of has plus the number of tails which also makes sense because you know this is basically empirical frequency of seeing the house I just mean like the the frequency of that you see that has in the in the tuning set right and that's your most likely estimate for uh for fee Okay so and [Applause] right so so okay so far everything seems to make sense right so but now let's consider a somewhat um kind of extreme case so what if you see all the examples you see are Tails right and you don't have a lot of like suppose you just have three Jaws Z1 Z2 and Z3 and they are all tail so according to this formula you're gonna get on the fee the best estimate for V is 0 over 0 plus 3 which is equals to zero but you should do really trust this right so if you do really trust that this this coin just never give you a hat right what does this mean this means that this coin just never give you a hat at all forever right she didn't really trust that um you know this is a little bit subjective right you mean you know if you are really really trust the data you probably can say yes but you know maybe you have some prior knowledge that most of the coins are not that crazy right so like you probably should at least have some chance to see the hat right with some chance so um so in some sense you can say so basically on the flip side you can say okay maybe this is some coincidence right it just happens that you see storytelles even this fee is a half right seeing this example seeing these cases is the problem to see this case is is at least one over eight right it's something like one rate so so maybe it's just Coincidence of the data set so maybe you shouldn't trust the data sets that much um so the so-called Law plus smoothing is is a way to in some sense incorporate a prior so that you you say look my coin shouldn't be extreme shouldn't be too extreme like this so so basically if the LaPlace moving refers to the following estimator so this if you use a plastic smoothing you're gonna have a different formula your fee will be equals to the number of heads plus 1 over a number of heads plus the number of tails plus two so this formula you know you know I didn't tell you any like a mathematical justification actually if you really look in the literature they are mathematical justifications so far I'm just determining the formula but it would solve this problem right like at least to some extent because for this particular case you are gonna get one on on the numerator and four um for the denominator so instead of getting zero you're getting one over four so still you you say okay the chance to see the head is pretty small right it's smaller than the chance to see the tail but you don't have a very extreme estimate so and and you can see that this LaPlace moving is mostly useful when you have not enough data right if you have a lot of data then this plus boosting is not doing much right maybe let me give you a kind of a example for example suppose you have a big data where the number of has let's say I guess I'm making up this right so 100 a number of tails is 60. so if you use the standard approach right if you use the the the standard approach or maybe I shouldn't call like the vanilla approach then this is like a the fee would be equals to 60 over 60 plus 100. right this is uh what is this this is like a I don't know what I say like this is something like this and then um if you use the LaPlace smoothing then this fee will be 60 plus 1 over 60 plus 100 plus 2 which will be um 61 over 162. this is 60 over one 60. and these two are just those very similar just because the one whatever you change here one two right it doesn't really matter that much because the the dominating term is the six and a hundred so so sometimes you you achieve a like you choose you achieve a kind of right balance right if you have enough data then you're prior or like your LaPlace missing is not doing much it doesn't really change much you trust your data and if you don't have enough data then La plasma thing would um would try to make it not too extreme we'll try to make your estimate not too extreme okay [Applause] so now let's go back to our problem so let me see where is the best way to provide this um oh I think I erased them oh I erased the formula but I'll write it again foreign [Applause] so going back to our spam filtering thing I guess actually there is a there is a there's one thing that is left here this is our estimate right and our other estimate was something like VJ wise one is equals to am I writing the super super I think this is I not that right so if you apply this to uh our case so you can recall that you know in this case it's kind of like you are saying that the numerator is the number of times this word shows up in a positive example and and the denominator is like the total number of positive examples right so in some sense the knowledge is that this denominator is very similar to the number of heads and tails because because the health means that this word shows up that tail means this word doesn't show up so and and here the the denominator is like the total number of times the total number of examples positive examples um and here the the the the numerator is the kind of like the has right so basically has means that the words show up in positive in a positive example and tail means that the word doesn't show up in a positive example so that's why this is like has number of heads and this is second number of has a plus number of Tails and if you use a lot plus more thing then um you're gonna add one to the denominator and you add to the numerator and you add two to the denominator so that's the that's the LA possible and the same thing for the other formulas you're gonna get add one and I2 here so that's the uh LaPlace moving and while this solves our problem it solves our problem because now we just never phone we just never estimate any parameter fee to be exactly zero right so recall that when we have this uh um artwork issue right so the issue was that these two parameters for the artwork was exactly zero um yeah let me ask um that's a fantastic question so the question was that whether you want to do the same possible thing for the fee why like fee why was the single scaler to describe the the problems of why um to describe the uh the probability of each of the class right so and you are right so suppose one class just never shows up right and you still only have like negative class of positive cost then you probably should use LaPlace missing for that but I think this is a little bit less important because you know in most of the data set you have to see positive examples and negative samples you have to see a reasonable number of maybe 50 of positive negative and then LaPlace moving wouldn't really matter that much you can still use it but it wouldn't matter that much okay so going back to this so so recall that our problem was that when you have the when you have the parameters you get this zero right you you thought that other boxes shouldn't show up at all right because it's very extreme estimate and now when you add this one and two here on on so what you're gonna happen is that instead of having zero over the number of positive examples you get zero plus one over this plus two yeah I don't have a different color sorry so I have to just modify um um I just only have black pants um but so you get this and then this will be at least no longer so this will be still a pretty small number right this is one over the number of positive example plus two so but at least this is bigger than zero so and the same thing for this right so you're gonna get plus one our on this plus two and this is bigger than zero so if both of these are bigger than zero then when you evaluate this formula you are not going to evaluate to zero so your P of X will be some positive number so then you can get a a number of the P of bike effects and maybe just a very small extension give you have two minutes so two or three minutes so if you have more than two classes you can um this is just a for your interest um so suppose you have like a maybe a dice something like that right instead of like a coin so suppose you have Z which is from something like zero one up to K minus one so you have K choices then uh the LA plus the general LaPlace smoothing would be something like if you don't do LaPlace moving what you're gonna have is that the pro the chance you estimate this to be something like the number of times z i is equals to J over the total number of examples so you count how many examples kind of end up to the choice J and you divide by the total number of examples and if you use LaPlace moving you're gonna add one to the top and you add K to the bottom where K is the number of choices so so this is just a small extension of Laplace smoothing you know I don't think this course will use it again but for your interest um okay cool I guess that's pretty much all for today any questions okay great yeah in the next lecture I guess so we're going to talk about the kernel method and then we're going to talk to deep learning