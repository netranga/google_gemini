Video Title,Video Contents
Stanford CS229 Machine Learning I Introduction I 2022 I Lecture 1,"Stanford CS229 Machine Learning I Introduction I 2022 I Lecture 1

so I am tongima this quarter we are going to have two instructors you know me and Chris I I'm tonyma I work on machine learning machine learning theory um including the theory for different topics machine learning reinforcement learning representation learning supervised learning so and so forth um I guess I would like Chris to say uh say something about him whatever he wants to say yeah I'm Chris also the machine learning group I'm really interested in how systems we build are changing with machine learning uh it's been a really interesting time for the last 10 years started a lot on optimization how we scale up these big models that was when machine learning had very few applications in our in our lives around you over the last couple of years have built things that hopefully some of you in this room have used my students contributed to things like search and Gmail and assistance and other places there and more recently really interested in how to make these models robust and we'll have a great new lecture that tangyu is going to give about what are called Foundation models or these large self-supervised models that are kind of all the rage Percy and a course about them last term and this course is really exciting because it's giving you kind of that absolutely foundational layer of machine learning that all that stuff is built on so this is a great time to study it because it's no longer abstract like you get to use machine learning products in every day and hopefully you'll get some insight into how they actually work and and why there's still so much research to do so really excited and looking forward to lecturing with you folks great uh I guess you'll see me and Chris alternate um for every few weeks you know next next lecture will decrease and then after two or three weeks you can see me um so to this lecture I guess I'm going to um okay I guess the first thing the second thing is that let me introduce the teaching team so uh we're gonna have um 12 like fantastic Tas and the one high ta and of course coordinator so what is the high ta and it's gonna be the course coordinator they will be probably uh doing most of the works behind the scenes you know you probably don't necessarily have to interact with them very often so they are organizing the whole ta team and we have like 12 currently 12 Tas I guess probably we're gonna have more if we have more enrollments um and I guess um I didn't ask the kids to show up in the first lecture just because I guess they also have to wear masks and and maybe the pictures you know serve the same need but I guess you'll see them you know pretty often in office hours and and different uh scenarios um cool so um okay so I guess the um this lecture I think I'm going to spend first probably 20 minutes on some of the logistics some of the the basic kind of like uh the structure of the course and so forth and then I'm going to introduce um the incredible at the high level the topics that covered that is covered by this course so I guess um we tried very hard to make everything available online like in a single dock I guess on a single website so we have this course website which has links to a few different Google Docs um one of them is about all the logistical stuff and the other is about the syllabus and the the final one is about um also there's the links to the lecture notes and there's links to um some kind of like guidelines on the final project so in theory I think all the informations I presented today you know will be subside of what you can found on the on the website and it's actually a very small subset um so I do encourage you to kind of like read through the the documents you know to some extent you know especially when you have some questions you know maybe first go to see whether the documents answer those questions and and then feel free to ask questions to us so um I guess the first thing I'm going to talk about is the prerequisite um so this course you know as you'll see will be you know at least the sum of the students in the past say this course is challenging of course some students say it's on the easier side no um there are different kind of backgrounds you know um so I think that's why we do this is my first slice right because um I think it's important for you to have some kind of like a backgrounds to be able to achieve your your goals in this course so I think the most important prerequisite is probably some knowledge about probability on the level Flex cs109 or size 116 now for example you probably should be at least you should have heard of this these terms like distribution random variables you know expectation conditional probability violence density so and so forth right you don't necessarily have to know exactly all of them like off the top of your head but this probably should be something you have seen in one of the previous courses another thing is linear algebra major specification eigenvectors I guess linear algebra was offered in mathml4 113 205 actually there's a longer list of kind of relevant courses in the logistic doc which you know taught linear algebra and the most important thing I think we need is measure specification and eigenvectors and we also require some basic knowledge of programming especially in Python and numpy so um I think if you only know python but not numpy I think that's probably pretty much fine because numpy is really just some basic you know numerical operations but if you don't know Python and numpy but you know for example C plus plus I think that's still probably pretty fun um because you know I think migrating from C to python is is pretty relatively easy in my opinion but you know um I think they have similar kind of you just have to change the syntax but if you know nothing about programming I think that's probably going to be difficult because a lot of homework you know they have some math part and they have some programming part and one of them the most challenging thing I've seen in my past about homeworks is that when you write a piece of code and something goes wrong which happens all the time and even when I write code you know there's something that seems to be wrong and you don't know whether it's about the syntax or it's about math right so these two things kind of sometimes entangled together so so you saw that I did I derived the wrong you know equations but actually probably you didn't use numpy in the right way so we're going to kind of cover python numpy in some of the TA lectures um just to kind of give you some refreshment or kind of like if you didn't know them you can learn something from the te lectures um um so but but I think you need to have some basic programming knowledge um yeah so um the tea lectures you know we also have materials for the tea lecture so you can um like we're gonna have three lectures on each of these topics um programming in the algebra and and probability to kind of review some of the backgrounds for you um I guess you know this is a mathematical intense course you know at least according to you know of course depending on your backgrounds but kind of like a good portion of students found that uh this course is mathematically intense um so it's kind of a heads up so so it's probably good for you to have at least you know at least 12 out of these three two like among the three things probably you need to know at least two of them um relatively well so that you don't get kind of credit entangles kind of like issues when you do the homeworks so um yeah but that's kind of why this is exciting and rewarding as Chris has said the goal of this course is to give you the foundations of machine learning this is the foundational layer um so so this is a simultaneously introduction course to machine learning we don't require you to have taken a machine in the course to take this right um so so it's an introductory course but on the other hand we hope that after you take this course you feel somewhat comfortable that you know like enough basics of machine learning so that you can apply machine learning to some of the applications of course if you really want to kind of be an expert in some of the applications like nlpm Vision you probably have to take those courses but this course probably will set up the foundations for the most learning component of some of the the general kind of like AI or other applications of AI right so so that's why we're kind of like this course is actually covers a diverse set of topics and and does involve some mathematics we don't have um mathematical proofs you know probably we have a little bit proofs but very little proofs but we do have a lot of like mathematical derivations right you probably have to follow um like a do some kind of mass derivations in the homeworks and we are going to do the revisions in the in the lectures as well right um by the way if you have any questions just feel free to stop me um um I'm happy to answer any questions yes the lectures are recorded and you can find the recording on canvas I guess right so the second important thing that I want to say is the owner code it's probably a little kind of awkward to say this you know um so early and I think the reason is that in the past unfortunately we do have some kind of there are some issues with let's be frank there's some kind of issues with the owner code violations you know I I don't want to see them you know it's very you know side for me to to see the to report to have to report students you know with designer code violation but that's what's you know that happened in the past so so that's why I want to kind of like put this up front um you know if you don't intentionally write Honor Code um I don't think there's anything you should worry about like um but anyway let me briefly say you know this is actually a subset of like uh things that we have on the uh the course website but I think these are the important thing so um for example you know on one side you know we do encourage you to have study groups so you can um collaborate with other people on homeworks or on on like uh homework questions like um so but um um but the thing is that you can own you cannot okay so you can discuss works on homework problems in groups but you have to write down Solutions independently and you also have to write down the names of people with whom you discuss the homework and I guess this is I'm copying this from the logistic doc which is a little bit longer you probably should read that piece of text you know in the dock as well so it's all the code violation to copy refer to or look at written or coding solutions from a previous year including not limited to official solutions from a previous year Solutions posted online Solutions you or someone else may have written up in previous year solution for related problems you know anyway it should be fine um but as long as you don't intentionally kind of like do anything bad I think you know don't be stressed out about it but on the other hand they were kind of like reporting of final violations in the past so we do kind of like check the code um like like using some some kind of softwares and also we we do we'll we'll have Tas to kind of like deal with this kind of honor code violations too much stress about this but I don't want to kind of like put it up front here um okay so um one another component I would like to kind of like um besides homework and homework is kind of like obvious where we have to have homeworks another component of the course is the course project so we encourage you to form groups of one to three people and um so you do a project with three people for example and it's the same Criterion for either one people or two or three people so and there are more informations on the course web on a website um and typically you you apply machine learning to some kind of like um applications you know or some kind of topics you are interested in right so um this is actually one thing that I really like about this course eventually you know after every year every quarter we got like probably 100 submissions from the the projects and we see all kind of topics you know um um or like all kind of applications of machine learning like these are just a list of topics we have you know see in the past and you are welcome to you know even work on other topics right so of course you can also work on just the pure algorithms for machine learning that's also fun but many people have actually also work on applications of machine learning uh to other kind of topics like music you know Finance which are kind of interesting um okay great so and we have homeworks you know we have four homeworks you know you'll see um you know uh and we also we are also going to have a meter there is no final um uh exam so the um so midterm cost project and homework those are the main things for the course um and another component of the course is the TA lectures um so these are optional uh you don't have to attend them if you don't find them to be useful and also there are actually two sets of ta lectures so one type of tea lecture is the so-called Friday the lecture of Friday section so um so we're gonna have problematicus six to seven weeks of these lectures the first three weeks will be about reviewing some of the basics um and especially the part of the basic kind of concepts related to machine learning and then the other weeks about more advanced topics um which are not required for the course but maybe interesting for some subset of you and we also have the discussion sections the goal of this is to have some interactive sessions our course is pretty big right so you know you can feel free to ask questions but I I guess you know after bullet you know like it's a bit less interactive per person compared to other courses um so we're gonna have this uh small sessions led by Tas which the goal is to kind of like imitate more traditional classroom settings and also work on kind of like a um more kind of like Bridging the Gap between the lectures and the homeworks right so um so basically the Tas will largely work through problems that are very similar to the homeworks or even sometimes simpler than homeworks so that if you um if you need it if you need them you know they will help you to kind of like uh make it easier to solve homework questions right so and I mean terms so um and this kind of sessions will be more interactive like the Tas probably will let you to do some questions live and and maybe present your Solutions and discuss with other students so and so forth um and the exact time and format will be you can find them on the on the the dock um the Google Doc about the logistics oops Okay so yeah so there are some you know many other informations on the course website on the Google Doc the dock is actually 15 pages now um it's pretty comprehensive um so for example recordings they can be found on canvas um there's a course calendar on canvas there's a syllabus page which will link to the lecture notes um and um and we're gonna have like the the ad the platform for question answering um we do encourage you to use that to communicate with us um if that makes sense like a like almost in all situations I think you probably should use ads to communicate with us you can have private posts or another's post um different type of posts depending on what you need um and if you don't have access to that then you probably have to email some of us I guess you can email the uh the ITA too um to add you to the to that to give you access to add um and there's grid scope which is used to submit homeworks and there are some late take policies which you can find in the the dock as well so I guess one thing that I need to mention here just uh as a heads up is that we don't allow late days for the final project and this the reason is just that especially for spring quarter the grading deadline is very tight so it's pretty much just a few days after the the final the exam week so especially because some of the students have to graduate and they have and it's the timeline is very very strict so um and we don't want to kind of like make the course project deadline the final project like very early right because then that would kind of conflict with the homework deadlines so and so forth so so the final project I've done I think is on the I think it's on Monday of the the finals week uh double check that but but it's it's we try to put it as late as possible but on the other hand because the the final greeting deadline we don't allow later is for the final project uh and there are some other IFA cues you know in the Google Doc as well any other questions before I move on to the topics the more kind of like the scientific topics foreign we have two uh Tas uh offering two discussion sessions I think we will try to make sure that the materials in the two sessions are pretty much the same and the times are kind of like somewhat kind of I think we haven't decided time yet um so you can feel free to choose any uh sessions you want to go um probably is the best for you to consistently go to one session maybe the Tas knows you better you know um but you don't necessarily have to and then this is also optional you don't have to really say go to all of them depending on your needs yeah other questions okay uh okay sounds great so I guess then I will move on to the more scientific part of the course so as I said um the main goal of this course is to set up you for the foundations of machine learning um and we're going to cover a pretty diverse set of topics immersion learning with the you know some kind of mathematical way so let me start by um some definitions of machine learning what is machine learning right as you can imagine you know when you are speaking about you know such a kind of like hot topic that is kind of like um that people are constantly researching on right so there's probably not unique definition right so that can fit uh everything right but I'm trying to find out some kind of like historical definitions of machine learning which I think describes the field um pretty well so in 1959 I think this is probably the first time machine learning the word the phrase machine learning was introduced on by author Samuel um he says that machine learning is the field of study that gives the computer the ability to learn without being explicitly programmed so I guess without being explicit program it's probably um something pretty important for example suppose I guess this is in the paper titled some studies in machine learning using the games of checkers recent progress I don't exactly know what the game of hackers is so don't ask me about what the rules of the the game but the point is that if you explicitly write a piece of code that play the Checker right that doesn't really mean that you are using machine learning right so if you just say I have this fixed strategy I know which is actually very good for checkered the first step would be this and the Second Step would be that move right I just explicit code that uh in my in my computer and with some branching algorithms right so um that probably doesn't count towards machine learning right so uh if you use machine learning you have to like the computer to learn without being explicitly programmed so you shouldn't have explicit programming so but how do you learn right how do you give the computer the ability to learn I think in the second definition of machine learning by Tom Mitchell it's kind of like describes more context or more kind of like a um like a context about you know how do you really let the program to learn uh without being explicit programmed I think it says that a computer program is set to learn from experiences e with respect to some class of tasks T and performance measure P if its performance at tasks in t as measured by P improves The Experience e the the rhythm is kind of nice um so um so I guess there are several important Concepts in this passage so one thing is that The Experience e right so so I guess let's still use this example of the game of Checker so the experience in this case could mean for example the data so data basically means that this policy could be games played by the program with itself it could mean games played by humans in the past it could mean you know other kind of like a data you collected from other you know source of information maybe you collect some data from I don't know like uh in this case maybe um um you know you can collect data from other sorts of information maybe here mostly just on collecting data by playing itself or playing by humans so so experiences mostly means data and there's a concept called performance performance measure which is kind of important in machine learning of course there's no unique performance measure for different tasks you have different measures of performance but but the metric performance is the this metric the performance measure is pretty important in here you can call it um the performance measure could be the winning rate it could be the the winning rate Plus for example the the number of steps you played right you probably want to win as fast as possible in some other cases the performance measure that could be How likely You can predict something very accurately right so you can Define many performance measures and um I guess actually you know immersion learning you know if you look at the research you know some papers about you know understanding what's the right performance measure what's the right way to formulate our problem and some of the papers are about that given the performance measure how do you make the performance you know as best as possible so um and there is so and also there is this um last sentence where it says that if it's performance at the task in t as measured by P improves with experience what does improves with experience E I think it means that if you have more experiences if you have more data then your algorithm should have better performance so so that in some sense is a kind of like a a kind of like a evidence for you to learn something from the experiences right if you have more and more experiences you your performance is not improved maybe that doesn't really mean that you learn something right that could probably just means that you are you just progress you just program some kind of like explicit program some strategy and that strategy probably wouldn't improve as you have more experiences right so so in some sense this last few words indicates that you are learning uh from the experiences all right so I guess you know the final thing is that the tasks right so here the tasks is really winning um is this kind of context of playing the game right so um and we're gonna see actually a many different type of tasks uh actually just in this lecture right so there are tasks about predicting um the the labels of predicting something given the the input or the task could be finding out certain structures in the data or the tasks could be something like this where you want you want to kind of make decisions about you know how do you play the game in any case feel free to stop me just with your hand then I'm happy to answer any questions um this lecture is supposed to be very high level so um feel free to ask any questions so uh I guess um speaking of tasks right so this is um a pretty simplistic view based on tasks you know how do you have a taxonomy of machine learning right so I don't think everyone agrees with this 100 um um but it's a it's a reasonable Baseline you know as a as a kind of like a high level kind of like taxonomy so I think surprise learning and surprise learning and reinforcement I'm going to introduce uh this um uh separately but it's not like these tasks are completely uh separated right so probably the real figure would be like this right so there's some overlaps right so in reinforced learning problem you have to use supervised learning as a component and and as I said like many machine Learners where people are trying to figure out what's the right way to formulate the question to solve the applications right so maybe for some applications you have to use two of this together so um they are not necessarily only tasks but sometimes there also kind of can be viewed as you know tools or methods to solve your question right so maybe some questions requires you know using for requires a formulation that involves both of these three ingredients in some way um but as a first other bet you can think of them as three separate or roughly separate kind of type of tasks so I'm going to introduce supervised learning first so um surprise learning I guess um we are actually in the lecture we are going to use this um uh house price prediction uh as a kind of a running example so the kind of idea is that I'm going to have a relatively abstract way to introduce this but you can use this you can think of the the house price prediction as the kind of application so what you are given is a data set that contains n samples and what are these n samples it's n samples are in pairs um of you know numbers or empires of like vectors where X could be a vector or a number let's say x is a number and Y is also number so you have n pairs of X Y numbers and you can actually draw these numbers you know um here you know have a Sketcher plot right so every you know cross is really just One X by pair and the X could mean um so ask means the hot I guess this as shown here in the in the caption or um so the square feet is X and the Y is the price right so basically for every example it's a pair of square feet and price you are trying to use the square feet X to predict the price y That's the task and and this data set is called you know using Tom Mitchell's language this data set is the experience in a problem more more than language we call this data set of data so so basically our goal is to learn from the data set how to predict the price given uh the square feet of the house and so so basically if x is 800 then what is y and 8 this x you know might not be seen in the data set right if you ask is already you know show up already shows up in the data setting it's easy you can just read it off but X could be something that you haven't seen in the data set so and you know one of the way that you probably have seen this you know in some of the other lectures or other courses where you can do a linear regression you fit a linear line and then when you predict you just read off the corresponding number on this linear line what is the corresponding Y where X is 800. so and of course you can do um other things for example you can try to fade a quadratic line right so which actually in this case in this example uh this artificial example I I created you know a quadratic line problem with fill the data better um and in lecture two and three I think our goal would be um to discuss you know how do you fit a linear model and all you how to create a quadratic model for the data to predict the house price of course you know the house price prediction is only your application you can imagine you know many other applications where you are giving a data set of X by Pairs and your goal is to predict y given X for example you know um we can even we can just simply make the house price prediction problem a little more complicated right so we said that in the previous lecture in the previous slide we use the the size to predict the price but actually you know probably more about the house right so and you know for example the lot size and maybe you know other things right then for example suppose you also know the lot size then your goal could be to predict the price using size and loss size and we call this kind of like input different dimensions of the input X features right so size is one feature and loss size is another feature so now you have two features of the the particular house and you want to predict the price based on the two features and now your data if you draw them um and then there will be three dimensional X1 X2 and Y and then you can kind of plot them in these three dimensional graph um and as I said you know the um the the kind of the the things you know in the class time right the size and loss size is called features or input and in this case this features is two-dimensional and typically people call the price label or output and you are trying to find a function which Maps the input to the output um so actually another heads up is that in machine learning almost every concept has more than one terms for them so you're gonna see that you know some people call this feature some called people call this inputs and in some other cases probably you have other names for for digital things we'll try to be kind of um uh comprehensive like I'm gonna tell you what are the different names but we're going to use one of them I think in the lectures mostly we probably we're going to use input because input and output is a little bit less ambiguous actually features sometimes could mean other things as well um and again now everything is the same in terms of the mathematical notations the only difference is that now your ex is a two-dimensional thing let me explain the notation here a little bit which will be used consistently in the lecture so the superscript here denotes the which example uh you're talking about whether it's the index for the example and the subscript here denotes the demand the coordinates of the of the data so x superscript i is a two-dimensional vector and the X superscript I sub 1 is the first coordinate of the two-dimensional vector yeah and also sometimes like a website that the price the Y is called labels and outputs and sometimes they also because they're also called supervicious or generally if you say supervisions that means the set of labels right that's why this is called supervised learning because you do observe uh some um labels um in the in the in the in the data set and also the data set you know sometimes people call it tuning data set um or data set um or training examples you know there are multiple names um for the for it any questions and you can also have like high dimensional features like before we only have two Dimensions but actually in many cases you know if you have a house listed on online right for sale then you probably know a lot more about the house and then you can have a high dimensional Vector uh say d dimensional vector and each Dimension means something right maybe the number of floors the condition the ZIP code so and so forth and you use this High dimensional Vector to predict the Y um the label that you are trying to predict and in lecture six and seven we are going to talk about you know infinite dimensional features actually so in some cases you can uh have like you know uh you can combine these features into not a lot of more other features where you can say I don't use X Y as my features but I actually use X1 times X2 as my features right leaving size times a lot of size I don't think that makes a lot of sense for this application but in some other cases maybe you can kind of like take the product of your two um raw features right two dimension of the input you have and use that as a as a new feature right so and we're going to talk about how do you deal with infant dimensional features as well um and in some of the other lectures we're going to talk about you know how to select features Based on data um so maybe not all of these features are useful if you use all of these features then maybe you can overfit which is the concept we're going to talk uh talk about you may kind of like be confused if there are too many informations um available so you may select something that is most important right so maybe um I don't know like all of this seems to be important but maybe there's other features that are not important for price prediction right and there's another concept that I'm going to introduce in the in the first lecture we'll talk about this um later as well so um typically there are two typos two types of supervised problems so based on this distinction is based on what kind of labels uh you have right so um one type is called regression problem so these are problems where your label y is um is a real number so you are predicting for example something like a price right so this is a continuous variable and there's another type of questions which is called classification and these are cases where the labels is a discrete variable what does that mean that means that your labels are probably like you have two labels yes and no right so you just have like this label that is just a discrete set with two choices yes and no um for example like in this case you can change the question like you know if you're given the size and lost size you can ask you know whether this house what's the type of this horse or this residence right is it a house or townhouse right so it's not a continuous prediction problem it's really just the predicting one of the two choices and you can make this problem more complicated for example you can have much more choices here um not only just two choices and then in this case you know the way that we can kind of like um kind of plot the data set um one way to product is the following so now you have a two-dimensional uh graph where uh the x is the size and the Y is the the Lost size and then for every dot you have a you know if it's a triangle it means it's house and if it's a circle it means a townhouse and that's how you kind of like visualize the at least one way of how to visualize a classification data set where the labels are discrete you just use the triangle and circle um to to indicate the label of this example and and and then you know the kind of questions you want to solve sorry my animation the question you want to solve is that now if you give me a house which is a two-dimensional Vector with the size and loss size given as it as the input and you're asking what's the type of this uh house um so whether it's a house or town house and one of the way to do it is that you say you okay now I see okay so you can fade a linear classifier that distinguish these two type of dots and then your answer here would be naturally house because it sounds like it's on the right side of the this correct this side of the the line so it's probably should be consistent with all the other uh examples on the same side of the line so I guess lecture three and four will be about classification problems and the next few slides I'm going to talk about some broader applications of machine learning which we don't necessarily will cover I think image classification I think probably we're going to have one homework questions on image classification so the kind of the type of question is that you are given all of these images and every image has a label which describes the content of this uh the main object in this image of course you know in other cases you know you may have multiple objects in the same image but here let's say let's focus on a simple setting where every image has a single important object and then your label is basically describing what this object is and you're giving this data set this is actually a real data set you know created by Stanford people like by Professor Philistine so which is called imagenet this is a very important data set that you probably should remember the name of it because this is pretty much the data set that in some sense make deep learning take off in the last five to ten years um like before after the creation of this data set and some of the new um deep learning algorithms with new artworks I think in the last five to ten years we saw um like machine learning took off and are able to kind of we were able to make a lot of progress because of the data set and speak of the data set here I'm only trying to say what's the format or what's kind of the task right so basically your ex is some raw pixels of the images where you just represent this image as a sequence of numbers actually here's a matrix of numbers and then um your Y is the the main object of the of the image so and you can have other kind of tasks in vision for example object localization or detection right so give an image you can ask you know how do I localize you know um find out each of the important objects right with the bounding box we're not going to cover anything like this because these are more specific to the vision applications and right so so here I guess the the thing is that your why becomes like a a bounding box right so so how do you present this box um you know you don't have to know this but you know if you are interested the way to present the box is to present a box by uh the the coordinate here and the coordinate here and these two coordinates four numbers by two points four numbers will describe the box so basically a y will become four numbers uh instead of just one number and actually you can have actually more complex labels or y's in other applications for example in natural language processing which is the area to deal with you know language problems so for example machine translation you can have this problem where you you want to translate um for example English to Chinese uh and your ex is I don't know what happens with my pointer okay so your X is the the English sentence and your Y is the um is the uh the Chinese sentence right all sentences in other languages and now you can see that the Y even though Y is a discrete set the the family of y's is the family of all possible sentences um in uh in Chinese right but so so why looks like this quiz but Y is much more complicated than the house versus townhouse application right so you have so many choices of why like almost like exponential or even number of choices so then you have to deal with them in some different ways I think we're going to cover a little bit about machine translation or this kind of questions uh in one of the lectures that we uh added on this year um I guess Chris mentioned that like we're going to talk a little bit about like a large language models uh for language applications but on the other hand you know this course only covers the basics of the foundational techniques of surprise learning so we're going to talk about language applications but if you really care about the particular applications how to solve them the best way then you probably would have to take some other uh more kind of like a specific courses um for those particular applications okay so before I move on to unspressed learning any questions about surprise learning so would you say it's a regression problem or classification problem I think I would say it's a classification problem because why the formula of Y is still technically discrete right because you still have like a a finite number of possible y's because um I assume you can say the number of sentences um Chinese let's say the name of Chinese sentences is finance right even though the number is very large but this is a good question because you know you cannot choose this as a simple as a you have to treat it in some slightly different way differently from the the most vanilla classification problems because if you view this as the vanilla classification problems then you're going to get into other issues right so um just because the set of wires is too big when you use infant dimensional features so um I think I might not have a very clear answer right now because this does depends on a little bit on some of the other things we're going to teach but I think generally basically sometimes you don't know what so basically okay first of all how do you create infinite number of features right like so you you have to create them from X right so for example I guess I I think I alluded to this a little bit at some point so for example suppose you have this number of features right maybe D is a hundred so now you have 100 features how do you create more features you're going to use combinations of the existing features right so um and you can come up with a lot of different combinations um so you can have like for example x d to the power of K right and K could be any integer right so that's how you create infinite number of features and why you want to use them sometimes it's because you don't know which one is the best so you just say I'm going to create all the possible features I can think of and I'm going to like the machine learning model to decide which feature is the most useful or how do you combine these features so that's why we use info-dimensional features in most of the cases in reality you don't have to literally with infinite dimensional features after you run the algorithm you found out that some features are more important than others but before you run an algorithm we don't know which one is useful so you like the algorithm the machine learning algorithm to figure out which one is the most useful and actually one of the interesting is that even the dimension of the features is infinite it doesn't really mean that your runtime has to be infinite so there are some tricks to reduce the actual runtime so even though you are implicitly learning with infinimental features actually your algorithm or runtime and memory all of these are actually finite and and actually sometimes they could be pretty fast in some in some cases so these are great questions yeah thanks for all the questions any other questions okay so the the second part of the um the course will be about Enterprise Learning I think Chris will probably um give about five lectures on and space learning so and space learning the if you still use the host um house prediction kind of data set as an example the basic idea is that you know you are only given a data set without labels you only see the access but not wise right so you don't know how this house is you know in a data set are sold um in in the past so so what happens is you know it's you still using this townhouse versus house example right so if you are supervised and you have this triangles and and and circles here to indicate what what are the labels but you guys should stand here so but uh um if it's answered you just don't have this this part of the information right you just see kind of like a this this bunch of dogs here in the in the scatter plot but as as you will see right even you just see this right as a human if once you see this you somehow tell that okay this bunch of points here is very different from this bunch of points here so maybe there are two type of residents here uh going on even as a human right even though you don't see the the triangle and circle you still kind of are able to tell there's something going on there right so that's the kind of the the nature of science learning we want to be able to discover interesting structure in the data without uh knowledge about the the labels right so you want to figure out the structure hidden in the data so to find some interest instructional data so for example in this case you know what you can do is that you can try to Cluster um cluster these points into groups right you want to divide these points into groups and what you want to say that each group probably have some kind of like a similar structure right so this probably doesn't sound like a very good clustering because um at least as a human being you probably wouldn't cast it like this but maybe you you like a good algorithm problem would produce this right so if you produce this then essentially you figure out and there are two type of residents and this data set even though you don't know the the name of these two happy presidents right because the algorithm wouldn't know townhouse or house is two words but the algorithms knows there are two type of things going on here in the in the data set um and in lecture 12 and 13 we are going to talk about a few different kind of algorithms for discovering the structures okay means classroom and mixture of gaussians um and there are other kind of applications uh for example um I think this is a paper by um Daphne Collins group who is a Adjunct professor here at Stanford so so here the kind of applications about Gene clustering so I think the idea is that you have a lot of individuals and for this particular part of the genes you know you can group uh the genes of individuals into different groups so and and you can see that um um I guess even basically you can kind of see I'm not sure what's going on with my foreign you can see that there are some kind of clusters here and it turns out that each of these clusters you know corresponds to how the individuals would react to a certain kind of medicine right so and once you kind of like can group people you know into groups then you can probably apply the right type of kind of like um treatment on to each type of people um now here is another example which is probably a little more kind of easier to understand you know like uh so so the the the the type of kind of question is called latency meta-analysis which I don't expect you to understand what each of these means you know it's just a kind of like a name iOS a so the idea is that you kind of like look at a bunch of documents and every document has a lot of words right so and you look at which word show up in which document for how many types so each entry here supposed to pick one entry it means how how often the word power shows up in this corresponding column the document 16 is it document document six there right so every answer is how often the word shows up in the in the in the document and if you see this you know it then sounds like there's any pattern here right so what's the structure it's unclear but if you use the right machine learning algorithm what happens is that you can reorder or regroup this kind of words and documents in the following way you see the video is working right so basically you permute the documents and words and and then you see this kind of like interesting and sometimes block diagonal structure not very prominently but still kind of interesting enough and now you can see each of these blocks has some particular interesting meaning for example here this group of documents and words has this it's clearly about something about space kind of like shuttles right so shuttle space launch booster these are all about kind of like a like um um like a like a space traveling kind of things right so and so basically you know that this kind of four words have similar meanings and these four documents or three documents are about this this topic so by doing this you in some sense you know at least in this application you can figure out the kind of the topics of uh in in your data set right so you can figure out probably here there's one topic two one two three four five topics and each topic is more likely to associate to a certain type of words and every document is most likely about one topic and sometimes it's about two topics and then what you what happens is that once you figure out these kind of topics and then you can use some humans to kind of like interpret each of these topics what each of these topics are and then give a new document you can figure out you know what topic this new document is about and this actually is very popular um tool in many of the social science because in Social guys actually even myself you know was involved in some of the projects in my PhD so the social scientists they have some texts right they maybe have like a lot of like a maybe blog posts about politics right and they want to understand you know what each for example what trends happens you know in the blog post that so suppose you want to understand that and then you have to know you know what are the kind of topics about each of the blog posts where you don't want to kind of like label them each one by one because they maybe they have like a million blog posts right so they use this to kind of group the blog post in certain ways and then they can do statistics to understand what happens with all of these blog posts and this kind of applies to other kind of things Beyond politics you can have like actually you can even apply this to I think many things like you know history um what else you know um like psychologists so and so forth right so um and this was actually an algorithm discovered probably 20 years ago or even maybe earlier than that maybe 30 years ago and it was pretty uh popular still like a um in in social science of course there are even more advanced you know algorithms they stay single Beyond this um um which we're also going to discuss right so this is actually one of the the more recent kind of advancement I think this was like a around 2013 2014 about seven eight years ago so what happens here is that um you have a very very large unlabeled data set which is the which is called Wikipedia um so um so you just download all the documents from Wikipedia there's no other human labeling right they are just just raw documents and what you do is that you learn from these documents using some algorithms and what you can eventually produce is the so-called word embeddings so you can represent all the Every Word by a corresponding vector and why you want to do that the reason is that these vectors has um basically a kind of like the the numerical representations of the of the discrete word and and there are some nice properties about these factors that captures the semantic meanings of the words so so what happens is that similar words will have similar vectors and and also the that's that's what I mean by like a the word is encoding the vector so similar word would have similar vectors and also the relationship between words will be encoded in the directions of the vectors this sounds a little bit abstract maybe it's easier with this figure so um actually this kind of happens in reality right so if you look at the vectors each point is the vector for that word right so Italy has a vector and the vector let's say is this point and France has the vector and Germany has a vector so you'll find out that the vectors for all the countries they are somewhat kind of similar in similar directions right so they are so for example suppose you have another country USA then you probably would find out the the point somewhere here like nearby right so all the country uh has vectors that are similar in similar that are that are in similar directions and all the capitals are also in similar directions so this is what I mean by the um the the vectors kind of encode some kind of semantic um uh like a similarity between the words and also interestingly the directions also encode some kind of uh relationship so here what happens is that if you look at the difference of the um uh between Italy and Rome right so you kind of this this direction right this is the difference between Italy and Rome and you also do the same thing for Paris and France and Berlin Germany you'll see that the three directions they are very uh similar to each other they are kind of like in these parallel positions so um so in so um at least one application of this is that if you want to know um suppose you are given let's say us right so which is a vector here right and you want to know what is the capital of U.S where is the capital of U.S you probably should you go along this direction to search for points and that's likely to be the capital maybe you'll find DC or Washington I guess you'll find DC there um because I think Washington is ambiguous which is a little trickier um I guess that's why I don't have the actually this is an interesting thing right so uh if you have the the Washington director uh would be tricky right it's not clear where the the vector for Washington will be not clear where it will be because Washington has multiple meanings right so it's a state it's a it's a person is you know um so so actually this is a sometimes you know you have this ambiguity and and then you can have this kind of like more kind of complex clusterance of the words so for example I think um so here I guess what happens is that you know you can also use these vectors to Cluster the words into groups um so for example uh you have this kind of groups kind of these words you know like scientific words some of which which some of which I don't even know and then you can use the the casting algorithms for the vectors to figure out what they uh what kind of topics or what kind of like scientific areas they belongs to right and and you can also have hierarchical clusterings uh to deal with kind of certain kind of overlaps because for example mathematical physics probably would be closer to the physics vectors and math factors on both right some somewhere in the middle of the math and physics vectors so um um so so so there are this kind of like an interesting many kind of different interesting structures in in all of these word vectors which you can uh leverage to solve your tasks I'm a little conflict here just because you know to exactly do all of this it requires a little bit more uh uh things you know um that we haven't you know discussing but we will discuss some of this in later lectures so and and most recently um in the last two or three years there's a new um kind of like uh say Trend or kind of like there's a new breakthrough in machine learning which is um these large language models I think many of us are very excited about it you know Chris has mentioned about that um and and I stand for new health actually um a lot of people you know working on these large language models um and roughly speaking these are machine learning models for language and they are learned on very large scale data sets you know for example Wikipedia I say um discussed before or sometimes even bigger than Wikipedia so you can download you know um a trillion you know words or maybe like a like a like a 10 trillion words you know on on online because there are so many kind of like online documents and you collect all of this um documents and you learn a gigantic model on top of it you know these are gonna be very very costly even tuning the single model would probably cost you like a 10 million dollars you know um so just for one time so they are they are very costly but they are very powerful because they can be used for many different purposes and in particularly here I'm talking about this breakthrough called gpt3 you're gonna heard this name probably pretty often uh not very often like in the lecture um uh like pretty often in general like a um in the next few years I think uh or you probably already heard of it in the in the lecture we're going to talk about this like in one lecture so GPD three is this gigantic model and they can do a lot of things so I'm downloading this is a example of from there on blog post so you can use this gpt3 to generate uh stories I think here what happens is that you you give human some person writes this paragraph about you know something right so like some I guess mountains or valleys and then um the model the Machinery model can just generate some story and some kind of like very coherent and meaningful text afterwards and you cannot probably if you don't tell you these are generated by machines you probably wouldn't know that they are generated by machines you probably guess this is written by um on some some authors so that's one application you can one use one way to use this model to generate stories and you can also use this model to answer questions right so here um you you give you give this model this long paragraph and then you can ask um it's kind of like a SAT I'm not sure like this is kind of like GRE questions I'm not sure whether all of you know GRE but like um so like these are kind of like just basic questions answering about the past passage right you can ask you know what's what is the most popular uh um politics in uh in Finland and and this will this is this is the information you can find in a document and and they would answer the right thing um so that's another application and you can also use this to do other things for example you can say you can just just write in the text say please unscramble the letters into a word and write that word and then you give this to the model and the model will just change the uh the orders of the letters to to make it a meaningful word and you can ask you know for example this is not like a simple numerical questions what is 95 times 45 and it gives you the right answer and all of this are not like so so what's the amazing about this is not because it can solve all of these tasks just each of these tasks the amazing thing is that you learn on this unlimited site this gigantic and enabled set you didn't specify what tasks you want to solve right the only thing you see is this gigantic data and then the single model can be used to solve multiple tasks just by interacting it with different things right if you want to solve this task you just write it in the human interpretable language 95 times 44 45 and if you want to solve another task you just do something else a slightly different kind of like phrasing um then they can be used to solve multiple tasks and that's that's why we call them Foundation models um at least you know your paper in a white paper written by Stanford people um so um so in some sense there are kind of Foundations uh for uh they can be used for a wide range of applications uh without um further and sometimes without a lot of like further uh changes right so the model itself can be can do a lot of work um for many tasks okay so I guess I'm supposed to stop like for 30 wait 445 right yeah sorry like uh um okay so I still have some time any questions sorry um like a um right so I guess um maybe one if I understand the question correctly so one concern is that whether this 95 times 45 already show up in the Corpus but maybe you just memorize it that's one possibility but I think that's that's not the case so of course some of the numerical um problems like some of these multiplication problems show up in the Corpus christ you will find one document online about you know what is 12 times 35 but you wouldn't find I think you wouldn't find the uh documents about all pairs of like multiplications um so like multiplications of pairs of like two digits numbers I don't think you'll find all of them um so so there's some kind of extrapolations where you see some of course you have to see something right so that you can learn from them right so you probably have seen a lot of kind of like a numerical um uh operations like all kind of like mathematics the formulas in the document but then you can in the in the tuning Corpus and then you kind of extrapolate to other uh other instances right like so you learn from um some basic stuff and then you learn like then you can use the model to Output multiplication so for example longer uh digits does it make sense does that answer the question right so how do you make sure that the like uh by pollution I guess you mean that how do you make sure that in the tuning Corpus they are not all pairs of double digits right so I think they do run some tests to check that so of course you cannot make sure I guess is that what you mean by pollution or you mean by pollution you mean like something wrong about the false information right right okay right so that's a great question right so I think abstract is speaking the question is about you know how do you make sure your training corpers don't doesn't have wrong information for you all right so I think we you know I think they definitely they are wrong information in the Corpus right I think what happens is that they are probably more correct information than raw information and and you somehow kind of reconcile between them and you kind of pick the right thing um so that's largely what's going on but of course you know if you are very specific about you know so this this area called Data poisoning so you can actually specifically change your tuning data in some special way actually you just change a small number of training data so that your model learns something completely wrong so that's that's actually possible so but that requires a kind of adversarial change of the tuning Corpus so so on one side this is a very bad thing because you know if someone do something out of a serial online and you use those kind of documents to train your model that's that's a huge risk on the other hand you have because you have to right so at least right now like uh like the it's not it's like a this kind of like a adversary Point thing is not happening very often just because it's not very easy to achieve them thank you okay any other questions okay cool yeah these are all great questions like I like to have more questions uh that's great so um okay so the last part is about reinforcement learning um this will consist of probably two or three lectures at the end of the um at the end of the course so the main idea of reinforcement is that here the tasks roughly speaking about learning to make sequential decisions so I think there are two things right one thing is that you are making decisions so before in both Superstar and express learning in some sense you are making predictions right at least in supervisoring it's pretty clear you are predicting wise but here you are talking about decisions and what's the difference between predicting decisions and like the decisions and and predictions the decisions has uh long-term consequences right so for example if you play chess right so you you make some move and that move will affect your future uh like it affect the future right so so you have to to think about long-term ramifications and also this is a sequential decision so so you're going to take a a sequence of steps right so when you take the first step you have to consider you know what this step will change my game and what happens in the future so so that's why you can see that you know this kind of like reinforcement algorithms are mostly trying to solve these kind of questions where you have to make a sequence of decisions so for example when you self-go right you probably heard of alphago and another example is like for example you want to learn a robot so if you want to control the robot you have to take a sequence of decisions right how do you change the joints you know how do you kind of like a control uh like actually there are always multiple kind of like uh things you can control for a robot and how do you control all of them in a sequential way so here I'm I'm showing this in a simulation environment so this is a so-called humanoid which is kind of a robot that imitates a human so and the goal is to you can control a lot of joints um in this in this robot and your goal is that you want to make this robot be able to walk to the right as fast as possible and and this is what happens uh how the kind of what happens with the reinforcement algorithm learning here so it's kind of like trials and errors to some extent so what you do is you say you first try some actions right you first try to kind of like do something like this and and then you figure out that this didn't work well right it's false and then you go back to say I'm going to change my strategy in some way right so I know that some strategy is not going to work I'm going to try some other strategies and maybe I know some strategies is actually partially working because at least the robot the humanoid is is doing something right it does walk to to the right for one step it just didn't keep the balance you know some part is good some part is bad and then you try to go back to change your strategy and then you probably can walk a little further something like this right and then I guess I'm going to fast forward to iteration 80 I think 80 works I forgot whether I have an oh actually 80 still doesn't work perfectly you can see it's working in a weird way um yeah so and I think at iteration 210 I think it's it can keep working but still it sounds that it sounds very natural but this is a problem like you shouldn't expect that the humanoid work as natural as humans um partly because they are different there are many different things right so like maybe for the robot this is optimal strategy you know that's possible but of course I don't think it's the optimal strategy but it's possible that optimal strategy for the robot is not the same as the option strategy for for us so and generally as I alluded to so the kind of like the the very high level idea of reinforcement algorithm is that you have this kind of like Loop um between training and data collection so so the algorithm so before in Supersonic and Einstein we always have a data set where someone give you a data set and that's all you have right you cannot say okay give more examples of the house uh house prices right so you have to work with what we are given but here uh in the reinforcement formulation you often can collect data interactively I see some questions it's a question sorry okay um okay so so here you often can collect data interactively so meaning that you try like for example in the humanoid example you try some strategy and you see that humanoid false then that's the data you see additionally right so then you can incorporate new data back to your training algorithm and then change your strategy right so you have this kind of loop where you in one one side you try the strategy and collect feedbacks and the other side you improve your strategy based on new feedback so in some sense you have a data set that is growing over time the the longer you try the more data points you're gonna see and and that will help you to learn um I'm better and better so I guess so um Okay so I guess that's my last slice about reinforced learning any questions [Music] oh oh sorry oh uh or after like an empty like the information is completely right right so is the feedback uh uh seen after each type of decision or is it after something else right so so there are many different formulations this is a great question so so the most typical way typical formulation is that you see the feedback right after the decision you make um but that sometimes it's not realistic for example let me see what are the examples so um I I think okay maybe I'm blanking on what are what are the best examples to show but in some cases you don't have the feedback right right after and sometimes even you have the feedback right after the decision you cannot change your strategy right after decision so um just because for example there is a computational limit or you have to really do something physical to change your strategy on a humanoid right or maybe there's some communication constraints so uh so there are multiple kind of different formulations reinforcement learning I think if you have delayed reward I think that's called the delayed reward problem and sometimes you also have this so-called deployment wronged in the sense that um so this notion of number of deployment means that you can only update your strategy for for example five types right so you cannot just constantly change your strategy and then you can ask this question what's the best way to to do this for example I guess one example is that um you suppose you are using reinforcement to control a nuclear plant right so you probably don't want to say that you just keep telling you you run algorithm like a um like you run algorithm and the algorithm keep telling the nuclear plant to change their strategy to control them right like every day that sounds like risky and and also kind of like inefficient you know there are many problems with it right so probably you're gonna say that I have to do some experiments you know for a little bit for six months and then I figured one strategy that I almost guarantee um uh I can guarantee that this new strategy is working better than the old one and then I deploy it and then collect some new feedback quite so and also I guess maybe another thing this is a great question and another thing um I I would like to mention is that in many of these problems you know there are multiple uh Criterion for example for reinforcement if you want to control the nuclear plant there's a safety concern right so then you have to have to care about whether your strategy is safe or not right for the human noise probably it's fine for the human not to fall down you know to some extent but still you cannot really let it fall down so often because it will hurt your Hardware right so it is frustrating the other constraints for example there are constraints about you know how how long is the tuning type right that's the typical metric and there is also a constraint about how kind of like a powerful or how kind of multi-purpose on these models are right How likely they can solve multiple tasks So So eventually like uh this is a very um like especially if you look at a research Community there are different people care about different metrics um just because all of these metrics you know have their own applications right so um so so this so it's the the real kind of scenario is much more complicated than this in some sense okay so I guess um in um there are a few other lectures about other topics in the course um which are actually in between some of these um big topics so one of the topics we are going to spend two lectures on is deep learning Basics so deep learning if you heard of the word so maybe some of you have heard of it so deep learning is um the technique of using the so-called new neural networks uh is your model parametrization so um so this is this can be used together with um all of these tasks right it's kind of like a technique that can be used in reinforcement that can use in Surprise learning and express learning and in many other situations so and this is something that um um is very important because you know because of deep learning uh took off around 2014 2013 in the last seven years we see this tremendous progress of machine learning because of these techniques a lot of things you know are enabled by this different techniques so uh and we're gonna also discuss a little bit about learning theory uh just for one or two lectures um so in some sense actually we don't really talk that much about the the the core theory in some sense the goal here is to um uh understand uh some of the kind of like trade-offs of some of the decisions that you should do when you train the algorithms right so what's the best way to select features what's the best way to make your test error as small as possible and also we're going to have a lecture on how do you really use some of these insights to tune um an ml model in practice you know what kind of decisions um I as though as the machine learning as a the as the algorithm implementer you know what compositions you have to pay attention to so and so forth so um I guess we're gonna have a guest lecture on on the broader aspects of machine learning especially robustness and fairness um I guess machine learning has a lot of societal impact especially because machine learning Now is working you can really use it you know in practice and it will create some kind of societal issues actually a lot of societal issues um and and these are things that we should pay attention to I'm not an expert in this area we're going to have a guest lecture James though who work a lot on this to talk about fairness and robustness of machine learning models okay I guess this is uh um all I want to say for today"
"Stanford CS229 Machine Learning I Supervised learning setup, LMS I 2022 I Lecture 2","Stanford CS229 Machine Learning I Supervised learning setup, LMS I 2022 I Lecture 2

so hello uh welcome to 229 uh so we're starting a block of three lectures that I get the privilege of of spending some time with you and kind of walking you through the building blocks and Basics before I get into the plan for those three lectures I want to make sure we understand a couple of logistics so I I posted something on Ed that kind of explained why I was setting up lecture in the way I am you are not obligated to read that but if you're interested go ahead and read it super happy to take feedback and discuss any of that one of the things that I liked about the pandemic was that more people were asking questions during class and I think part of that was because people were you know using the anonymous feature on Zoom quite a bit and I and I wish we still had that we don't in this class for various reasons so what we're going to do instead is we're going to have this Ed thread that I just set up that says lecture two 330 in class question thread and feel free to fire away questions on there I may not take all them I reserve the right to skip them Tas May jump in and answer some um and I'll try to follow up on anything that's there but it's really helpful to me that that you ask questions and happy to talk about whatever you want uh really be relevant to the class is helpful but like pretty much whatever you want um second thing there are a couple of downloads that I put up before my lectures I put up two things one is a handwritten note of what I'm going to talk about which are the same notes that I use I modify them a little bit and then also a template in case you want to follow along again you don't need any of this stuff you can just sit watch it on video watch it here ask questions do whatever you want but it's just so that you know the material that's there and that you know things like data that I want to show you and look real I can cut and paste that in and you can have it in front of you while I go through it okay all right so that's the logistics I will use I'm going to try and use the iPad I like using the Whiteboard feel so this is a good compromise because it slows me down if I get excited I'll start talking all kinds of nonsense so this will focus me a little bit more on the class and you'll see how long I last all right so what we're going to do in this first three sections of the class first three lectures is kind of build up uh increasingly sophisticated machine learning models and your what you're going to see is that they are very very similar to a model that you probably already know and love which is linear regression if you don't know linear regression don't worry today's lecture is effectively going to be talking about linear regression with slightly fancier notation and you know some little bits around the algorithm but it's basically just fitting a line okay it's it's really hopefully going to be something that you've seen and you can grab onto and then what we'll do in the next lecture is we'll generalize this from regression which is the kind of traditional fitting align to classification now I have a couple of twists we choose our notation a little bit carefully and what that allows us to do is show that that way that we're looking at classification and we'll talk about what classification really is allows us to do a much larger class of models which are called these exponential family of models and they're going to kind of rear their head throughout the course so we're going to see a precise definition that allows us to have a huge number of statistical models and kind of treat them in one way so we don't have to understand the details of every little model we have an abstraction of how to find its parameters how to do inference on it let's get a prediction out of it and kind of understand it and kind of understand these algorithms I'll try to highlight for you as we go through there which of these pieces actually carry over to what I would call kind of modern or industrial machine learning feel free to ask questions effectively the way we solve these algorithms or what we solve these underlying optimization problems is exactly the way we run everything from how images are detected to how you know search works in various different corners of it to natural language processing to translation weirdly enough this abstraction kind of carries over for all of that and the underlying Workhorse algorithm which we'll see is called stochastic gradient descent and so we'll try and introduce it in that absolutely simplest setting okay and so that's the idea it's going to be building parallel structure for the next kind of three so linear regression classification and then we're going to go through this generalized exponential family and they will have a very parallel structure if you go back to the notes you'll be able to pull out like oh this is the solving part this is the model part and what we're going to do there all right then tangyu takes over teaches you a bunch of awesome stuff neural nets all the rest of that stuff kernels then I come back and teach you unsupervised and there again is a different structure there but it's very very similar and graphical models and the rest make it make an appearance there okay so today our plan is to get through first just some very basic definitions um we'll be a little bit pedantic there but that doesn't mean you shouldn't ask questions means if you don't understand something you should and I haven't done my job so just fire off a question in any form you like then we're going to talk about linear regression which as I said is is fitting a line except we'll be fitting High dimensional lines eventually so we're going to want to abstract that away we'll talk about batch and stochastic gradient descent which are two algorithms and machine learning as tangyu talked about we're not great with terminology this algorithm is called incremental gradient descent in the 60s it's been around forever our incremental gradient methods actually it wasn't even it's not even a descent method formally doesn't matter the point is these are old things that people have been using for a long time and weirdly enough it's what we use every day like it's as I said this is like a Workhorse algorithm that you're going to see and then I'll very briefly cover the normal equation because I think it's a curse on your homeworks um you know and it's like you know also gives you some practice with Vector derivative so you do need to know the vector derivatives stuff to make your life easier in this class you'll have to compute occasionally compute a gradient or compute a derivative and this is a place where you kind of know what the right answer is so when you compute these derivatives like you know it's an easy place to check yourself but I wouldn't say that like normal equations are like the most important thing you'll learn in this class it's just you know it's all you should know what they are it's not hard okay all right great so let's talk about supervised learning all right so this next section as I mentioned is going to be all supervised learning and it'll all follow kind of the same general schema right and what I mean is we're going to try and have some what we call prediction function and basically all that's going to be is a function H which will use this notation consistently that goes from some set X to some set y okay before defining this formally let me just give you a couple of examples so one idea is that X could be the set some set of images right so we can look at in images a bunch of images and we could ask does it contain a cat right that was actually like a very important machine learning problem at kind of one point in time people still work on that right you know what's the object that's in this image that would be a prediction right we are wise here would be a set of labels that say things like cat dog things like that all right it could also be text right so we could look at text here and we could ask questions you know that you know maybe we arguably should be better on in machine learning like is it hate speech all right right and so we ask here you know this x here these are all examples of data types that we want to work on and these are all labels or y's that we're talking about okay now we'll look at as tangyu showed in his lecture we'll look at house data now historically house data has been one of the most common machine learning and statistical uh tasks it's in like every stats 101 course so you may have seen this before I kind of hoped you have um and when we look through it we're gonna I'm gonna point out the real data that you can use to try this out in a competition like kaggle there's a kaggle where you can download house prices from Ames Iowa and try and guess how much they you know how much they should sell for things like that right people actually make money on that by the way right not everybody sometimes hard if you followed the news right Zillow tried to sell houses and estimate them and flip them and they lost a bunch of money Blackstone if you care about private Equity I managed to make money doing that right they bought houses and they were able to predict how much they were going to sell them at so this may be trivial as it seems um is there actually problems that people care about okay anyway so we need an abstraction so we have this X and we have this y we need something else to make this a supervised problem and we talked about it yesterday we're given a training set okay so what is a training set well formally it's just going to be a set of pairs this is just introducing notation you have an X1 and a y1 okay now comma all the way x n to y n all right all right now x i here is going to live in x it's summing coding of an image maybe it's the bits that are in the image that would be a reasonable encoding maybe it's RGB values that's in there if it's text maybe it's the ASCII characters or Unicode characters that are in there it's some bag of bits okay now we're later going to abstract this away and almost always work in a vector space we'll talk about where those Vector spaces come from but that that's kind of where the the data actually lives and y i is going to live in some set those are going to be our labels oops right so now our do given that information is we have to find a good age X to Y okay and we'll call often we call it h because it's a hypothesis all right all right now that notion of good is going to occupy a fair amount of what we worry about over the next couple of lectures what does it mean to be good right in some intuitive sense because I have these examples of x's and y's one reasonable thing I should expect is I I kind of get them more often right than random chance right that's like kind of a very basic idea of what would be good you show me an image it has a cat in it I get most of the cats right now you've used enough machine learning to know like we don't get it right all the time right and it's still useful so we'll have statistical Notions we'll try to get it right kind of uh you know on average now more advanced things like just recency biased because Tatsu was talking about it in the class before on the board you could also worry about how well you do on some groups versus other groups some groups you know you're predicting really well on but other groups have qualities and you're not predicting as well on them you could worry about that and say I want to do my prediction I only care about is being you know as well as I do on any one of these predefined groups okay so you could have multiple Notions of good we're going to stick with the simplest and basic which is like how accurate am I at the task in this okay but this mathematical framework can accommodate all of those like when you actually write it down the tweaks that I just mentioned to come up with those uh different what they're called loss functions is really really kind of straightforward mathematically they'll kind of go through the same thing okay so all I want you to take away from this is we have a training set that's what's provided to us these y eyes are going to be supervision they're in some set our goal will be to find a good h among all the possible functions and by the way the class of functions from like one space to another is enormous right so we're going to have to restrict that in some way and that's kind of the setup for supervised learning okay right now this here we will often refer to as the training set or the training data that's there and what we're really interested by the way in which is probably a little bit counterintuitive the first time you hear it is we're not doing strictly machine Learners we're not doing strictly what's called interpolation we're not just trying to predict back on the X and Y pairs that we have we're going to try and worry about how well we're going to do on a new X and a new y so why does that make sense imagine someone shows up with an image odds of that like you know they just took it with their phone right my phone is just littered with pictures of my daughters okay so if I take a new picture of my daughter and probably the label should be the same as the last thousand pictures I took but it's going to look a little different right so when I show that that picture I don't care how well I did on the last picture that I took over I care how well I did on this picture right on that on those X and Y Pairs and that's a little bit weird and that means that implicitly what we're going to assume here is that these x's and y's you should think about is drawn from a large population of images right that are out there and we want to do not we were sampling some piece of it and we want to do well on those images that are going to come in the future that's why we think about it as a prediction so it may not be great to just return the label of every X and Y we've ever seen right we have to in some way kind of generalize is the technical term to those new images okay all right so the reason we call this a prediction is we care about new X's that are not in our training set right now if you look at that and you're mathematically minded you're like how the heck did you say anything about that and hopefully you got a clue there if you don't if it doesn't make sense yet don't worry we're going to make some assumption like we randomly sampled from all of the images and how well do I do on another randomly chosed image okay that's what we're going to do in some way the set you train on though better be like the set that you evaluate on that you take your predictions on or you're out of luck if you train your model on pictures of my daughter and ask to know about cars I don't know how it's going to do right so there's clearly some link here okay now weirdly enough although I say that one of the big trends in machine learning that's going on right now in fact of course that I co-taught with Tatsu and Percy last quarter was about these large models that we just trained to predict kind of everything that's on the web and they seem to do pretty well on things okay so just want to highlight like it's there's a really strange notion of good you spend your whole life trying to think what good is if you're a machine learner okay a couple more things as I said I'm just going to go off on tangents if no one stops me all right so if Y is discrete this is just terminology so it's a discrete space we think about this as classification okay that's the terminology you could think the simplest version is yes or no does it contain a cat yes or no binary classification you could also have a bunch of different classes is it a car a plane a truck what model of car is it those are classifications they're enumerated sets the other thing which you're probably familiar with from you know calculus and we'll talk a little bit about today is when Y is continuous and this is called regression so this is an example of something that's discrete this cat yeah and the house price this is going to be an example of regression and that's what we're going to look at today in lecture three we switch and we start to look at classification which has some subtle differences okay awesome all right let's look at some data any questions about the setup or or kind of higher level questions about what it is what goes on here all right sounds good okay so let's look at some some real data here I'll try and get it all on the screen so I'm going to look at this house price data as I mentioned this is the Ames data set which follows a very famous data set just for historical reasons of Boston house prices that you can go look at and download you can download in one line into pandas if you want happy to put uh you know information online about how to do that this is real data of real houses and Ames and so what I'm showing here is like these are their real IDs I just randomly selected some to kind of make the picture pretty right just be honest and then here's their sale price right so this is their actual sale price and the data and this is their lot area this is kind of like some notion of square feet that's actually present this data set I think has something like 93 or columns inside of it I've just selected a small set of them we'll come back to that in a second now one of the things that I did here is like the first thing you should do when you're encountering a new set of data and I cannot emphasize this enough is look at it the number of times that like people especially engineers in Industry like take their data and like start running fancy stuff on it I'm like well did you did you look I still remember when I was running a machine learning team at an unnamed large company and they're like why are you sitting in the cafe just like labeling data just like looking at data sets for like days it's like I don't know what's going on I want to figure out what's actually what people are actually doing on this data set and it's it's really important okay so when you're doing your projects like first plot it so here's a plot right x-axis square feet y-axis price and clearly there's some like General upward trajectory Trend here we're going to be more precise about that in the next slide right you get bigger houses they cost maybe as you can think about it you're like that's not quite true like if it's in a really desirable neighborhood um like you know costs more and if it's in a less desirable neighborhood maybe it costs less so they're clearly other factors those are gonna be called features in a minute but this is our first model okay so let's look at one other feature so we can also look at the number of bedrooms right so you see here a plot these are categorical values that's why I put them in there I mean they're kind of continuous in some way you can still treat them as numbers so that's fine you see there's some spread you know among three bedrooms and among four bedrooms and the price is the waxes so what do we want here going back up for a second what do we want actually we want to get a function what's our hypothesis go from it goes from lot area and it predicts price okay that's just notation okay this is what we're after okay so you show me this data and my goal is to produce some age okay now they talked about there are lots of functions that could take in you know a lot areas and return sale prices it could scramble it it could do whatever it wanted it could you know go look up from an oracle whatever I wanted to do there are tons and tons of functions we're going to look at a simple restricted class of functions in just a second okay but I just want to put that in your hand like this is actually a pretty hard problem so we need some some representation for H okay so how do we represent that age now we're going to look at a class of models which is called linear although if you're a stickler you'll realize right away that they're affine I'll explain why I allow myself to cheat like that in a second okay so here's a here's a model that we could use okay okay so the idea here is you give me the variable right X1 which in this case would be like the square footage or of whatever you have and then I will multiply it by some Theta and this Theta is going to be a weight we'll call it or a parameter of the model and this is how I'm going to form my regression okay looks like a line right so far so good right now let's see if I can show you a line there's a line that does it okay okay this is that's basically that line through the data that we just looked at okay now I want to actually come one in one more second how does this actually map onto this oops scroll down sorry for the bad scrolling here I'm going to go to zero remember my H is going to look like x equals Theta 0 plus Theta 1 X1 well what does it look like just so you make sure the picture is clear this here is Theta 0 right it's where I am it's a response at zero and then this gives me the slope right this is the of slope Theta 1. okay and then when I go to predict what do I do I grab a point let's grab this one I project its value onto the X and this is where I predict its price would be right this is the price of this one does that make sense okay all right awesome okay so this looks like a relatively simple model but if you look at it like you know at this scale not so bad honestly right there's some kind of linear Trend there there's some errors or what we call residuals in a second we'll try and minimize those these errors but this is this is like our first predictive model okay and as I said it's something that you're hopefully quite familiar with just in kind of fancier notation for the moment all right awesome okay so now I'm going to go sorry for the skipping I'm going to go and say okay how do we generalize this right so imagine we had our data set we had X1 X2 so on and we have a bunch of features and I'm going to use my features for my notes but hopefully this doesn't cause you any panic I have size I have bedroom lot size and as I mentioned in the actual real data set there's like 80 90 of these things and have price okay okay and remember price is my Target this is my why and these are my x's so this is uh you know I'm just going to put numbers here don't worry about them I don't know why I wrote These in my notes but these are the ones I used just for the sake of consistency so write these 45k 30k 400 900 doesn't matter too much okay the thing that I that I care about is that this is my notation for the first data point in the second data point and this is X11 this is X12 this is the second feature okay right now I called this a linear model right but if you're a stickler and you took a bunch of things you're like no it's an affine model you have this you know Theta 0 hanging out there the way that we get around that is we're going to assume that Theta 0 for every model x0 for every model is identically one okay so that's just a convention don't stub your toe on it that is x i zero equals one and I claim you should convince yourself for one second that means that what is linear in this new set of features is my old affine models right and I just put I'm just putting a one here every place okay all right that allows me to just simplify my notation okay so what's my what's what's the model the class of models that I'm looking at here well they're linear models again with that terminology and they're going to look at Theta zero times you know x0 which we know is one plus Theta 1 times X1 Plus data and I'm going to call it D times XD okay and this equals sum J goes from zero to D Theta J times x j all right and remember I'm just going to write it again x0 equals one and NB means you know no well none okay all right now this allows me now I have a very high dimensional problem now High Dimensions don't work like low Dimensions I won't go into a whole thing about it but High dimensions are very fun and interesting spaces okay you can you can build really interesting machine learning models by taking your data doing what's called embedding it and then training a linear model on top and that actually in some areas is actually state of the art of what we know how to do so those models have potentially you know hundreds of features that are underneath the covers for us these features right now are going to be all human interpretable they're going to come from the table so when you give me a the row X1 I fill in the value this value here with 2104 I fill in this you know the X2 value and so on and as I go okay so I just fill in the values as I go that's how I form my prediction okay a little bit more notation right now if you don't remember I'm just going to introduce Vector notation here these are column vectors they're going to look like this and this is just going to save me time and space and you know fill the things okay X1 is going to be a vector 2. with X11 X2 X12 so on oop sorry about that I wanted to start at zero x 1 0 x 1 1 and so on and remember this thing is one which we've said many times and this is whatever the value is up there 2104 okay in general this is going to be the size feature the bedrooms feature and so on clear enough right these are the parameters and these are the features all right so why be so pedantic about this piece it's because we're going to use this in several different guises these parameters are going to mean different things as we change the hypothesis function over time and we just want to make sure the mapping is clear so just make sure the mapping is super crystal clear in your head of how I take a data point that looks like this and map it into a feature Vector that looks like that that's all that I care that you get out of this and then we have some different vectors and y i is going to be the price in our example is price okay now recall this notion wasn't we didn't pick this by accident this was a training example this pair x i y i is a training example this is the ice training example right just the iPhone and the set okay so far so good now I'm going to create a matrix here capital X that's going to have one row for every example oops so on X so there are n of those characters by my notation and so where does this Matrix live well there are n rows and recall because of my convention that I added a extra Dimension which I always made one it's D plus one and I'm just highlighting this and being pedanta because I don't want it to bite you when you realize like why they have D plus 1 where did it come from it's the one and this is someone who say this you taught this course many times someone's going to get bitten by it I'll say it many times okay it's uncomfortable when it happens okay so this is now I can think about my training data as a matrix awesome okay so now we have a bunch of notation I've basically bored you to death with a hundred different ways to write down your data set but I haven't answered the question that we actually cared about which is how do I find something that's good right how do I find an example of something that's good all right so now let's look at here so why do we think this line is good you remember this from how you fit it you think it's good because it makes small errors right like if it were all lying on the line right on top of the line the distance from any point to the line would be zero and we think the line was pretty good if we could kind of minimize those errors okay and this is the error this is the residual now for computational reasons and historical reasons we'll look at the squares of those residuals in just a second okay don't worry too much about that you can do everything I'm telling you with the absolute value of the things right you don't want to do the signed value of them because what does a negative error mean right you should pay a penalty is the intuition whenever you make an error all right so let's look at this all right so we're going to look at our H and I'm now going to write it sub Theta J goes from 0 to D Theta J of x j okay so now picking a good model I can actually make some sense for what do I want well I want somehow that h of theta X is approximately equal to the Y when X and Y are paired right if X and Y come from a new example you show me a new image it's a kazak cat or not that label may be opaque to me but it exists I want my prediction to be close to that y on average or for house prices you give me a new house I predict its price is as close as possible I may not get the exact dollar but it should be penalized a lot if I'm off by a million dollars maybe but not if I'm off by ten dollars right that's kind of the the intuition here right so how do I write that down the idea is I'm going to look at this function J which we're going to come to a couple of different times and it's this is that one half is just normalization I'm going to look at my data and I'm going to say take my prediction on the ice element y i and square it okay now this is our first example of a cost function and I wrote it in a really weird way but I want to come back to why I'm doing it this way okay this is also called least Square so you've probably seen this a bunch of times and that's okay and if not don't worry we'll go through it there's nothing there's nothing mysterious okay so let's unpack it so this thing here is the prediction it says you give me a point x i what's my prediction on x i some Y and that says it should be close to whatever the training set said why I was remember what we're given we're given x i and y i pairs that are together image cat house information all of its description and the price we should be close okay we're penalized more for errors that are far away I could give you a big song and dance about why this is appropriate and indeed there are lots of statistical song and dances about it but really we're doing it because it's easy to compute everything that I'm going to do you just want something that's kind of sensible right like you should be penalized more the more wrong your guess is Right roughly speaking in this example okay now what does it mean to pick a good model well our model is now determined solely by those those Theta J's right if we knew the Theta J is our model would be completely determined that was the trick I pulled on you and I said oh we're gonna how are we going to represent our hypothesis we're going to represent it in this class that means now we reduce from all the crazy functions that you could have ever dreamed up any computer program that you could ever have written that was functional to the class of functions that are represented by these weights the wild thing is there's a lot of function you can represent that way okay and we'll see that over the over the course of the class okay especially when you start to get really high Dimensions Okay cool so which one am I going to pick yeah please do constantly awesome question yeah very Advanced question so the question is hey you wrote this one half there it seems unnecessarily and potentially confusing why would you pay the cost to do it and the reason is when I take the derivative in a minute it will cancel out make my life easier okay but there's no and the other point that you made is and I love the way you said it this is exactly right we don't care I wouldn't call it that we care only about the gradient but we only care about the minimizer for the loss function so if your loss function costs 10 or cost 100 doesn't matter what you care about is what Theta minimizes it you got that concept exactly right so I hope that makes sense when we're setting up the cost function in some ways sometimes we give it an interpretation almost a debug it to understand what it's doing but really all we care about is what is the Theta when we minimize over all the thetas of J Theta this is what we're solving for right so we we basically want to solve this J Theta now as we'll see in a second for linear functions we can do this for more complicated sets of functions it's not always clear that they're even you know it exists a minimizer that we can that we can reasonably find okay right so there could be these wild functions that take bumps and everything so I'll draw one for you in a minute right when we talk about solving it but for linear functions what's amazing and why we teach the normal things you can prove what H Theta is in this example wonderful Point okay but that's the central thing we're going to set up these costs so that we get a model out we've restricted the class of what we're looking at to something that's relatively small where we can fit the parameters then we just have to minimize okay awesome right this is this this is what I mean by optimization by the way just solving this equation okay I haven't told you how we're going to solve it yet but hopefully this is good now just for leading a little bit ahead for the and also to kind of stall in case anyone wants to ask a question what we're eventually going to do is we're going to replace this J with increasingly complicated potentially functions that we're going to look at one for classification one for other statistical models but we're going to do almost everything that comes after this part to all of those models so once we kind of get it in this form where it's like a prediction and some penalty for how how poorly it's doing we may use different cost functions everything that comes next we'll be able to do for all of them okay that's why we set up all this kind of kind of elaborate notation for like fitting a line it is still by the way boggles my mind how much machine learning you can do by just fitting lines like just higher and higher dimensional lines but we can talk about that some other time okay awesome okay all right so how are we going to solve this now there are many ways to solve this if you've taken a linear algebra course you're like oh I compute the normal equations and then I'm done least Square so your Matlab or numpy person you're like oh I do you know least Square solve or whatever it is backslash whatever you want to do we're going to solve it in a way that sets us up for the rest of machine learning because machine learning will deal in functions that aren't quite as nice as linear regression quite a bit and in fact the trend has been when I like first got into machine learning and Antiquity we were all about what were called convex or bowl shaped functions just roughly we were really obsessed were we getting the right Theta right we're like statisticians like at Large Scale can we get the right Theta is there one individual Theta modern machine learning done care we don't even know if we get the right answer we don't even know how there's a paper I was reading from deepmind this morning that was like oh you should run your models longer no one noticed right how do we not know when to run the models longer we don't that's the world we live in so how does this work so imagine we want to we want to this is our cost function okay now just as an aside I want to say the linear function doesn't look like that so don't think about the linear function looks nice and bowl shaped Okay the reason that's important as I was just saying is a local minimum this is a local minimum so is this so is this roughly speaking is global when you're convex that doesn't make sense to you don't worry about it okay four convex we'll come back to that point later in the course but I just want to say like don't think of this function I'm drawing here as what happens with least squares we're just optimizing A J for right now okay all right so how are we going to do it we're going to use a very simple algorithm we're going to start with a guess which is going to be Theta 0. how did we pick this gas felt good randomly set it to zero all reasonable things to do their entire machine learning papers by the way written I've even written some which I'm not sure if I should be embarrassed or proud of that talk about how you initialize various different parts of the model okay for us though it won't matter for our least squares and some of the other models we're studying because we'll be able to get to the right solution all right so now imagine for the moment I found you a model I found your initial model well it's clearly from looking around imagine I'm just looking I'm the point and I'm looking clearly I can go down from here right so the natural greedy heuristic is compute the gradient what does the gradient look like here it looks like this oops I can make it do this fancier okay you see that good I compute the gradient and then I I walk downhill sound good right tells me to go downhill from here right I'm at whatever shape I'm at this gradient will also tell me what to do now there's some problems right just as an aside what if I were right here oh it doesn't tell me what to do but don't worry about that it's a local maximum I'd be toast but here tells me to go downhill okay now once I go downhill how far do I go again feels good I pick a value it's called a step size so my next value is going to look like this t plus 1 is going to be defined to be 5T minus some Alpha Theta J Theta T now my notation is a little bit weird here imagine it's one dimensional for the second okay compute the gradient go in the opposite direction that's all that's going on this thing here is called a learning rate embarrassingly I think I've won awards for papers that are about learning rights but they are not very well set so you just kind of pick a value for deep learning people now have all kinds of what they call adaptive optimizers if you look in the literature about how to set these values for you um you don't want to set it too big or too small there is a theory about how to do it for linear things but don't worry for you you just kind of pick a value okay just imagine like what could go wrong what happens if you pick it too big well then you kind of shoot off over here right you pick it too small then you make little bumps like this right you don't make enough progress it's not too hard to think about what what should happen here and then what happens well I get a new point this is my Theta 1 and as suggestively done here I iterate I compute the gradient and I bounce down and then hopefully I get closer oh sorry that is just a this is my notation uh for the gradient with respect to Theta this is a partial derivative with respect to Theta right so imagine it's one dimensional and I'm just setting up for the fact that I'm going to use multiple Dimensions it's literally just a gradient with respect to Theta the derivative in this case now now what I'll do is I'll compute that J for all zero to D characters and that gives me my high dimensional rule okay please uh yeah so right now I've just shown it I've just shown J as an abstract function I haven't decomposed it as a sum that's a great Point let's come back to that in one minute exactly what happens when we have a data point it's going to be my next next line other questions okay is it clear so I did actually a fair amount of work there and tricked you just so you're clear I went from one dimension to D plus one Dimensions by just changing the sub index and did them all by themselves so make sure that that sits okay with you right please yeah so how can we understand it on a graph what do you mean by on a graph like on this graph in particular awesome yeah yeah so just imagine that so the one-dimensional case carries what you need to deal with so you're in a particular basis right meaning you have like Theta 1 Theta 2. so imagine I'm standing in two dimensional space I can look down one axis and then I have a one-dimensional function then I have a gradient there that gives me the vector in this direction then imagine I turn 90 degrees orthogonally I look 90 degrees there I get another one dimensional function I compute its gradient now the gradient is actually if you look at the derivative it's actually the all those vectors put together one after the other in component but that's exactly right yeah so yeah but you're asking exactly the right questions right so just picture it as the tangent to the curve if that helps you in high Dimensions if not don't wonderful questions okay so what do I hope that you understand here's some rule you have the intuition that what it's going to do is it's going to bounce slowly downhill okay now if you start to think about high dimensions and I think this is why the question came starts to get a little weird what does it mean in high Dimensions you can imagine like something that looks like a saddle if you know like a saddle then you're like oh gosh what's going to happen when I get to the top of the saddle clearly I can go off the sides and get a little bit smaller right that would be good maybe it goes down and stops but I can get stuck on the top of the saddle too and weirdly enough it's called the saddle point don't don't worry okay sound good right we're not worrying about convergence right notice this algorithm has a very clear error mode here we found what looks like the global minimum but what if we started here we would go bounce bounce bounce and we'd find this one now how do you stop this algorithm you stop the algorithm when it's stop when this update becomes too small okay and you set that tolerance maybe you set it to What's called the machine Precision like 10 to the minus 6 16 where you set it to you know 10 to the minus eight or you want a quick solution you know 10 to the minus three or something the point is no matter what you do you're going to get stuck here with a with a descent method because it's going to go downhill and get stuck here and you're going to miss this much better solution that won't happen for linear regression we won't talk about why at this exact moment we can prove it in a little bit but for things that are bowl shaped every local minimum is a global uh minimum then we're in good shape that's why we cared so much about these things 10 12 years ago we care about them you know occasionally now less than we used to okay all right so let's compute some of those some of some of those derivatives getting back to the earlier asked question which was hey what does this mean for a sum okay all right so remember RJ had a very specular form so we're going to compute the partial derivative with respect to some sub J of J Theta okay so this is the the derivative here oops the derivative with respect to the jth component okay now we take the sum I goes from 1 to n I'm going to put the one-half inside because I can and then this is linear and we'll come back to what that means in one second okay I just did a little bit of work here not much I just rewrote the definition of j which is this sum and then I took the the partial derivative and I pushed it inside because it's linear okay we should know that gradients are linear okay now when I do that I get something actually fairly intuitive and this makes my uh you know heart sing times partial derivative Theta J of x okay now this is I cancel the 2 with the one half back to the question about kind of the cooking show preparation and that is standard by the way now look what I have here which is kind of nice this thing is basically the error but it's signed tells me which way I'm making a mistake you know kind of too high or too low right that's all that thing is this is the the misprediction or the error okay now I have the derivative with respect to the underlying function class now why did I bother to write it out this way clearly I could have skipped the step of doing this and jumped right to the end but this is this is going to be general for almost all the models we care about that's why I did this okay so what is it in a specific situation we'll recall h of theta of X was equal to Theta 0 x 0 plus Theta 1 x 1 plus you know Theta 2 x 2 plus Computing the derivative of this pretty easy it's just oops Theta j h Theta of X is x j right please superscript over X on the right here on the right here yes oh this should have a superscript oh I'm so sorry great catch this is at that data point wonderful catch thank you I generalizable because your H is normal equation or some trigonometry it could be whatever you want all I care about is this is the error times the derivative with respect to that underlying model this is a very basic version of like what looks like a chain kind of rule and we're going to use that like nobody's business so if you didn't know the chain rule before this class you will definitely know it by the end because we use it Non-Stop but yeah this is just set up for that that's why it's generalizable it's the error which is totally generalizable for any model that has to do with prediction times how you compute the derivative like what's the change of the underlying model we'll be able to generalize that and in this case it's just XJ all right so now right getting back to this what is our whole rule it looks like this Theta J Theta J T minus Alpha sum over all the data answering the earlier question at this point we're doing what's called batch gradient which we'll come back to in one second minus y i times x i j now notice I'm going to try and do some highlighting here I hope this is okay for people to see and I apologize if you're colorblind and this doesn't doesn't help you too much but these are the same okay hopefully these are distinguishable colors these J's and then the i's are the other index that's going on and these are the data points themselves okay so I look at every data point and I'm doing the jth component of each one right now by the magic of vector notation here's what I can do I just write this as this H Theta x i this doesn't change this is a vector equation okay so this is basically looping over all the J indices at once if you're unfamiliar with Vector notations one of the reasons I'm doing this quickly is I will do it second hand throughout the course it's not deep it's not like it requires a lot of stuff just requires a little bit of reps kind of repeat on that please same brand for a wonderful question so Alpha you will typically set for the for an iteration right when you take a step you typically you can change it across steps so one thing is here I've said Alpha does not depend on uh on T the iteration step but in general it usually does you usually Decay The Learning rate over time so that's just what's done in practice that's done for really good things what you don't typically do is have Alpha depend on the data points itself because then it's kind of almost functioning like a free parameter at least in classical machine learning but in both optimizers one of which was invented by our own John duchy and other folks you actually do change the alphas for every different coordinate which was you know I think his first paper was at a grad and then out of Delta so people do things like that that are a little bit more sophisticated and why they do those I'm happy to explain offline but right now just think of alpha as a constant like it's small enough that it's not going to lead you too far astray like if it were too big you jump too far and maybe you could do a little bit better but you know maybe not too much in fact there's a very very basic rule which is called with gradient descent rule is is actually very widely used very very widely used with just one alpha wonderful question and those are the right questions to ask like how does this parameter depend on what's around it start thinking like that as you go through the course that's really really helpful to understand okay so far so good so at this point we know how to fit a line which doesn't feel like a huge accomplishment maybe but I think it's pretty cool um and we fit it in this kind of obfuscated General way that's going to allow us to do more models I claim but I'll verify that in two classes this vector equation here is just showing you like all the things that we computed this is specific to the earlier point to the line right this is this gradient here is this guy those are the same that's why this model popped out we'll come back to that in a minute okay now a topic that is practically quite important for machine learning is and it was hinted at earlier is and I'll copy this equation is what do we you know do in practice so one thing that we may not like about this equation is this thing is huge in modern machine learning we'll often look at data sets that have millions or billions of points right well it's not uncommon to run models where you're like every sentence that has been emitted on the web in the last 10 years is a training example or every token right every every word and would be just enormous right at that point it'd just be a huge thing so even doing one pass over your data is potentially too much okay that's a really extreme and crazy version of that uh that's a really extreme and crazy version of that but you can also Imagine situations where you're looking at hundreds or thousands of images and you potentially want to look at fewer so we'll come to how we do that in a second sorry yeah oh it's t and t plus one it's the it's these are the steps remember we started at Theta zero so superscript uh zero was our initial guess here and then we moved from one to two to three to four and so this is just the recursive rule that takes you from Theta T to Theta t plus one e is just whatever exactly so you just imagine it as a it's a it's a it's a you know kind of recursive way to specify where at particular T and here's how we evolve to t plus one exactly right you got it perfectly exactly right so Theta T when we go back to here oops sorry I hope that's not dizzy and I always throw a way to skip without making you sick is this Vector it's just a particular instantiation of those vectors one for every of the D plus one components please stuff yeah so we will take steps as I said until we converge typically or we can take a fixed number of steps I'm eliding that because for this particular problem I can kind of give you a rule of thumb I can point you out a paper that tells you how to set Alpha in general for machine learning as I was kind of very obliquely referring to we don't actually know uh how to tell that we've converged and part of the reason is when if you knew your model was this nice bowl shaped then you could actually prove that the closer you get to the optimum the smaller your gradient is getting and you can predict kind of how far away you're going to be basic for A Nice Class of functions for nastier functions and the ones that we're going to care about more you can't do that so it doesn't make sense to say that you found the right answer and so I don't emphasize that for these models I can give you a beautiful story happy to type it up online and tell you but but in general for machine learning honestly we just like run it till it feels good like oh the curve stopped it stopped getting better and that was this deep mine paper that said hey for these really large 280 billion parameter models so their their Theta has 280 billion parameters in it they're like we didn't run it long enough if we kept running it it was better and like everyone who works in machine learning for long enough in the last five years has a situation where they forgot they were training a model hopefully like you're not paying for it on you know AWS or gcp or something and then you come back like a week later and it's doing better than you thought and that is a very strange situation so I don't have a great rule for this for your projects it will be clearer I'm telling you the real stuff though [Music] so we will only use it in the forward direction of going T to t plus one but you know you could imagine that it's reversible if you wanted oh wonderful question yeah yeah so in the sense that like if you shoot past let's go back here so if you're here and you shoot past your step is kind of too big for the gradient you kind of trust it too much then in the next iteration the gradient will point in this direction right and so you'll step back so it will actually have kind of this ping pong and that's actually a you actually want that to happen it turns out the optimal rate I mean I can bore you with this for days the optimal rate is actually when you're doing that skipping for for whatever reason yeah but it's more intuitive for people to roll down the hill yeah wonderful point you got it exactly right please uh so is it possible for the update to be zero even if H Theta of x i is not necessarily yeah so um so it's not possible for it to be exactly zero everywhere but it's possible to have gradients that are not giving you any information wonderful question absolutely wonderful question it's because it's a linear system right so that's not full rank for the linear algebra nerds wonderful question say you have like um or something right but you flip it so Theta zero is equal to zero but on the other side when you only get the local minimum over there and not the actual like exactly right yeah and that's what I'm saying like uh we used to worry about that quite a bit now we just say it's good I wish I could tell you something better than that but we'll get into why that's true but yeah when your function is in a good class and good here formally means like convex and bounded in some way then you will provably get to the right solution we'll talk about those conditions later I'm just the reason I de-emphasize them now is because modern machine learning actually works on functions that look like this not on the other class of functions and so that's less important uh for students and then you know you would rightly say like you told me all this stuff I memorized all these conditions and then I got into the workforce I'm like none of them worked and no one uses them like yeah that's true but it's you're exactly right and so people worry about initialization where do you start so that you're kind of guaranteed to get a good model in fact there were a couple of awesome Theory results and one from my group one from 10 news that said for certain class of these nasty non-convex models if you initialize in a particular way you would be guaranteed to get the right answer actually I'll show you one in week 11 a simple version of that where if you initialize cleverly there's not a unique answer but you'll get the right one every time yeah or sorry class 11 not week 11. yeah yeah people try random initialization the problem is this model the trend is for models to be really expensive so you run huge models so any one run could cost a couple million dollars like I was looking at the Amazon's gpt3 service they cost like six million dollars a month to run so like you know do you want to try to run it multiple times like if you got money go ahead but you kind of want to try and do other tricks people used to do a lot more random restarting now we've kind of it's really sad to say this is the state but we've kind of absolved like flexonomies like like if you train these models you kind of know uh you know what are the right parameters and what is everybody else using and not everyone tries and explores everything let alone how long you tune it you know what optimizers you use we kind of all use the same stuff um but we don't have great formal justification for it maybe I'm exposing too much it's not as bad as it sounds it there's like there actually are principles in this area I'm just like telling you the plates that are broken because they're more interesting to me and we're going to come back to that so the the solution is like do I wanna there's a phenomenon that a lot of people know about in machine learning which is if I take my uh my model and I exactly fit my training data maybe it won't generalize well like it'll be it'll fit to some error some noise in the data and this is roughly overfitting we cover that in lecture 10. in lecture 10 at least when I taught it last I also talked about something which is in modern machine learning we realized that actually sometimes that concern is overstated for some models there's a wonderful paper by Misha Belkin that said you can actually interpolate perfectly fit your data and optimally generalize for some classes and models so that trade-off isn't as clear for modern models as it was for old models may I just stop telling you about this stuff but yes in general overfitting is a problem you can overfit a model and believe your training data too much but it's this area is fascinating I can obviously rant about it for for weeks so wonderful questions yeah yeah this is absolutely great okay so what do I want to tell you so I don't want to tell you normal equations I thought that was pretty clear from the from the beginning so you can read about those if you want I'll type up notes Andrew's notes are great on this point but I do want to tell you this one little bit with my last couple of minutes about batch versus stochastic mini batch because this is it actually is is relevant and useful okay so when we last left off we were looking at this equation and when you notice this problem that n was really big and I just hopefully told you like n is really big and so is D the number of parameters is really big so this is expensive I wouldn't want to look at all of my training Data before I took my first step because probably my initial guess is not that good that's why I'm training a model like if my if randomly initializing the model which is something people try to do gave me good predictions I just used that so obviously I want to take as many steps as I can so here's what I'll do I use mini batches so what does mini batching do okay I won't get too formal but basically what I'll do is I'll select sum B let's say at random okay I'm being vague here what random means I wrote a bunch of papers about this you can either pick uh you know randomly select them or you can Shuffle the order and in conventional machine learning we Shuffle the order for a variety of reasons and then I pick B items so B is going to be much smaller than n okay all right and then I update I'll call it I'm going to call it this because this made me make it more clear I equals one and or actually I'm going to write it a little bit Strangely I apologize for this notation notation is better in my notes but I want to write it this way because it's easier to say x i y i that makes it more clear what's going on I hope okay what's going on so I select a bunch of indexes B and then I just compute my estimate of the gradient over them okay I could even pick B to be size one just pick a single point if someone was alluding to earlier and take a step now what are the obvious trade-offs here on one hand if I pick a step that step is really fast right if I pick a single element it's super fast to compute relative to looking at the entire data set but it's going to be noisy it's going to have low quality I may not have enough information to step in the direction I want to go on the other hand if I look at the whole data set it's going to be super accurate about what the gradient is in fact I'll compute it exactly up to numerical issues but it's super slow now what people do is they tend to pick batches that are on the smaller side right and you pick them kind of as big as you can tolerate and I won't go into the reasons for this underlying Hardware happy to answer questions about it but basically you pick batches that are kind of as many as you can get for free Modern Hardware Works kind of in parallel so you'll grab and look at like 128 examples there's not much you know more expensive than looking at one okay on a modern kind of platform now I'm using these noisy kinds of proxies and you may think am I still guaranteed to converge and the answer is effectively yes and under really really uh harsh conditions in fact yeah I'm very proud of something that my first PhD student I and and collaborators Ben Rector and Steve Wright wrote about this paper called Hog Wild which is very stupid has an exclamation point but also got a 10-year test of time award for saying that you can basically run these things in the craziest possible ways and they still converge these kind of stochastic sampling regimes okay I won't go into details about that my point is like this thing is actually fairly robust this kind of take a bunch of error estimates and step them and in fact almost all modern machine learning is geared towards what's called mini batching if you download pytorch or jacks or tensorflow or whatever you're using odds are it has native support to give you mini batches okay and that is basically just taking in oops taking an estimate this piece here and using that noisy estimate and why might that make sense well imagine your data set contains a bunch of near copies if your data set contained all copies then you would just be reading the same example and getting no information right If instead you are sampling that same example you would go you know potentially unboundedly faster and if you think about like what we're looking at when I told you images like the images on my phone for my daughter there are a lot of pictures of my daughters a lot okay I'm a regular dad I take lots of pictures so that means there's a lot of density and so machine learning operates in these regimes where you have huge dense repeated amounts of data okay all right so this is going to come back we're going to see this next time we're going to see it in particular when we start to look at various different um um loss functions we're going to generalize How We Do prediction to classification next time and then to a huge class of statistical models called exponential family models to go back to the top I skipped just to make sure you know what's here and what I skipped we went through the basic definitions we saw how to fit a line we went through batch and stochastic gradient Descent of how to solve the underlying model we set up a bunch of notation this is going to be one of the drier classes where I'm just like writing out all the bits of notation and we saw how to solve them those will all carry over to our next brand of models the normal equations if you run into problems blame me I'm happy to take a look through them they're relatively straightforward and the notes are pretty good but I'll look at Ed if you run into any problems there and happy to answer questions with that thank you so much time for your time and tension and I hope to see some of you on Monday"
"Stanford CS229 I Weighted Least Squares, Logistic regression, Newton's Method I 2022 I Lecture 3","Stanford CS229 I Weighted Least Squares, Logistic regression, Newton's Method I 2022 I Lecture 3

hello so welcome to lecture three this is going to be about classification and regression this moves us from our first task which we were doing last time which was It was kind of regression how we fit a line into a class that into a task that will look really really similar at first but we'll have a couple of subtle differences and we'll go through that which is classification remember we talked about classification was for discrete objects like is it what's the animal on this photo is it a cat a pig a horse something like that those are the types of problems that are pretty prevalent in machine learning and so we'll talk through kind of those basic issues today what we're going to do though is we're first going to start with this probabilistic view of linear regression and the reason we're going to do this is we're going to walk through again in that sitting where it's hopefully relatively familiar what's going on how we give a generative model is the the term for it for this underlying kind of optimization class for linear regression so we're going to interpret it probabilistically and that interpretation we're going to be able to use for classification and then again on Wednesday for a much richer class of models which are all these exponential family models okay and I will assert at a high level like to try and keep in your head these things all look the same we're trying to get to this an abstraction that lets us solve them do inference with them and kind of Reason about them in a similar way so this is one key building block so we'll start with that probabilistic view of linear regression okay then we're going to talk about classification and at first blush classification is going to look just like something we could solve with linear regression so I just want to make sure it's really clear in your mind when we use classification when we use linear regression and kind of what the little problems are challenges are as you go through actually doing that then we're going to introduce the Workhorse of machine learning logistic regression this is something that you know I probably use every day in some form or another it has different names and deep learning now and the way people use it sometimes called like the linear layer and soft max if you're a deep learning Aficionado don't don't worry about it but this is the kind of the standard Workhorse and we can say a ton about how linear how logistic regression works and it was not invented by Machine learning people this is a an old and classical algorithm that our statistical friends invented we use it in slightly different ways than maybe they originally intended and I can get into those in a little bit now as I mentioned there's going to be parallel structures so we're going to talk about logistic regression we're going to talk about for why not linguistic regression I mean sorry linear regression then we'll talk about logistic regression which is confusingly enough a classification algorithm although it says regression in there don't blame us blame the stats people and then once we do that we're going to parallel exactly as we did in the last lecture and talk about how to solve it okay and when we solve it you're going to be introduced to a method called Newton's method which maybe you've seen if you took a kind of a stats course at some point or a calculus course and we'll reintroduce a way to solve it and Newton's method when it's applicable is really really fast meaning it converges very quickly but each step of that algorithm we'll see is quite expensive and so it's not really appropriate for a lot of the places that machine learning people care to use it okay so the messages from this lecture if you get is kind of what is classification why does it differ from regression what is the Workhorse model and logistic regression and then a method to solve it and then we're going to come back at the end and kind of compare and contrast the different ways that people solve these different things right as last time there is a thread that started online if you feel more comfortable asking your questions anonymously last time we had a bunch of great questions I'd love to keep that going super happy to to talk with you about anything that's there but I will keep the lecture three question thread up on Ed if you want it okay awesome let's get started so we're going to start again with our friendly squares right so just to give you kind of the format you should think about these tasks you're given something and your goal is to do something with it so we're given some x i and y i right for I equals 1 to n and recall this is the training set and right and in this case x i oops x i lives in some r d or Rd plus 1 as our convention is D plus one recall because we had that convention that there was a bias term where every every single entry had a one appended to it if you remember that from last time if not don't worry just remember like why is there D plus one there it's by convention and we had a Target variable Y and this target variable which I'll I'll highlight in purple this target variable was a real valued number right so this is picture picture align for the moment okay and our goal was that we wanted to find some Theta element of also r d plus one such that Theta was the argument or very very close to it because remember ARG man we can't really solve these exactly even though we like it over Theta of sum I equals 1 to n y i minus h of theta X of I squared where h of theta X of I equals Theta dot X actually I'm just going to remove the x i because I don't care true for any example okay I'll put transpose here just to make sure it's clear we I have a DOT there but awesome okay now I'm using this slightly more General notation this h of theta and that looks like a little bit of overkill for a DOT product but we're going to use that in several ways through the next couple of lectures so apologies for that if you remember here we had this the way we Define this Theta is we said oh it minimizes the losses or the residuals squared we didn't give any justification for this and what we want to do is go in this next part go kind of one level deeper and ask this kind of why question why did we pick to minimize the sum of squares now this will introduce us to one of our favorite friends in this course which is the gaussian distribution and we'll talk about why that's a plausible thing to do right you could ask why the gaussian distribution and I can wax with you philosophical I'll give you a sense of it but we'll come back to that in one second so our goal is okay we have this equation but kind of where did it come from and by thinking about where does it come from that's going to tell us how to generalize it right that's the that's the plan for what we're up to all right so this is our first model in the class really like generative model we're going to assume that y i looks as follows it's going to equal Theta t x i I'm going to unpack this in one second plus some Epsilon I and this character here this is an error or a noise term okay all right so let's unpack this because the first time we're seeing one of these things okay so this thing here maybe this makes sense to you you're like oh there exists some true Theta that's out there that's what this is saying this model is saying there's some Theta maybe I'll call it Theta star just to make sure it's clear there's some Theta star that's out there some you know one that's hidden from us but all of my data was generated by the taking that parameter with the features and generating the Y okay this would be a situation where all of your data laid perfectly on a line right because it's just saying that like you know given the features I know exactly what y's value is now what we're saying is something that's not quite that strong that's that would be a noiseless situation we're adding in a little bit of noise and when machine Learners or statisticians say noise what they mean is like stuff we can't really explain that's the kind of thing to think about it we're modeling it up to this okay and maybe we know a little bit about the noise and we'll talk about what we might expect from noise in a second but like we expect that there's some kind of random gyration maybe this accounts for in kind of physical settings some measurement error right some classical Jitter that's underneath the covers maybe there's something where you know if you're more kind of sophisticated Bayesian you think it's you know subject to your information I only know the features X and there's some unmodeled piece that's in the noise and I'm willing to kind of minimize that okay so what are the properties you would want of this error okay so this is a forward model by the way it tells me I don't know Theta but I know how my data is generated right that's what I mean when I say generative models like I know if you give me an x i and presumably knew how to construct Epsilon you could get a y i value okay now we don't get to see Epsilon I by the way it doesn't appear in the training set it's just a mental model for how the data are actually linked together okay and that's going to be fairly important when we start to generalize these models right they have certain kinds of Errors we can characterize Okay so what are the properties that we would expect of Epsilon I okay so first notice that it's Epsilon super I that is that noise is different per Tuple it's not like there's some noise offset that was just added to all our data and shifted every single point if you like we're going to have a random model you get a random sample from that noise okay and that's what determines what the y i is okay right so what would you expect from that well it's random we probably want something where the expected value over all the Epsilon I's over over that random process is zero this is sometimes mean it's unbiased okay now this is on one hand like a deep philosophical statement and other hand kind of a trivial statement the Deep philosophical statement is we're kind of saying that these errors if you think about it's kind of unreasonable like if I average over infinitely many of them they're not going to appreciably change what the true y value is right they don't have any information that's inside the model that's really what it's saying I may still get a sample where Epsilon I is 0.1 or 0.2 or negative point one whatever their value is but on average I'm just making a statement of the population like I don't care about this value it's going to be averaged away to zero in a precise sense okay now on the other hand if it were not a zero value that would be kind of a strange thing because we have a bias term suggests that you could actually just kind of incorporate that standing bias in the Theta star okay so I don't want to go too far down that path but like operationally this is not an unreasonable thing to say and I want you to think about it as kind of a statement of information like I modeled all the features in the X's right those are all my my features the house price remember the the lot size all that stuff and there's some thing I haven't modeled maybe the house looks a little bit nicer maybe it reminds people of where they grew grew up as children maybe it was you know that there's like a famous house or something that probably doesn't count for noise but there's something about it that's unmodeled as a statement of information okay the second thing is a little bit more subtle the second thing is that the errors are independent okay now we don't always make this but this is going to allow us to do some pretty healthy mathematics and let's talk about why what would happen if it failed and what this means formally is I'm going to write down a strong form it says that these two things equal the expected value of Epsilon I and I mean it in a very strong sense if you know about various different Notions of Independence and uncorrelation don't worry okay all right this is for I not equal to J that's what that little little piece is here let me write it bigger so you see it 4i not equal to J okay what does this mean okay so if you remember your notion of Independence what we're saying here is that they're statistically independent and I mean in a really strong sense like knowing the error for one Tuple doesn't tell me anything about the error for another Tuple this is consistent with my earlier kind of interpretation of Errors like how much information I know about it if I did know something about that error and I could model it then I'd have to have a kind of a more complicated model here okay all right any questions about this so far all right awesome okay now with this setup there's one other thing so so far I made two assumptions at this stage I hope you think they're like plausible to make progress and that's one of the things by the way about machine learning that I think people kind of get uncomfortable about I certainly was when I first started in kind of statistical modeling that like you're like well is that really true ah kind of not the right question to ask it's like is it a useful assumption to make to make progress like what am I giving up by making it which is a much harder thing to assess it's not ever true like if you look at real errors they're very infrequently gaussian distributed right that's kind of terrifying why do we use it everywhere well it still works pretty well because we're not assuming too much about the underlying data now if you know something about that data we'll come back in the next lecture and tell you how to put more information about the error but I just want you to get a sense like okay these are kind of strange modeling constructs now one thing that we'll care about which is a function of this is how noisy is it we need a measure of noise and so a natural thing to assume is that and we we can relax this assumption let's imagine that they're all kind of a uniform background of Noise Okay so everyone has the same variance this is a standard variance assumption okay the sigma squared so there's noise we don't know anything about it we know it's unbiased and we know that it has about the same magnitude it's not wildly different on some piece of our data versus others and again it's kind of like a statement of you know if you want to be really philosophical like an ignorance prior like we don't know anything so how would we know that this part of our data has more noise than the other okay it's just an assumption to make progress please [Music] until like the expected value of that square oh because it's oh yeah so this the variance here the variance formally is Epsilon squared minus the MU so normally we have a variance but I've alighted that because the the uh this first term is zero so it is actually the variance I just wrote it as the square there so it is Epsilon minus the MU the MU is just zero wonderful question thank you so much for that please so the error here is a sample from a value so it's actually a discrete value that's underneath the covers so it's like the way to picture it is like imagine I don't know I hate invoking deities but like imagine there's God and she's got her table right and then you got the value of all the x's and she has her Theta zero and she produces a y then for whatever reason she adds a little error to it that error is a specific specific scalar that changed y from 0.2 to 0.4 and I'm just saying that's the piece that we want to model and the reason we want to model that is because what we're coming to so does that make sense the types check it's a it's a scale or not not a function there the the reason we want to get to that is we're going to solve this problem kind of up to noise and we're going to worry a lot in the theory we won't do it too much in this lecture but we worry a lot in the theory of like could we solve it up to the noise floor like if you're making decisions that depend on how like noisy it is and your data is you know has a sigma squared of one and you're trying to make decisions where your values are like 0.1 apart intuitively something should be wrong there you're kind of reading into the noise so we'll worry a lot about how our procedure scale with the noise so you can kind of think about this as saying like there's some average noise and you know it's noisier or not if it were zero then our data is perfectly clean okay that's the way to think about it for now and if my explanations about like scaling and other stuff seem like obtuse and weird please don't worry about it we'll come back to a picture of the gaussian and it'll be hopefully a little bit clearer in a second okay other questions all right now here's the remarkable thing here's how the gaussian comes up it comes up for a really interesting reason which is if I tell you that I want a distribution such that uh it's unbiased and it has this Sigma squared I know it's variance and I assume nothing else about it right the bayesians used to make a lot of a lot of noise about this then that distribution is uniquely the gaussian okay so you don't have to know that in some fundamental sense but it leads you to this conclusion that is a distribution that has these two properties and you're not assuming things about how it's like third and fourth moments like if you wrote a three or a four there I don't have to assume what they are right they're they're just given okay okay so let's see um if I can skip to it okay so it turns out this is the unique distribution this is unique in some sense that doesn't really matter too much that's more like philosophical of the above and I've been a little bit too imprecise to really appreciate that but that you should kind of take away okay so this is our our friend the gaussian let's go through the notation first of what we mean so this Epsilon I here getting to the great question earlier is what it what it says is Epsilon I is drawn this is what the twiddle means drawn distributed to n of mu Sigma squared and I'm going to draw that in a second this is a normal distribution means this is the mean this is the mean of that normal distribution and this is the variance okay and here's a picture of it we'll get to it in one second okay right this is the mean right here and this is kind of how the distribution looks and so Epsilon the way it's picked is I will sample with probability proportional to the height of this curve right here maybe I pick a value here and that's how I get the Epsilon I and I'm repeatedly drawing from this underlying distribution okay that's what that's the mental model I have of Epsilon now is that actually what's going on I don't know don't know God don't know how that works but it's our model of how the world is going okay sound good all right now as we go through this a couple things this distribution is actually fairly peaked um so maybe you've seen this like a central limit theorem or something before in earlier classes at some point you've seen this idea that if I take a bunch of things and add them together they kind of converge to this distribution I won't make that statement precise but there's a reason this thing kind of comes up if you have a bunch of additive errors they end up looking when you average over all of them they end up looking gaussian okay so if you have a lot of little tiny additive errors they end up looking gaussian there's too much Philosophy for why this thing shows up if none of that matters to you don't worry it shows up and we're going to use it okay all right so just to make sure you understand this function and what it looks like here's the mean value of it if you see the sigma version so this is the square root of that Sigma squared you see that within one Sigma you have um here 63 percent of the or 69 percent of the mass these are by the way these notes you can go download the templates and they will have those things and this picture is from Wikipedia so you can also just look there please uh we will do a lot with population statistics we will not do things with sampling until a couple of key points and the difference between those two will be kind of immaterial when we do the actual salts but it's a great question yeah and if that doesn't make sense to you don't worry yeah so in our setting before when we had Epsilon I that's exactly right I should have written this for us great call this should be zero for our Epsilon I in general this is the notation so mu is equal to zero mu equals zero great point okay awesome all right another thing is this is the function right here that we're looking at now when you look at this you start to see why least squares may come up there's this quadratic looking thing in here okay there's a one-half and there's a sigma squared and whatever this is the normalizing constant that's just if I integrate the entire area under this curve this function that just makes sure that it's one that's a PDF okay that's all that thing is you'll see it a bunch of times in various different guises here okay but this is the function okay now let me unpack this notation for you you may not have seen this before this is not conditional we'll come back to this I'll hammer on this a couple of times this says the probability distribution density of Z and then this semicolon is not conditional formally it means these are the parameters of the distribution you don't condition on the parameters this may be a little bit pedantic but we will stick to this in the class you have mu and sigma Square those are like things you plug in right so why does this matter when we reason about the the normal distribution we're going to have these parameters the mean and the sigma squared we just plug them in and then it gives us a distribution okay and that's going to come back in one second when we get to what's called a likelihood function okay these are our parameters let me write that on here these are our parameters okay is the notation clear if you're familiar with conditioning this is not conditioning you can still condition you can write it in a bar and we'll do that in one or two steps you should be familiar with conditional probability for this class not the most advanced versions of it but you know basically what it does okay awesome all right so now let's write something that's conditional so what is the probability of y i given x i and as we said I'll write here Theta who or right here Theta which is our underlying parameter well it's going to equal what 1 over square root 2 pi that's the normalizing bit I'm going to move this down sorry X of and then here it's y i minus Theta x i squared over 2 Sigma Square okay so far so good okay so this is the probability distribution what does this it says given that I saw x i I saw the feature what is the probability distribution over the y i what value should I expect okay to come out of this right and that I have this Theta model which is our parameter here okay so our parameter okay we could put Sigma squared in there too okay now we'll write this in a more compact form x i okay this is the bar okay so this is now the conditional probability okay it says given that I saw x i condition on all the probability distributions under Theta this is the probability distribution over y i and I'll often write that this is this conditional distribution is n of theta t x I Sigma squared so this mouthful is the same as this mouthful does that make sense this is just notation at this point and hopefully the fact that it's a generative model kind of adds up go ahead foreign yeah so great question so here what I'm saying is I'm basically asserting by Fiat that because the only random variable is Epsilon I that x i here like so this really is there's a the difference between these characters right here is Epsilon I right that was by Fiat in the model when I did it earlier right I said oops sorry to scroll I I really wish it didn't look so nauseating but it does so because of this model I could substitute in here so this value here is nothing more than Epsilon super I and just a different guys which tells me all these pieces but it has to be conditioned on x i because I saw that like I saw that variable when I had to add it in right so that's how I get a distribution over y i wonderful questions are there more yeah yeah oh yeah so someone asked on the thing is e-i-e-i the the product of the two and the answer is yes thanks for the question someone's asking if this is the product up here and yes this is the product these are just multiplied by one another there's no hidden operation there yeah sorry the graph paper makes sure that I write in a line otherwise like I'll end up writing all crazy but I can understand it's not awesome for rendering other questions foreign okay so why did we do all this maybe I just like torturing you with notation the truth is I really don't like notation but we're going to use this in several different ways and so hopefully right now you can kind of piece it together and say like okay I kind of could see a model underneath here and what we're going to do is we're going to try and justify the optimization that we did for least squares by picking the most likely parameter so let me explain what we mean okay so before I do that notice here one fact that I've kind of hidden from you a little bit picking Theta picks a distribution let me make sure that claim is clear before I move on okay what do I mean once you tell me Theta and I have the data fixed then all the distribution over the Y Eyes Are Fixed does that make sense so in some sense by picking a Theta right once I have the sigma squared fixed I'm picking a distribution now over all what the Y I should be and that's going to be interesting because what it means is as you pick a different Theta I can compare how well does it line up with my data so intuitively right if I pick a Theta all my data lies on a line and I pick the Theta that exactly fits the line that should be much more likely than if I pick a different Theta where it's scattered my predictions are scattered all over the place and they're really far away so that the thin parts of this so let's come down and write that a little bit more precisely but that's the intuition of what's going on here okay so ask me ask me a question about that okay so for this we need a notion which will be very much used in this class which is the notion of likelihoods and this allows us to pick among many distributions okay so at first that sounds pretty fancy like how are we going to pick among these distributions right it's a huge unmeasurable class if you know what that is all this nasty stuff but we just have to pick in our situation among the different thetas that could fit our data okay and we're going to pick the one that is most likely so let's write that down right now so what is the likelihood of theta okay it's going to be the probability of all the Y's given all the X's given Theta right or can or with input Theta okay this just says How likely the date is and clearly as I vary Theta I'm going to get different scores here for How probabilistically likely all the Y's are let's break it apart if it doesn't make perfect sense what that statement means when I start to write it out mechanically hopefully you'll see how it decomposes and then please ask me a question I'm going to write something which at first may look actually I'm going to write here a bit unmotivated I can break this down into many smaller assumptions or many smaller pieces okay why is this the case why can I take the big thing and turn it into a product of the Small Things what am I using exactly I'm using Independence and the strong form of Independence which I which I kept bringing up that told me that I could write this big product over all the vectors as this as this product among all of them okay and sometimes you'll hear this referred to as the IID assumption independent and identically distributed okay all right cool please yeah so Theta there should be a zero and a sigma squared I'm being a little bit glib about what happens with the I could imagine a model this is a wonderful question thank you for letting me say this I could imagine a model where because the way I've specified the model it's implicit that mu is always zero but I haven't told you what Sigma squared is for right now imagine that Sigma squared is fixed I told it to you ahead of time so I don't have to plug it in here I could also fit it right I could look at my data and see among all the thetas that are there and all the noise levels what's the most likely one and that's actually a slightly different model but here I'm imagining that Sigma squared is fixed but it should kind of go under this rubric and you're like why are you being so sloppy about that and the reason is because later we're going to be much sloppier about it because we're going to introduce notation that says it's all the parameters in the problem okay but wonderful question you're exactly on on target for this look please foreign no no so here we have this probability because it's conditioned on X we've removed all the dependence on the data so like everyone gets to see all the data and now all that's left that's unspecified in the model is the epsilonize if they were all zeros you'd be able to get the Y eyes exactly but the only Randomness that's left is that Epsilon I that's what we're doing and that's kind of what we cheated on here when we we said this guy is really Epsilon I as well wonderful questions you folks are really on top of this the other questions please [Music] no no so there's there's really not there's really not much to to say here all that there is is we went through this model where we said why eyes are of this form now that we've conditioned on x i we know this and we're plugging in Theta so you're giving me a particular Theta to evaluate and now Epsilon I is a randomly is a random variable it has yet to be determined so there's a distribution over that the distribution of Epsilon I is given by this equation because this is exactly equal to Epsilon I and so this now gives me a say I don't know what Epsilon is but it has a distribution that looks like this so if I sampled it that's a weird statement I want to be clear that's a really weird statement it means that if I picked enough Epsilon I's I'd expect it's mean to be here whatever mu was in this case zero and I'd expect kind of the scatter plot to look like it was inside here or like actually the histogram to look kind of like this like if I bend how many were in each thing and eventually converging to this distribution that's exactly what I mean awesome questions these are great okay oops all right oh man I even had it down here all written nicely sorry um we'll go back to my messy version okay so here we've gone from from this piece to this piece and then here all I'm doing is because these are the Epsilon I's which I've we've assumed Independence in the strongest way I can move to a product okay we will do this throughout the course a lot of machine learning is based on IID because it's an okay assumption does that mean the errors are not correlated no no of course they're correlated but we're not modeling it that's all it means it's not true it's just a good model okay great now I substitute in one more thing equals product i1 to n and then I'm just going to write out the distribution Sigma 2 pi times X of so y oh sorry what did I maybe forget a minus sign why I minus Theta x i Square over two Sigma Square okay why did I do this all right so I just wrote this whole thing out oops I just wrote this whole thing out right now the reason is we don't use this so minimizing this seems like a nightmare and so what we do instead is we use a simple transformation of this which will make it nice and additive which is called the log likelihood okay all right now I want to make sure of one thing let me go back there should be a minus here I messed that up this has to be the it has to be a positive square otherwise it will spiral off to Infinity I'm sorry this isn't my lizard brain I messed that up okay clear enough they're Epsilon here right otherwise it's the wrong shape yeah please oh two questions one is there a negative in the original formula also yes period yeah I made a mistake one two three yes oh exponential function so it just means this this character here sorry about this x of X it may be more familiar to you as e to the x there's no e to the power x and x is everything in the bracket everything in the bracket wait I thought the original function was appropriate because effectively we're competing a negative scene by doing the real minus the predicted rather no it's squared right so this character's School great point it's my mistake awesome okay is that clear I don't want to make sure that's clear it's a small detail and it is in the notes maybe I don't know all right okay so far so good all right wonderful okay so we have this function with all of our with all of my bugs that I've introduced that we're catching on the Fly which is awesome we're going to introduce a new function it's going to be the log of our old function say why are you doing that and the reason is what is law what does log do for X functions well it brings the contents down right which is nice and it also uh separates out things that are products so we have a big product we take a log we turn into a big sum that sum looks a little bit more like what we were expecting intuitively from our from our least squares let me write it here so it's sum I equals 1 to n right because we turn this product into a sum 1 over Sigma 2 pi minus y i minus Theta x i Square over 2 Sigma okay I just took the log of the exponential please [Music] oh log Sigma excellent point let me move this oops yeah it should be log Sigma you can see where their typos and things I don't care about that term is going to disappear in a second but awesome awesome find yeah Okay cool so what do I care about here the thing I was just about to say is this term doesn't depend in any way on Theta right and so remember when we talked about minimizing the the loss function we're like oh if I added a constant it didn't matter and this is a constant the sigma squared that doesn't depend on my data anyway so I can just kind of toss it away in contrast this thing very much does depend on my data and Theta right comes on data and Theta is that clear yeah please submission side is sort of a blue trip so think about it like this yeah sorry awesome questions yeah wonderful okay so now what does that mean if I want to find the most likely function that corresponds to doing what I claim it corresponds to Max over Theta L of theta why is that the case Well Log is a monotone transformation right so this original thing I wanted to maximize the probability log is monotone right looks like this and all the rest and so log of theta is the same as as maximizing that then this term is just a constant we talked about how that doesn't really matter too much so I can drop that term and then I have a minus here so it's the same as minimizing over Theta 1 over 2 sum i1 to n y i minus Theta x i Square and then you say well what about that Sigma squared what happened to it well as we talked about last time it doesn't matter if you scale the loss by a constant it's still the same minimizer we don't care about the value and what is this character that's least squares okay and we call this thing j Theta right this was J Theta in the last lecture okay so what was important here I walked through this fairly slowly um and what I the reason I wanted to walk through it like this and you should run through this is because we're going to run this same Playbook again and again we're going to talk about what the error is for these these kind of linear models then we're going to try and reduce them to this likelihood computation oops this likelihood computation will almost always use the log because it turns it into an additive problem and remember stochastic gradient descent likes to work on additive problems this is of a nice form that we like to deal with and then we solve the underlying equation and so there'll be kind of this mapping that I give you a distribution and then out comes a loss function and that's going to be nearly automatic after this lecture in the next lecture to be able to do that for a pretty wide class of models then how do we solve them it turns out we're going to solve them all the same way awesome okay so is that clear is the probabilistic interpretation of least squares or fitting a line clear please please go ahead foreign [Music] so if you had Sigma Square here I'm not going to show you how to fit this right now but there's another model where you have it this is called with known variance this is what I call fixed design with known variance uh linear regression if you also don't know the sigma squared you have to learn that too and there's a parameter right Sigma comes out it doesn't come out quite nicely to the least squares formulation you have to do a little bit of extra work to estimate Sigma but you can do it and I think it may be a homework problem so I'm not going to tell you too much more about it but it's not it's not complicated yeah it shouldn't be complicated but great question for now we're assuming Sigma squared is given you do not need to make that assumption please no either way all right all good all right so um at this point we've gone through that that interpretation let me make sure if there's anything else fantastic all right let's talk about classification little primer on classification this is where we are we're going to talk about how classification works why regression isn't the thing that we would necessarily want to do in this scenario and then we're going to run the same Playbook I assert to be able to solve the model okay uh that is estimate the Theta underneath the covers all right so here what is classification what are we given we're given not surprisingly X I's and Y is no change so far 4 I equal one to n okay but y i we're going to work on binary classification in zero one okay and the values of zero and one aren't super critical you could have minus one and one I actually prefer that because it makes some of the math a little bit nicer that's not what we're doing in the course you could have just categorical values there are discrete encodings of the variables okay now we often think in terminology why I also like the minus one we call this often the negative class this is just convention right there's no intrinsic meaning to these things this is our model and this the positive class okay so like a negative Class A positive class could be we found the tumor right there is a tumor in this image versus the negative classes there's no tumor or this is a cat this is not a cat right we're doing binary right now you can do multi-class which we'll come to later which is you know there's a cat a dog a pig a horse right now we're just doing two okay okay great so you look at this data and you're like oh okay I plotted it you've told me there's zero one encoded so you could use basically so we can use linear algebra and Vector techniques and all the rest so zero one here's some data right and you're like oh why don't you just fit a line right like I should just like you know kind of fit a line maybe the line kind of like I don't know goes through here or something like this and it's fine right and indeed for a lot of problems if you run linear regression and just kind of say like is it closer to Cloud to one than zero and round at the end like you can get out a classifier but it kind of feels a little bit weird especially because your data there's no reason it should be nicely clustered what if like there were a blue point all the way over here or over here or way over here what's going to happen to your line well it's going to start because it's fitting those residuals to go crazier and crazier if you like in this direction right naturally it's going to skew more and more towards more of the data and so whatever decision boundary you kind of put there you're going to get into kind of stranger and stranger situations okay now that's just a motivation for why you want to treat something that's natively categorical so let's let's go through the function here maybe you've seen this in a stats course already this is logistics or logistic regression so we're going to do one trick here over linear regression our hypothesis is going to generate something of X is going to live in 0 1. okay so this is a graph here of zero one uh that we're going to get to in one second and H of theta of x is going to be written as G of theta t x where which will equal uh 1 plus e to the minus Theta TX over 1. okay now this function here G of Z equals 1 over 1 plus e of Z this is called a link function okay the terminology sometimes also called an inverse link function the literature goes back and forth doesn't really matter so you say why did you do this well our model is still going to be linear in our features but we're going to feed it through this non-linearity and that non-linearity is all of a sudden going to make sure that it kind of saturates when it gets too big and saturates when it gets too small so it's not going to have the behavior if we looked at our old data right let's go back up to our old data it's going to kind of have a function that looks more like this does that make sense but at least at a high level okay so that's the intuition okay and this function here has a special name it's the sigmoid so you may see that if you use you know modern deep learning packages you'll see sigmoids or things floating around that's what they are they're just this function that kind of Smooths it over now you may ask like why don't I use a different link function you could there are lots of different link functions to use this is by far the most popular for a variety of reasons one is that you can turn it into kind of what they call probabilistic estimates which we'll get to a little bit later please [Music] yeah let's get through great question how do we how do you do multi-class let's first get through how we actually do the binary class that's a great question you can think about a standard way to do multi-class is to do what's called One versus all if it bothers you where you say am I in class one or any other class class two or any other class and you can kind of you can put them that way there are more sophisticated schemes there's a wonderful paper from 20 2004 that talks about how those more sophisticated schemes don't always pan out and it's written in a very aggressive style which I find interesting and entertaining Anyway by a guy from the media lab Okay cool so at first this looks kind of weirdly motivated but there's there's some motivation for it which is just that it has this nice property and it's smooth and it kind of looks close to like a threshold function now the other thing when I say it's smooth is we could also Imagine the function that was like a step function that you could use that right that seems like a natural thing when you're below zero then you know return when you're when you're negative return uh you know zero when you're a positive return one right that would be a thing you could do in deep learning these are sometimes called you know there's there's you know it's a sine function the problem is the derivatives would give you no information here right if they were flat so this is smooth so like it tells you like a kind of a nice smooth transition and that will work better with modern optimization that's one thing we want out of it please let's explain what h of X and G of Z are with this functions name uh remember recall this is the same notation we had earlier this is the hypothesis sub Theta this says how do we do prediction so the way I do prediction in this model um in the logistic regression model is you've given me Theta which is some parameters that you have that chooses your model then you give me some X which recall is like your data point and then what I'm going to do is I'm going to produce a number between 0 and 1. and the way I'm going to produce it is I'm going to run I'm going to take their dot product as I was doing before and then I'm going to run it through this function and I'll come to in one second how we interpret those scores but you can think about those scores as being closer to one means I'm confident it's in the class and closer to zero means I'm not confident in the class and what I was saying is this function looks like it was picked out of a hat and it really wasn't the reason it wasn't is it has a couple of properties it's smooth and it transitions nicely between zero and one and I was trying to explain why those properties were important and so that's where H data links to this image does that make sense the key of beta transfers isn't actually equal to the thing to the right of it in the parentheses it is so so if you look G is this function here but Z is a scalar right and so it's just substituting in Theta TX for Z yeah I just wrote it this way so I'd have more room to write the numerator and then here I wrote it one over because that's the more standard way to write they're equivalent great questions please oh yeah a link function is a general class of G that you could apply that's some kind of non-linearity one that you may have seen if you ever played with a deep learning package of something called relu or rectified linear it looks like this right so there are other link functions that are out there there's probits and logits and all kinds of things we're going to use this one but I want you to be aware of it because I think you have to to try one other link function on a homework and the phrase is used in the literature and it's very mysterious if you don't hear it first very good questions sometimes it's also called an inverse link function that's a separate issue cool awesome all right so how do we interpret those scores now this is the twist that gets us into probabilistic modeling which we're going to generalize so we say the probability that y equals one according to the model is equal to H Theta of x now this is a testable statement okay now just to complete it also what's the probability y equals zero this is going to be proportional to X Theta of one minus H Theta of X Y because probability is sum to one okay we only have two classes now this is actually testable if you took a bunch of data and put it through and looked at the probabilities and bend them right so you took all the predictions that were between 0.5 and 0.6 and 0.6 and 0.7 and you counted them up in every bucket how many were accurate this is testable you can see if the model is what's called calibrated okay and that's very useful that like the errors are meaningful and so you can check that it's a it's more of a condition than they're right or wrong now in modern machine learning that's less important but you will hear people talk about like the probabilities or the scores that come out of these models and using those scores for something and this is what they mean they'll sometimes use the log of this which is called the logit okay but that's how we interpret what the model tells us that's why this link function is important that it's between 0 and 1. cool it doesn't damage optimization it's not obvious but it doesn't damage optimization is the other major thing okay so let's use that information to write our likelihood function the probability of Y I'll emphasize that it's a vector this time x I don't really like that notation but it's okay is well why did we get here oops well this is again the independence assumption kind of rearing it's you know ugly or not head right we're able to go from the entire data set to a product over all the terms nothing surprising there and then we'll write this in one form which hopefully makes a little bit of sense x i this seems like a cheat but it's actually okay 1 minus H Theta okay so why does this seem like a cheat it's a weird way to do it okay but it'll come become nice in a second so what I'm writing here is I'm saying the probability is the probability I said that it was true now what is y i when y i is 1 I select this term because this term is zero right so think about when Y is one this character is one this character is zero so this goes to one and this is the only term that matters when the true label is is one it's exactly reversed okay when it's zero I should say when it's zero this term is one and then this term goes away does that make sense just think through like the cases y i zero or y i is one so far so good so it's kind of like encoding both simultaneously I get to see why I so really only one is present but it just makes my arithmetic a little bit cleaner below does that encoding make sense cool right all right so let's take the log of L Theta we're doing exactly the same thing that we did before one to n now I'm going to write it write this out it's one i y I log H Theta x i plus 1 minus y i log and oops one minus H Theta x i okay so so far so good but now notice this is in exactly the form that I need for SGD to run that's pretty wild okay this is just a sum over everything I can just write gradient descent or anything else I wanted in terms of the thetas and I'm all good these are these are functioned underneath the covers now one other thing which I won't arrive but you can see very easily from this okay so just just to be clear same recipe I want to write down what I mean by same recipe we have Theta t plus one equals Theta t minus Alpha Theta I J Theta now when we do this something oh so right now actually we're sorry we're gonna do gradient Ascent because where this is still maximizing probability we haven't pulled out a negative term sorry about that but one interesting thing pops out so you should verify this we'll see if we can do it we won't do it in class but you should see if you can you can do this Theta J of come on of L Theta equals the sum I goes from 1 to n of y i minus H Theta x i times x i j okay so this is pretty miraculous if you look at this what it says is if I look actually at this underlying function and I take the derivative it comes out in exactly the same form that we had when we were doing linear regression it's your prediction error times the X I now the prediction itself is different right before the prediction was just Theta dot x i now it's this this H function but this gives me something I'm like these models are very very similar right they're like how much error do I make then take a gradient step with respect to the data that that tries to minimize that error and so this is the sense in which I mean like these models really all are kind of like all the same we're twisting these pieces at the edge for how we model things okay and so that means actually after you you pop all this stuff out you can use exactly the same Rule and this rule is extremely General okay and that's surprising like this rule of like I just take my predictor and then I do and I do the derivative like that's kind of shocking that like a large class of models and in fact that's what we're going to generalize in the next lecture to make sure that we understand exactly the breadth of that any questions to this part of what we're talking about please oh right great question so remember the reason that we got this minus sign here sorry to go back oh the minus sign was our was our Nemesis the last time that popped out and so this turned our Max into a min right we didn't have a minus sign here and we were maximizing the the loss that we put in and I didn't I didn't oops I didn't change anything so when we were we never had a minus sign pop out of here but when you actually go through and see it a minus sign does pop out you have to take my word for it or you just do the calculation see okay but here that's why we have gradient descent because we're maximizing the loss not minimizing the underlying function please oh this is XJ is the jth component so here I did this J's are the same so I was taking the derivative with respect to the jth component of theta and so that's the underlying derivative same way if you remember in the linear regression we calculated the the derivative with respect to each component independently exactly right well if this is yeah this is the exactly right yeah I don't have anything to add now um yeah oh great great question why I is the label so and here that's what I was saying I said in an extremely confusing way for some reason so why I is fixed I know why I get to see why I it's a label so when I have y i is equal to zero then this term is zero so this thing is just one and this is the term that's inside it's like a switch statement when y i is 1 which I get to see right for the values that it's one then only then this statement goes away and I and I have only this character if I could draw faster with colors that's a terrible color here we go does that make sense so it switches between both based on what Y is it's just a compact encoding of both cases that's why it's a little bit awkward yeah great question please so we compute it so the the thing here is this derivative here this this log and I'll make sure this is clear we wanna like this I'm asserting I haven't shown you this but the way you compute it is you take the derivative you put it inside you say Okay y i doesn't depend on Theta this term does you compute the derivative of this character internally and then that is what I'm saying you can simplify it to down here but you you compute it like that's up to you to do once the model is in this form you just use the rules of calculus to compute it yeah exactly this follows this notation I will use reflexively without thinking this is the log likelihood I mean change colors Yep this is the log likelihood and this is the likelihood likelihood okay this is on the probabilities this is on the logs of them great questions cool please so we'll almost always do gradient descent so one rule of the course or not rule but one thing is you typically use the log likelihoods for a variety of reasons but one is that they're nice for optimization and so that's what this link is meant to show you very cool awesome all right I will pause a second all right so at this phase right now we've seen another model what we're going to see next lecture is we're going to generalize this with a little bit more math and so the thing is it's like you know maybe it's a terrible terrible metaphor but like you know a frog with boiling water or whatever but like you're you're getting more complexity and you're not noticing it right we started at lines like I know how to fit a line then we had some probability distributions okay they came in and then we started these predictors that were actually instead of just giving you a value of regression they were actually giving you a probability we interpreted those as log likelihoods now we're going to make next lecture we're going to make the probabilities more complex and that's going to allow us to to generalize before we do that I want to show you one other thing which is the which is the Newton's method which is another solution method so that we can compare and contrast with stochastic gradient descent and give you a chance to ask questions since we were a little rushed at the end of last lecture because I screwed up okay sound good all right okay so let's talk about Newton's method we're now talking about forget about your modeling side now we're talking about optimization right so Newton's method is the following we're going to be given some f from RD to D it's got to be a scalar out okay at this point actually doesn't but and what we want to do is we want to find f of x equals to zero so it's root finding this is in general a hard and intractable problem sometimes it will work sometimes it won't work okay if it were really nasty function if it's continuous great so why does this have anything to do with what we care about just as an aside remember your your uh thinking here if you want to minimize say l Theta and it's convex or has a nice shape that's the same as L Prime theta equals zero sorry if that's too small right so if I want to minimize something it's the same as finding or finding the roots of its derivative right assuming it's convex double shaped okay so they're related clearly all right so what how does this thing actually work so the idea here is and probably you've seen this method at some point maybe in a cowgirls class or somewhere and it's it's a good method but it's it has trouble with machine learning and I want to talk about why okay so here we have Theta zero we take our guess F of theta zero and we compute the gradient okay remember the derivative in this case because it's one dimensional is the directional of maximal increase right that's the function of the way the function is increasing that's what the the derivative is actually giving us now what we're going to do is we're going to follow the derivative to where it crosses the axis okay so our guess is going to be Theta 1. now you should kind of convince yourself it's not true everywhere but almost always if you think about picture of function in your head that crosses zero this is going to be a pretty interesting way to find the zeros and to get closer in fact this method is insanely fast for a large class of functions it's called what's called quadratically faster which I'll emphasize again but it means you get two you get twice the number of digits of precision as you run it's wildly fast okay when it runs in terms of steps that it takes okay so this distance we want to call Delta so Theta 1 is going to be equal to Theta 0 minus Delta what is Delta how far do we step then we have an algorithm here right and then we'll repeat it right just to be clear we go up here we would compute another derivative and so on and we would we would zoom in on this this would be our Theta 1. or Theta 2. okay so we have to solve this key step so how big is this well if we look at it F of theta 0 equals F Prime of theta zero times Delta okay it's just a triangle rise over run that's all I'm doing that's it okay that means Delta equals F Prime of theta 0 which I'll write in kind of an obfuscated way times F Theta of zero okay oops that is that looks terrible let me erase that there you go inverse okay so I have to do an inversion okay so this gives us the rule Theta t plus 1 equals Theta t minus F of theta t over F of theta T Prime okay this is our roof finding algorithm and as I said this thing converges crazy crazy fast right so it like you know if you go to 0.1 then the next iteration will be 0.01 this is error going to be 0.0001 right this is the error this is what quadratic speed means that's insane you don't have that many digits on your uh you know on your device like it'll you know get to machine Precision very very quickly now this algorithm looks great like in one Dimensions it's quite good but there's a problem with it when we scale up to higher dimensions and the problem we scale up to higher Dimensions is right here the way that you write the higher dimensional version of this rule is Theta t plus 1 equals Theta t minus and I'm going to write the the typical way we do this H inverse gradient and I'm going to put it in the way that we would use it okay so what have I done here let me let me unpack this it's a little bit obtuse okay so when we want to when we want to generalize to vectors so we want to generalize and use for minimization we get here so Theta is remember our front end Rd plus one right L Theta becomes our our F Theta right as we were using it above right I've written it as a gradient with respect to this this thing here if you remember your your Calculus this is the Hessian how big is that thing well it's in D plus 1 cross D Plus 1. all right this thing is small this thing oh it's not small this thing is small it's in a this is an RD now one thing that is thing definitely by the way if you don't remember what the hestian is H I J equals in this case uh I'll write it as L Theta is the Matrix of second partial derivatives all right okay so it's all the mixed partial derivatives okay if you remember this from your Calculus class if not don't worry I'm sure a brief refresher will be fine a couple things that are great about this algorithm first is notice there's nothing there there's no step size there's no Alpha this thing just runs okay this is a great algorithm in machine learning Antiquity like 2003 and 4 2006 people use this algorithm because it carried over from statisticians this is how statisticians would solve logistic regression so like if you go into R I think up until very recently maybe even still and you say like solve logistic regression it will use this algorithm under the covers and the reason is it will get super super accurate right it'll get all that fill up all your digits and be very very efficient in terms of how many steps it takes but each one of those steps for a machine learning problem could blow out your memory if you imagine you have a machine learning model that has a billion parameters a billion squared is a lot right it's huge it will blow out your system and so people have ways of relaxing this uh over time that they try to get more information in but it hasn't historically been worth it okay so does this algorithm make sense do you recall this algorithm happy to answer questions about it all right so let's do a rough comparison and if you want please ask questions about I mean anything I guess but you know relevant's fine oh I think I have a chart for this okay okay so let's look back at the methods we've seen because I want to put them in context we saw this SGD algorithm pure SGD every iteration took one data point right we looked so I want to compare the methods so we're clear on the method name how much they cost per iteration that is every time I take a step and change the model that's what I mean by pre iteration how much compute do they do as a result of that per iteration and how many steps do they take to the error to the air right this is kind of the conversion straight off so SGD has a pretty bad estimate of the underlying gradient but you can go super fast relative to the size of the model so you take many of them and you kind of make up for it in some situations so let's see this so to compute here this is proportional to D does not depend on the size of your data set there are situations where you can train these models you don't even see all of your data you only sample a small actually there are models that people pay money for that they have huge huge collections of of data and they only ran on the first like 30 percent of it and they released the model there they're like hey we sampled from it it was fine right it was fast enough if you ran even a single episode of batch gradient descent batch batch grading descent you would have to look at all the data points which would potentially be much much slower okay so it takes time at least o-n-d put data's here although I don't really mean them formally okay and then there's Newton's method Newton's method also looks at all data points it's extremely expensive we won't talk about how expensive you can get it slightly down from this but and I've written papers with other people and a lot of people have tried to improve this method but it literally like it's huge it has this D Squared is going to kill you okay you can try and get around it because you have to compute the interaction those those remember these partial differentials here these are like the interactions between every pair of variables that you have that's a lot of information quadratically more okay so that's where their D Squared is okay now these things are super fast I'm gonna be a little bit glib here because I'm not going to State the true precise running time this thing is really fast if you want to get to Epsilon error you take log 1 over Epsilon steps right potentially a little bit less than that too but it's fine that's super fast okay like you have an Epsilon of 10 to the minus 16 log of 10 to the minus 16 is like take a couple hundred steps and you're done that's wild SGD a couple hundred steps it's likely still spitting out random values okay in contrast at The Other Extreme this is like Epsilon to the minus two this is like Epsilon to the minus one approximately these are very vague Notions these are this is only under some considerations I just want to give you Engineers intuition of how well these work okay and the point is is that like there's a clear trade-off here of how expensive each one of these points are versus how many steps you have to take so if like you had a Computing device that made it absolutely instantaneous to look at all end data points simultaneously then maybe batch gradient descent makes sense because you would just take steps really really fast if you had an oracle that could compute Hessians right which is what people tried to do for a while and compute them really really quickly then you would prefer this algorithm right so it's a trade-off between size and speed now we tend to operate as machine Learners in situations increasingly whether it's a good idea or not I happen to like the idea of huge huge models trillions of parameters are the new like thing people care about it's wild Computing a trillion squared if you thought a billion squared was bigger trillion squared is bigger right but about a factor of a million it's huge so we can't run on those those kinds of models and we tend to train on data sets that are much much larger over time now that how what much larger means changes every generation of Hardware like every two to four years what we mean by that changes but like you know we train on the web like all emissions or like all of the we still can't train on all the video right there's more video that's put out there than we can possibly train on we would like to be able to do that eventually Hardware will catch up and hopefully the same dumb algorithms will work that's our that's what we're praying right now we have no we have no justification for that statement okay now the one thing that I highlighted last time is G the one thing I highlighted last time was there's a little character that squeezes in here called mini batch right we talked about this very briefly what minibatch does is instead of said electing one points it randomly selects B points now its estimate is somehow better than SGD but not not kind of theoretically doesn't change the curve the point is for modern machine learning you can do a sample of B things in parallel in the same wall clock time that you can do one and that's what's kind of distorted us to use these batch methods really candidly there's a little bit of error reduction in the noise like you get a better estimate of the gradient but really it's because it's free for the compute device the way a GPU works or any of these kind of batch kind of parallel systems you put in D points they can do them all in parallel so that's the thing that's lurking under the covers because after the election people ask me like well why would you prefer that you're still taking the same number of steps in batch gradient versus SGD and it's because of this parallelism that's underneath the covers we can we've built Big parallel machines Humanity right like your phone has an ungodly number of teraflops in it like super computer level teraflops some number of years ago and we'll continue on that thing so that you can get your photos tagged I mean I don't know that's how it works anyway so those are why we do mini batch right okay so far so good all right any other questions all right so the last thing I'll just put on the on the thing here is in classical stats these were all things that people cared about classical stats D was really small and N was you know kind of moderate size like if you look at where if you talk to like your friends in the social scientists who are like you know maybe they're not doing the same thing now but for a while ago when they would solve these models when they would solve these models D would be like a hundred right and they really cared what their responses were down to you know very very fine levels you have to run for a really long time to get that level of accuracy for SGD what machine learning is about in a really fundamental way is like taking these kind of bigger models and kind of solving them approximately and weirdly enough they end up pretty robust which is something kind of horrifying we don't understand it please oh yeah so it means at least so I mean like the Big O style notation it means asymptotically it grows at least this fast it's a little bit slower but I don't want to kind of get into it wonderful question yeah please [Music] awesome question so one one access that you could also look on here is how how well do they handle kind of noise in the data and SGD turns out to be kind of remarkably robust and you know there's some versions where you can prove this so when you're optimizing so there's folklore around this we'll talk a little bit about this but SGD because it's noisy some there's some belief that it doesn't get stuck in local Minima as frequently as some of the other algorithms do if you imagine this picture right right here imagine that it went back up all right then somehow like you're using all these second order information to race you down to the closest local Minima that's potentially not what you wanted the entire time so there's some folklore theory that says SGD is a little bit better and I say folklore because we can only nail that down in some cases they're basically theorems that say of the form like if your data looks like this then this happens or for certain things I'll show you in a couple weeks this happens for like solving certain Matrix equations you can you can prove that it happens there so that's another access the other axis which you may think about if you're an Optimizer is how numerically stable is the underlying algorithm and here's the thing that's pretty wild about machine learning the trend has been not to make more numerically stable things so if you care about how a computer works you have doubles inside double precisions floating Point numbers now you know if you saw nvidia's last announcement they're going down to fp8 which means instead of 64 bits for a number they're using only eight bits there are people right now training with integers those methods we really only know how to do over SGD because these methods you you kind of Can't Get Enough meaningful information in there so there's another argument about how kind of statistically robust they are um and how numerically stable they are they're not very numerically stable there's a lot of tricks we're pretty primitive there compared to like you know the optimizers of the world but yeah wonderful questions awesome fantastic um any other questions okay great so we're going to end a little bit early today uh what we're going to do uh on the and and know like in general like you have to stay at l445 but today we got through it um next time we're going to talk about our exponential models these are going to be models that have a more complex link function and allow us to model more of the world that's around us and kind of interesting noise things see you have fun I'll stick around for a couple questions"
"Stanford CS229 Machine Learning I Exponential family, Generalized Linear Models I 2022 I Lecture 4","Stanford CS229 Machine Learning I Exponential family, Generalized Linear Models I 2022 I Lecture 4

all right uh let's get started so today we're finishing our first kind of piece of the tour of the very basics of supervised uh learning we're going to talk about these family of models that's called the exponential family of models and why we care about these models as we talked about is they're going to allow us to generalize basically the kinds of models that we were using before to a wider range of error modes okay of different kinds of errors and they'll also come back and play a starring role when we start to tackle unsupervised learning where we don't have access to a Target variable and the underlying mechanics that we'll use here will set us up quite nicely for that just in terms of pacing in terms of the course what happens is I go away for a little while telling you is going to come in and talk to you about a bunch of different things kernels svms and deep learning and then I'll come back to teach you a little bit about the unsupervised learning piece which is again like a you know two-week block where we kind of see you know kind of from first principles how those things work and that area I have to say just as a plug for what's coming um that that area is something that's been really exciting kind of thrilling over the last couple of years how much we can learn without label data or with really weak sources of data that's been like a revolution in machine learning so hopefully I can share some of that excitement with you okay uh the threat is up the lecture threat is up on Ed if you want to ask questions as usual and everything's online um before the lecture I didn't put out a template today because I'm going to handwrite almost everything so all right so what are we doing today we're going to learn about these exponential family models and they're basically going to be what you already know with slightly fancier notation basically we've been ramping up the fancier notation each time and generalizing as appropriate how we want to go through them we'll go do the definition and the motivation and the definition at first will look like at simultaneously like a little bit weird and like kind of like oh that doesn't really mean anything it doesn't have any content and then it will also look to you like it's impossible to satisfy and that's kind of true right so these are fairly interesting objects we're going to be looking at but they have a very nice kind of canonical form we'll then do a bunch of examples a couple of different examples and the notes Here the master notes here are really good I would definitely recommend going through them the type notes just so you work through a couple of details this is something that like you're going to get like a high level piece of how we go through it just do the calculations once and you will be convinced of like all the different claims that are in the lecture if you try to reason about them without doing the calculations it just makes your life more difficult than it needs to be so just just go through it once it shouldn't take uh take too long but I'll give you kind of a high level tour today so we'll do that definition of motivation we'll do a couple of examples and then last time we were talking about uh this question of how do we deal with multiple classes right last time we were talking about binary classes yes or no now if we want to have multiple classes out there you want to know if there's a dog or a pig or a horse or whatever in there this is called multi-class classification and we'll talk a little bit about our friends softmax what you'll really get out of this is that it will look kind of to you in the end like oh okay that seems pretty reasonable but you'll learn about some encoding that is fairly widespread called the one hot encoding which you would need practically if you were going to actually use any of these kind of things I believe also your homework is out but I don't don't quote me on that I think it's out now and I saw there were some questions I had a class about that any other questions before we get started oh please oh yeah I'm the wrong person but you're free to ask like just tell you like I don't know oh yeah yeah so we are blessed at Stanford with many great things we have wonderful weather we have like incredible faculty we have the best students on the planet um and we're also going to create course support until I have no idea all right so let's see what's next Okay so the exponential family now I want to be clear like I have mixed feelings about how I present this because on one hand I want to convey to you like The Unbelievable historical significance of this and why you should know this uh kind of by rights of machine learning on the other side like there's an argument to be made that a lot of modern machine learning is not going to use this this formulation in this framework but if you start to read papers it's canonical enough that it will come up in various different places okay so I don't want you thinking like oh this is all you can do when you model machine learning this is like where the field is stopped it's important historically it is very nice to understand it has a bunch of properties we care about but it's it's not the state of the art right it's not what we it's not what I go home and you know use exponential family models it's weird that I go home and use any of these things but but and I do um but it's not this one okay all right so what we want to do uh is we want to have the following idea here is that and this is why it was it's so beautiful and and will come back as a form that we want to think about if P has a special form which I'll show below special form then some questions come for free some uh you know inference learning come for free now what do I mean by for free I mean what you already know automatically applies to them okay and when we start to worry about more complicated models this will form like a subroutine that we'll use again and again like oh if we can reduce it to that form then we're in we're in good shape that'll be the way we get to things like unsupervised learning all right now the form looks like this okay now one thing I should I should highlight as I go through this so this is the data which you know you already know this character so data labels this thing is called the natural parameters okay and I'll Define the form in a second the reason I want to highlight that is one place where you're likely to get kind of tripped up when we go through this is that there are kind of three sets of parameters and I'll come back to that later so if you're confused there'll be natural parameters canonical parameters whatever doesn't matter you'll see them all written down at one point and that will kind of explain the mappings between them but there are many different names for parameters in this lecture and and that's like the essence to understand it and the reason that's important is the natural parameters if you like are so we can write this form this functional form P of Y exponential this is where it gets the exponential name T of Y minus a of okay so I'm going to unpack this for a second okay now this form says basically my probability distribution factors if you like or can be written in this form not every probability distribution can okay the way you show that a probability distribution can be written in this form is you write it in this form okay there's no secret shortcut here right you have to be able to express it as some linear thing in the parameter so this is the parameters here these natural parameters times some T of Y I'll unpack that in one second what T of Y is minus this thing which is the partition function okay so that's what it says it says that your your function right there are many functions that are in the world that could be probability densities this one has this technical form okay we'll unpack this this shouldn't be like obvious that these things are important or exist so T of Y is called the sufficient statistics efficient statistics now in this course primarily we'll use T of y equal to y we won't kind of massage the data but you can kind of think about t of Y as capturing everything that's relevant to your data right these are the things that you're modeling that are in your data and so in this example like we're keeping everything T of Y being equal to y means just it's y itself okay now another thing people get confused about this is necessarily the same dimension foreign right why is that well we take their dot product okay this is a DOT product here you can think about this I'll write it above just so you clear also you could write it like this if that's more clear to you it's an inner product between the two okay so to take the inner product and for it to be meaningful those have to be vectors of exactly the same Dimension okay so you have if you want to have so many parameters you have to have so many sufficient statistics right okay so far so good B of Y is called the base measure it's not the most critical element here but you need it okay the ex the intellectual content of that is that b depends on y but it does not depend on not depend on beta okay so it says basically if you like what's going on here is this term has all the interactions with Y this term has some interactions with ETA and the only way that ETA and and t y interact is through this term okay it's really a statement about how they interact these functions are pretty powerful right those are just arbitrary functions but when they interact they interact in a linear way sometimes these are generalized linear models okay all right keep staring at it this character A is often called The Log partition function now this is a weird comment that I'm going to make okay and it does not depend on well it's friend it doesn't depend on y just as I just said does not depend on why okay now this thing is picked effectively as a way of normalizing the distribution so it's a probability so it sums to one when I integrate it or I sum up over the discrete values all the Y's it's going to sum to one so that may make you think that a in some way is like not the star of the show you know it's just this thing that kind of like adds up everything but actually this log partition function contains almost all the information it turns out of the function and that's a weird thing but it's true okay so talk about that so this contains a and then you have this linear interaction term where the statistics interact with the data okay now just to make sure it's clear Y A and B are scalars scalar functions so a of ETA B of Y are scalars okay just to make sure the types are clear and these two characters have the same dimension okay so far so good all right so let's highlight where everyone where all these characters are so you just see them visually same and wise okay so far so good all right now here's the crazy thing many of the distributions that you've encountered in your life I don't know how often you encounter distributions but if you encounter them at a relative frequency a lot of them are of this form and that was a huge win for statistics because they said all the stuff we're doing on distributions a lot of it can be mapped into this what looks like as I said a trivial statement and the same time seems also impossible that it would be there so let's look at some examples and see how we map into this form probably you're thinking about you are going through some distributions in your head if you look at them and you're like I'm not sure that it is of that form so let's look at the simplest version of that and see that actually yep these things are of the form so let's look at some examples okay before doing that example I just want to are there any questions about about the content of this and I'm happy to defer so please feel free to ask a question and I can tell you to defer if there's something else clear enough please don't worry about X they'll come back in a minute yeah yeah wonderful wonderful Point yeah there's no X here there's just a y which is your data there's no x's and features and they're going to make it they're going to make a an appearance uh later they will be one of these parameters it's not given it's remember it's our semicolon those are the parameters remember there's a bar which says given which is condition and there's a semicolon which says these are parameters yeah see there's these Ada are the parameters of the model right and and they're going to sweep up all that nasty notation that I talked about last time wonderful questions please why does that mean Q5 is also a Spirit uh and some of the examples that we'll see you later yes yeah it doesn't need to be yeah just for just to make my life a little bit simpler in this in this class it's not a requirement of the model yeah awesome wonderful questions okay this is a requirement just so we're clear these have to have the same dimensions otherwise it doesn't make sense I'm not saying something deep I'm just saying like otherwise it will your brain should segful like what am I taking a thing and multiplying it by okay right because this has this entire expression has to be a scalar for it to make sense okay great so let's take a look at an example so probably the first example that you should think about I would guess are bernoullis right Bernoulli random variables so what are these so we're going to have some Phi probability of an event right I guess the most common thing people use is like flipping a biased coin right for some probability that it's had some probability that's tail something like this so here when we've seen this before Phi it had this form remember this form that we used maybe this looks a little bit familiar one minus y okay so the probability of Y uh you know for one is Phi and when Y is one um this term when Y is zero it's this term okay just a compact way to write it P and 1 minus p is all I'm writing here if you think about kind of a heads Tails distribution all right so this is not obviously of the form let's go get the form let's go get our friend from up above how are we going to put it in this form okay and I'll draw a box around it all right now when we do this we are like well we got to get an X somewhere right I mean that seems pretty natural so we're gonna we're gonna put an X in there and we're gonna X above y log Phi plus 1 minus y log 1 minus 5. okay so far so good we're making a little bit of progress well the problem we have right now is the Y's are interacting in two places right so what do we have to do well we have to bring all the Y's together because the Y's and the fives the parameters like they're not eight is yet well we'll give ourselves some freedom but intuitively like the parameter should all be looped all be somewhere together okay so what is that going to be 5 1 minus 5 plus log 1 by 5. now we seem to be in kind of the right situation because we have this character here which kind of looks like the interaction terms right Y and the and model parameters are interacting and we have something here that's isolated that's just a function of the model parameters okay now it's going to turn out that Phi is not going to be equal to Eta right because this thing here is not of the right functional form so we got to figure that out so what's our what's our best guess for what this ETA will be well why not what it looks like 1 minus 5. okay so we want to set to to show that it's of the right form we want us we postulate that this thing is actually equal to Eta okay now that means this thing here has to be some function of ETA right now that seems at one hand kind of obvious right because it only depends on Phi and that's a local transformation for Phi but we're going to solve for it explicitly if that kind of hand waving implicit function theorem kind of argument bothers you okay right so here our goal is we're going to take Ty as again getting equal to y we take ETA that way and we want to understand what is the value of a and what we claim it is right a of ETA or of Nu is we claim that that's going to be minus log of 1 by 5. all right let's let's check that claim right that shouldn't be hard and so why does that work well copy this guy so this goes to well I just take e of both sides 1 by 5 then I move this thing across right so that I can I can do whatever I like 1 minus 5. equals Phi then I want to make sure I get all the files on one side so I get 5 or 5 times e ETA plus one and I'm off to the races right so now I have 5 is equal to 1 plus e to the minus ETA over one okay now this means by the way that log of 1 minus Phi well that's going to be equal to oh sorry did I screw that up let's make sure I didn't screw that up I'm just going to redo this piece just to make sure because I didn't do it in my notes um so it's going to be oh no it's one I'm right e to the FI okay so now I take 1 minus this right so that's going to be equal log of this is a lot of arithmetic to do in one Peak setting but this is going to be log of uh 1 plus e to the ETA okay so far so good so why does that satisfy me because that's a function of ETA right now as I said it's kind of straightforward from here because these things are just functions of each other but this is technically what we needed to do to show that these things are actually equal and in fact we're in good shape now okay so what am I saying I'm saying like this seems trivial on one hand because you're like wow I could just put in whatever I want but you can't put in whatever you want you have to first separate out the interaction between Y and the firm and the form and then you have to be able to pull out the term that depends only on on new here the parameters does that make sense let's see another example please [Music] right I mean a is a function yeah right that's just a function of a just this happens to be a different one okay now here's what's weird okay I don't know if I should really mention this but you should look this thing actually remember I told you all the action was in there you can kind of look at this encoding this is the log partition function so a log is what we expected this one plus e n thing well if you think about like the different weights on the spaces this is actually encoding the fact that there's like a positive and a negative State and try and think about why that might actually be the case in fact if we start to compute the derivative of this thing it's actually the expected value of y that turns out not to be an accident okay all right okay before we get into that let's go down one more okay so what did I hope that you got it from this it's a tedious thing to walk through you can should walk through these on your own there are three examples you should also walk through the logistic kinds of examples and the others basically the whole thing is I want to make it super clear what the statement means I don't expect anything here to be mind-blowing I don't think like our use of fractions is gonna you know change your lives I'm just saying that like this is the content of the statement clear right you give me a probability distribution in one form I'm going to translate it into a different problem into a same functional form such that it has you know satisfies these conditions and then in that case this Ada is now the this Ada is now What's called the natural parameters okay and you're typically not given the function in the natural parameters right and you're going to be responsible on homework and in other places to do this and the reason why is if you can do this mapping then a bunch of stuff gets easy inference gets easy learning gets easy because now it turns out that you can show and we'll talk about in a second you can do gradient descent on these parameters and it's going to be concave and that's wild that you can solve all these models the same way okay so I just want to make sure that the functional form is clear and the reason we're doing it is because it's going to simplify some stuff please yeah awesome so could you use the T function to massage in the form now in this class if you find yourself doing that too aggressively you've probably done something wrong just as like a heads up because we don't we don't use it too much but yeah you could do that in t if t or like expressed in some way and you were only modeling a piece of it uh as a result of this and saying the probability distribution didn't depend on one part like T was a projection you could do that too but this is pretty hard to get around so I think you're thinking in exactly the right way kind of like how do I get around and break this and basically what it's saying is your interactions are arbitrary and why arbitrary arbitrary and new and the interactions between them occur in the X right and that are linear and once they have this linear interaction term whatever the function T is those sufficient statistics that's what you're modeling up to and some folks asked and I answer some Advanced questions on the on Ed which none of you are responsible to know about deriving things like uh why is logistic regression calibrated in certain ways in data those features those sufficient statistics are what feed into those arguments so up to these sufficient statistics this is how well you do so when you play the game of massaging it you're either throwing away information uh or you're not in which case you know that's what we're doing here yeah so it's a modeling choice you could do it nothing that presents you but it means something about what you're doing underneath the covers okay this is awesome so so now you learn something which again hopefully seems sort of trivial you're like oh I can take some I can take that distribution of like heads and tails and put it into this weird functional form and that would be interesting because one thing you should test your understanding of is can you now given a bunch of samples from heads and tails like estimate the underlying parameters right Computing derivatives here seems a little bit nasty right these are like fives and y's that are up in things it's like it looks like a weird function once I put it in this form all of a sudden like it's nice and convex and life is good but I have to look at estimating this parameter not the original one is that making sense please ask me a question if not all right we'll see one more example and we'll come back to it this is the only important thing that from from what we need to do now let's look at the gaussian example two we'll only do these two examples okay this is the gaussian with fixed variants this one's really good because remember what the probability of Y is going to look like sorry one over two pi X and there is a negative y minus mu Square over two Sigma Square okay let's make Sigma Square equal to one actually make my life a little easier okay just for no reason okay now how do we get that in our favorite form so let me go copy our favorite form seems to be pretty close right I mean we're in pretty good shape what do we need to do well we have to factor it in some way so this constant we can absorb anywhere we don't care about that we have to somehow pull this thing apart okay so how are we going to do that so we're going to put the 1 over 2 pi square root 2 pi here and then we're going to pull out the E minus y squared over 2. okay so I'm just going to factor this right this is going to be minus y squared plus u squared minus 2 mu y over one half that's what this term is going to be right let's just straight multiplication so I factor out the e y squared what does that leave me with leaves me with X of this character mu y minus one half mu squared oops minus one half mu squared okay so what are our natural parameters well ETO is Mu which is why I accidentally wrote it right at the beginning which would have been a little bit weird to do T of Y and a of n equals one half ETA Square oop mu squared okay wow no yeah okay right so because this is Mu now again notice I differentiate this thing what's the expected value of this of this character well it's mu right but when I differentiate a y I get exactly back mu which is kind of interesting this is Trivial here in the last example it was non-trivial it was like a weird function that I differentiated and I got back the actual probability distribution right which is kind of bizarre that I would get that back okay by the way if that's not clear differentiate dysfunction with respect to Ada and then see what you get okay does that make sense okay so just make sure we're verifying everything Yep this looks right uh this is my B term oh sorry this is my B term I'll highlight in different colors this is b y this thing is the part log partition function okay and what I'm just trying to highlight is like this thing contains like a lot of information about the distribution in both of the examples we've seen please no it's a wonderful question so right now we're worried about the case where Y is going to be a scalar which makes our lives a little bit easier and so we're going to look at that but you just have to have that the t y and n and ETA actually resolve to a scalar that they're the same type so if you have if your y has multiple dimensions then you need more natural parameters that's actually pretty important because um that that's why they're and why we call them natural like it's like your problem has you know Dimension D then you need deep free parameters [Music] foreign there is not no you could the thing is I wanted to have Sigma squared be um fixed like I didn't want it to change per data point that was important and so it was easier to just write one there if you put if you put Sigma squared in here and it was just a constant then you would just push it into the appropriate spots and be done they would just fold into this if Sigma squared were something that we're changing per data point right like we were trying to estimate for every data point not only it's mean but it's possible variance like I give you a temperature reading and I say like from all the data I've seen I think it's 30 degrees but I know that I haven't seen enough data so I'm like plus or minus two degrees if I've seen tons of data and I'm very confident I'll say plus or minus 0.1 degrees right that is an estimation where Sigma is part of the model and then you would have another free parameter for it great question and try and write that out I don't know if that example is written out in the notes if you get stuck or whatever please send me a note I'll write it up for you on it does that make sense that capture your question awesome okay yeah awesome so two questions on the on the live thread um the examples to go through on your own are the ones in the hand and the typed written notes which contain these in one more but just go through them on your own like I'll just wax pull whatever one minute if you haven't studied for a mathematically minded course the way that I always do it did it I still actually read textbooks and course notes and everything else is I read them I watch the lecture or whatever it is then I try and remember those key spots and I try to derive them myself it's the fastest way to figure out what didn't stick so if you're like oh I can write the derivative in two minutes and you walk away well okay you know everything for the lecture if you get stuck it's a really good signal that you don't do it if you don't do it and then that builds over time something that you thought was trivial and you didn't actually put the time into will end up biting you right just lesson that I learned from too many years at the University why is the derivative equal to the expected value I'm not going to prove that I will just assert it here I just want to show that oops sorry I just want you to observe that in both cases it's true it's a wonderful question okay it just takes a little bit of arithmetic to show it's a wonderful question yeah yeah okay so I'm just gonna I'm gonna put those assertions in here now so why do we care about this form first is what we said inference is easy okay the expected value of y given ETA is the partial derivative with respect to the Natural parameters of a of n okay I would encourage you to compute this on all the examples okay you don't want it proving the general thing just takes a little bit of extra thing but on all the examples so far you've seen it's clearly true and then why that's true in general is because it's the log of the sum of all the possible outcomes proving it for continuous stuff takes a little bit more effort okay so don't don't worry about it okay but we've seen this as true okay and this this pattern holds the variance is also the second derivative of n okay now this pattern you may think holds like oh the third power is good no it doesn't work that way okay just these first two that's the only ones that matter okay these are the only ones that work okay so why is this so interesting to me one because once you take your distribution whatever the crazy distribution is however wild it is and distributions can get pretty insane there's you know uncountably many of them you put them into this form you basically have a mechanical procedure to do inference and to do uh you know variance estimation inference more important to us but that also means that you can do learning right and in fact learning is well defined okay in particular this function is going to be concave in data okay well let me write it this way the mle remember we did the maximum likelihood estimator for all those things previously is concave okay please that helps yeah so that is definitely a piece of it you have to do one extra step but you're exactly on the right thing that's exactly how you can go and prove it just compute it directly right and see that it's positive the second derivative is positive everywhere then it's um then it's comebacks but yeah so you have to there's a negative in front but that's a monster wonderful yeah exactly right that's that's if you remember last time the way we framed all of our estimation problems was take the log likelihood that was there L of theta and then use gradient descent on that and this is basically saying that the resulting formulation if I use the natural parameters is always guaranteed to be concave please yeah so the thing is you can compute this directly by looking at exactly when you compute the derivative and and pull it out the way that you do it I'm not going to prove it in class but I'll just tell you like it's not a mysterious statement what you do is you look at all of the for the discrete case you look at the fact that it's a log partition means it's the sum over all the possible worlds meaning all the possible ways that y could be assigned right so there's going to be a term in there for each one of them because it sums over all of them that's what it does so maybe this is getting way too abstract and mysterious maybe this is better to prove so if I look here look at this this part of the distribution this may not sum to one right if I just started to do it so like if I took and summed over y oops if I sum y of this expression let's call this g y it may not equal 1. it's not guaranteed to be equal to one okay because it's just some collection of values and some other stuff this thing uh is the the function this ETA here makes sure that it's equal to one so it's the scalar as we were talking about before that make sure whatever this sums to it's going to divide over it so it's sometimes written as 1 over Z the part the part partition function however that means that the way you get it one derivation of it is you sum over all the different values so it is like the sum of everything over one is actually what so this character has to be equal to this right because it has to cancel out it has to be actually equal to 1 when I compute it and that means that it's basically of the form a sum over all the possible values of Y and so if you compute the derivative inside when you take the log of that each one of the Y's is going to come down next to exactly this functional form all right so if if that's too mysterious I didn't want to prove it because it gets like strange but I'm happy to write out the proof it's super super straightforward okay awesome so all I care about this the thing that I'm trying to get across because I'm trying to give you a guided tour I don't want to get to I want to do enough details that you see all the pieces so you can go back and understand how they work but I don't want to get bogged down in things that I don't think are like super critical for you to understand um and also I don't want to do things that I think you should do on your own because doing them mechanically will teach you better than me like inscrutably writing for the hundredth time how to prove this um but if I'm wrong and you want to watch me incredibly right I can I don't know you can log into a twitch stream or something um awesome all right all good okay so clear enough like why we did this so barring these assertions if you get your probability distribution into this form then all of a sudden you get inference and learning and a bunch of other stuff for free however one of the things is as was correctly pointed out there was a little bit of a bait and switch here we started last time to think about various different models and how we put those models inside so where did the data in X go and that's what we actually need to figure out here okay all right so let's talk about generalized linear models foreign that I really want you to get across is these are all design assumptions okay so these are all design choices that you can actually make in your model and we'll get to them and and talk about what you want to do okay and you can also think about them as assumptions all right so first what we're going to say is we're going to claim that the distribution of our label given X for some parameters Theta follows an exponential family okay now I claim without much justification here that this is an important family now you can say it's an important family because as we'll talk about many different data types like fall into this that you've seen so if you look at binary things you want to do y as binary well that's Bernoulli right so we looked at that classification if you want to have real valued wise well we have a couple of them that we could use but we saw gaussians okay if you want to do counts like you're actually counting like uh you know how many people walk by a particular uh thing or how many packets arrive at a server or something like that then that's a different distribution it's called poisson okay if it's oh you want all the whole real line right you don't want you want real positive line well there's two different fancy distributions there called gamma right you don't need to know these per se but what I'm trying to explain to you kind of proof by writing a lot and gesturing wildly is that these oh sorry exponential laplacian's also in this uh if you want distributions just one more then this is called the duration distribution so what am I trying to get across here these are probably most of the distributions you've heard of there are more that I'm not writing down but they all fall into this exponential family now one hypothesis is that we figured out this technology and that's why we described distributions this way but that's actually not true we went the other way around we were doing things ad hoc for each one of them and this tied them up and put them nicely together okay so you pick your error mode and the way you pick the error mode is it has to have the right type if you're observing binary data you want to use a Bernoulli right or something that has a binary type if you're observing real data you want to use a gaussian counts plus on so on okay so there's a data type mapping here right the second thing that you have is that your natural parameters and this is where they come back in are going to be of this following form with Theta element of r d plus one and X also element of r d plus one okay so here we're going to make the assumption that after subject to noise our model varies linearly with some underlying features okay now you're going to see later there's actually a more powerful assumption than you realize if you take your model to be very very large and have a huge number of features almost everything becomes linear in that space High dimensional geometry is very very weird so it's not like if you think in low Dimensions you're like oh there's only so many lines I can draw I can't separate out my data if you take your data and you know put it up into a huge dimensional space odds are it will be linearly separable there'll be a line through it we'll come back to that okay so this is just the thing there and then three once you've made that assumption your inference is super easy right a test time you output e of Y given X and Theta okay I said another way h of state of x equals T of Y given X okay so hopefully this makes sense okay so I'll walk through exactly what's going to happen in one second just to make sure it's it's clear but this means that we're doing this prediction and one thing that we sneaked past you was that when we went to logistic regression all of a sudden we started with these hypothesis where instead of returning like yes or no we just returned a probability distribution over it okay and that was a that was a change right when we're doing regression we returned to just a value here we're just returning actually like your probability that you think y has a particular value that's what you do for inference that's how inference is defined okay so let me make sure this is super clear how this works your data comes in as X it then goes into your linear model you compute Phi transpose x with your parameters this is your box you get out Ada right Theta T becomes ETA and the parameter you have before you feed that to your exponential model X model does whatever it does there's B's and T's and whatever in there but now you know your value for Ada or new whatever you want I'm using both and then if you want to train you do Max over five of log p y x Phi if you want to do inference you do ey X5 okay this is learning this is inference okay so all you pick here is you pick your your data you pick the features and then you run this procedure and everything's kind of you know automated for you in the sense that like um yeah in the sense that you now know a general recipe to do maximum likelihood estimation and do inference it's not obvious how to do that by the way there are scenarios where we don't know how to do maximum likelihood estimation so right now like your universe is like oh everything you've shown me you've done maximum likelihood estimation it's been really easy I'm like yeah that's fair but there's a big world out there of stuff that is hard to cram into this and so what this says is if you can put it into this form maximum likelihood estimation and inference becomes super super easy okay and learning here has a nice form data J looks like this and you can directly check this plus alpha y i minus H Theta of x i x j i so why is that the case well we saw that it was the case in all the other models you just have to go through and compute the derivative and convince yourself but now that it's in this form just Computing the derivatives with respect to Theta as you go through here because it occurs only in this Theta TX we'll get that out so one of the reasons I was delaying proving it in the special cases is because you have to do all of these transformations to put it into the right the right nice form and then when I compute the derivative my life gets really really easy if you compute it in the natural parameters it looks weird but if I mean if you computed in the original parameters it looks weird but if you compute it this way life is pretty good okay and this by the way here is always this thing right always hypothesis okay so far so good please yeah so let's actually it's a great question let me do one more example and then hopefully that will become clear about how they relate um I wanna yeah so let me just run through logistic regression then I think that'll probably maybe answer that question um so it'll show exactly how they fit together terminology okay so there's the model parameter this is Phi there's the natural parameter which in a linear model we always substitute a generalized linear model we always substitute this was the Ada before and then there's the canonical parameters parameters okay so these were like five for Bernoulli okay uh or mu and sigma Square for gaussians okay and this G here we're going to call the canonical response G inverse so G is called a canonical response okay so why do I do this I want to make sure it's really clear what all the pieces are and what's going on here there's some model parameters that's the thing that we're going to solve with respect to that's the thing that we're going to do gradient descent on that's the thing that we're going to do the H data over with respect to Theta is then dotted into the model or the data that's what this x is this is data right here that becomes the natural parameter which then goes to the which the exponential model now tells you how to operate on okay and then I can do everything I want on the natural parameter that's what tells me the distribution and so in the case we're doing logistic regression which we'll talk about in one second you have a linear thing and then your errors are of the form I make an error you know that I sometimes switch to class if you like I get the wrong answer with some some uh probability and then there's this link which are these canonical parameters and this is the content of what we're talking about here is you write them down in these canonical parameters and then they have people write them down in whatever messy form we found them and I'm asserting that a lot of them can be put into that nice exponential form through what's called this canonical response function or its link function and that allows us to treat them in the same way so this is super important because when you encounter one of these distributions you probably encounter it in this form you have to put it into this form and then that lets you do everything that we just talked about in one clean way learning becomes this nice simple rule inference becomes this nice simple rule okay awesome all right okay so let's look at logistic regression just so it's super clear what that means all right so H Theta of x well we said it's the expected value of y given X and some parameters Theta okay this Theta right so we have right so there's uh theta equals one plus one Theta minus n and then the model one over one plus Theta oops e to the minus Theta TX okay so this piece here Theta was our was our or sorry Theta should be driven sorry this was our model parameter this was after we transformed to the Natural parameters that's this character and this is what we wrote down last time so when we went to do logistic regression remember we had a loss function that looked like this this was our sigmoid or logistic function and then what I'm saying is is that we right now to get the derivatives and do everything else which I skated on last time I now no longer have to skate on because I just made it more abstract I transform it into this parameter space and I'm good okay so in one hand as I said it's totally trivial I'm just doing a transformation of how I represent the numbers but it also seems weird that I can do this and I'm inserting that in these cases I can and that's what allows us to go and treat all those different distributions in some way so if I give you some features you dot forward you learn a model and then you have an error like I'm looking at counts I'm observing them counts have a very different distribution than the errors I would expect on zero one things or the errors I would expect on a linear regression I just plug that model in that natural Model N and outfall is a pretty reasonable class of machine learning models okay and you may say the thing that people usually react to is they say something like well what if I want to do something that's more complicated than linear but linear is pretty powerful I can take my features and square them I can multiply them together it's still linear right I can take my feature five could be the you know product of the preceding seven and so this turns out to be a wildly popular class of machine learning models in fact our entire books that are written about generalized linear models there's a citation to McCullough which is the standard reference I'm not sure I advise necessarily reading it but it's uh the standard reference not because it's not a great book just like you know it's long okay please do you think that there will be more improvements in the theory of machine learning such that quadratic and other models are more applicable in these folks right now when you're extremely powerful we're going to make a lot of sense yeah I am I I really feel like I should buy you something so that's a great question so there's a lot of folks that have done through the years but as we'll talk about when you get to kernels things like polynomial kernels and exponential kernels and those are very powerful ways to model the world what I was kind of hinting at before is the linear model has this sneaky out in the back which is you get to pick the features so if you know up front that like you want it that the squares of the temperatures are more indicative than the you know the raw values you can just put that into your model and and learn more and more features and so it's not a question of like you know eventually we're going to get powerful and use those features we can use them today the crazy thing is we can reduce a lot of the things you would naturally do to this model and you'll see eventually you know someone was asking about these infinite dimensional feature models those are what kernels are you can reduce those to linear in an infinite dimension space so it's wildly like uh you know important there to do this the other bit which is also you know my personal opinion on these things one of the things that has really bitten me again and again is that simple stupid things work extraordinarily well with a given enough data and in fact the trend has been larger and larger amounts of data for the last 40 years and every time we think it's going to run out of gas and get fancy a bunch of fancy people academics start writing papers about clever ways to do X Y or Z and usually they get smoked by more data and linear stuff and there's a great paper about this that I can post of like different eras of machine learning when this happened and like the thing that's remarkable about modern machine learning to me is not how sophisticated it is but how we basically do the same thing and just pour in mountains of data like right now there's a particular model that's very hot in five years will it be hot I don't know right maybe five years ago it wasn't I guess five years ago was kind of hot but you know and then a new one will come but the pattern of like we just dump in all this information and just optimize the crap out of these parameters like that works really well so the Sim so the thing I'm trying to get across there is as you said is like the future of machine learning seems to me to be more tied with like under understanding relatively simple applications when we have huge amounts of data underneath the covers um yeah but I'm a zealot there I taught the large language model course with Tatsu last quarter so I'm a Believer you can think it's nonsense it's a personal opinion but wonderful question yeah awesome others all right so again you know same thing I'll do gaussian just just to stall to make sure this is clear because I hope you know if I'm very optimistic I'm here like oh this is obvious uh there's there's no content here I understand it perfectly if we are in that Universe like I will be extremely happy if you're like baffled please ask me a question um this is the same this is a gaussian how do I do prediction well when I have something I pick the mean value that I would have that's exactly the same piece that's here how do I do the estimation well that's exactly Theta TX that's what we were doing before when we fit a line okay same thing so all that's all I'm saying is what we've done so far in the first K lectures we've now compressed to basically one equation one schema of this thing we now know how to do inference and we know how to do learning and maybe it's tough to appreciate in the sense that you're like well I didn't encounter a thousand models before but now all these different models can be shoehorned into this and that's quite powerful and we'll use that quite a bit later when we do you know much more uh kind of advanced stuff right awesome if there are no questions I'm going to move on to multi-class all right so the last thing I want to describe here which is important for you and I think you have to use in your homework is uh how we deal with multi-class classification and I should say the trend has been in machine learning not only these big models that I was just excited and ranting about which you can you know take or leave but is that you train models on a variety of tasks more broad than even a variety of classes you train them to do many things at once in fact weirdly as I may have told you the the thing that we seem to be doing as a field right now is training a model to do something task a and then using it for task B and weirdly that makes the model more robust so the typical task that we do there by the way is predicting the next word which is a really seems like a really basic task you look at a sentence and you produce of all you view the words as individually it's just oversimplified but how it works every token is a class right so is the word cat dog whatever you just take a vocabulary of 50 000 words let's say and then you predict which one do you think is likely to be in the next space so I read the first part of the sentence and I predict this turns the entire web into a training Corpus right because now any piece of text I can evaluate in a multi-class way we train it just to do that and weird Behavior emerges like it can write pieces of code for you it can answer questions in narrative form and only when we train it on lots and lots and lots of data and it's a little bit spooky so anyway so this is what multi-class is for and this is this is actually you know something that we use every day [Music] um awesome wonderful question what is the disconnect between the power of linear models and the need for non-linear components in a neural net wonderful question so right now what we've done is we've said and the exchange was wonderful hey what about more powerful feature representations what neural Nets basically are and why they're so amazing is and they take your data if you like and they pick out what those features are what those X's should be from the underlying data and then usually on the end you just have a linear model that's you know there's little tweaks and variations but that's pretty much what you have so the question of where does The X come from you give me an image of a cat how do I get good features about images that's what the neural net is actually solving and that's where we need non-linearity but it comes in at that piece not at the prediction piece great questions all right so here we're going to look at discrete values if you're familiar with distributions up to some fixed k so we have uh cat you know dog car I don't know whatever else oh I wanted one other one oh bus okay so here K is 4. all right so I want to predict among that set it's kind of weird I promise you that you're only going to see a cat a dog a car a bus you could ask well what if I show it a horse it's not it doesn't have to predict You can predict whatever it wants in that situation right but um for this case imagine that I'm just distinguishing among those four classes or the crazy example I gave you where your classes are every word and say the English language okay all right how do you encode this it's encoded as a one-hotvector it's called a one-hot vector right and the distribution the error distribution is the thing that we just talked about to the k okay so uh such that you know sum y equals one okay equals one okay so there's a vector that's in zero one but it's precisely one thing is is uh lit up so for example you could have you know the cat one is one zero zero zero this could be cat zero one zero zero this could represent dog you get the pattern zero zero one zero this could represent car and so on okay so those vectors basically these one hot vectors they seem pretty wasteful but you don't have to store all the zeros it's not as bad as it looks um but like this is how you intuitively think mathematically think about how the data looks okay clear enough so we've reduced our problem from uh you know dealing with these categorical labels to dealing with vectors now let's try to classify them all right so let's draw a quick picture okay oops so let's imagine our data looks like this so there are some class ones which I guess are cats here there are some buses here which are fours there are some dogs here I'm just drawing all nice and clustered of course your native never really looks like this but that's okay all right so what do I what do I want in this situation okay I want lines so this is you know this is corresponding to one class you know as I said this is the cat class this is the dog class so on so how does multi-dimensional how does this uh how does this multi-class thing work that's too close the colors don't really matter but I've started so now I'm going to finish all right car bus okay so what do I want here what I need to do is I want to pick because we're looking at linear separators for this I kind of want to look at you know what I'll do is I'll pick a line that for example separates the cats from everything else okay so this will be something this will be you know Theta 1. X is equal to zero so this is the line I'm drawing here so I want to pick Theta right so that on one side are the cats and then the other side is everything else does that intuitively make sense right for Theta 2 for Theta 4 what would I like to do well I'd also like to pick something where you know here oops Theta 4 dot x equals zero so I like it so that again the buses are on one side and everything else is there now if you look at this geometrically it becomes pretty clear by the way like there's lots of choices right I could have picked here I could have picked there right what we will try and prefer sometimes called Max margin we actually prefer that it's kind of as far away from the two data points as possible it's like as close to in the middle as possible you can verify that actually makes sense and um you can verify that that's actually what we'll you'll hope will happen I get something like this all right this is X3 so in this case by the way which is really nice everything is nice and linear separable right this side has the dogs on it this side has the cars on it and so on okay so there's one line that explains uh you know that kind of can separate each one of them now the question is how do you pick right so the way you do it right is when you get a point you're going to compute its value against each one of them Theta 1 Theta 2 Theta three I'm just going to draw the first ones first three yeah data four whatever okay and these are going to give me some values okay and the values are going to be basically you can think about as like you know kind of how close are they to the various lines it's a DOT product I'm just saying like this is the line where on this side Theta is going to be yeah so for this side Theta is going to be positive and on this side it's going to be negative all over here right and so I'm just drawing like the deciding line so as you're more cat-like you're getting a higher score from this you're getting something that's like you know a larger score Azure here you're getting a negative cat score okay right so the point is each one of these things because of that is going to give you great question is going to give you some score so maybe it's a cat so it looks like you know this thing 0.1 and everybody else is like yeah I'm not really sure maybe I'm I'm kind of like borderline about all the other classes this is what you would hope would happen okay each model gives me a score and then okay so what happens yeah yeah great question so right now remember like you give me a horse and you put the horse in a center who knows right who knows what I'll get I will be able to run this procedure and it will be confused so one of the things that happens in neural Nets by the way is that they're in large scale models is when you're really high dimensional right and you have a bunch of uh a bunch of these lines and other things there will be Pockets that are actually nowhere close to anything but now look it's totally well defined this character here has a normal this way this one will get a negative it will get a negative score from everybody right now you can look and say if it gets a negative score from every single thing maybe I should be suspicious of it but that's not in general something that will happen it one of these unfortunately will be higher than the others like if it's here it may be closer to cat and so it says oh it's a a cat that was near the border or something like that and it will pick but let's get to the procedure first before talking about the exceptions okay so we have all of these then what happens we exponentiate them right 0.7 minus 0.5 0.1 so on and then this actually leads to probabilities right actually let's make this really negative because like minus 10. either minus 10. then these things are approximately like you know uh 2.1 whatever so on I get the values out and then I normalize them by summing all of them so I sum all these together so I sum all these characters sum Theta I dot X and that gives me some value Z and I divide this number by Z divide this number by Z and that's going to give me a number between 0 and 1. because I sum them up and they're all positive so 0.5 0.17 so on 2.5 up okay the point is is I compute this exponential I sum oops it should be e of this thing and then that is my normalizing term I sum it up so let me write it in a cleaner form it's a falling thing so the probability of Y equals to X a probability of y equal to k is given X and Theta has the falling form it's X of Theta K dot X over some K sum J all right X Theta J dot X okay J goes from 1 to K okay I just described the procedure in elaborate detail here but this is basically what's going on okay this is exactly what's going on it's not basically please [Music] okay so two things one is this makes sure that everything is so exactly as we were talking about before these are each think about these as each like a logistic regression model so in the logistic regression model you take this and get the natural parameter then you exponentiate it if you had an offset like if you had some function there you would still need just as we did in the exponential model you need to sum to make it a probability distribution so that's where the Z comes up so the x is because we're doing these General linear models and that gives us the nice kind of functional form that we wanted underneath the covers that we've been using in all our predictive problems for in the class or not think about logistic regression binary or not and then we have many different scores and then the procedure is just to normalize them okay and this kind of makes sense right it's like this is saying like how strong a classification I am right if these cats like if there's a cat way over here maybe this is a super cat like the clearest picture of a cat you've ever seen in that case like it should get a really high score and you should be really confident in it that's the intuition now is that always true no certainly not a picture of a horse could show up over here we hope it doesn't but mechanically is this clear what happens right yeah so exactly so this is basically a compact form of what we'd call One Versus All which is like okay how confident is my cat detector how confident is my dog detector how confident is my whatever this is car detector I don't know what that is does that make sense and so you just bake off their relative strengths that's how you do multi-class classification at once all right so how do you train this well what you're given is you're given something that looks like this say one two three put four in there you're told it's a cat you have probability one here and you have zero everywhere else this is what you're actually given that's what the label looks like okay your probability your P hat your estimate will not look like that it will actually look like oh I'm pretty confident it's a cat but I have little pieces at each one of the others one two three four okay maybe some spike okay this is at inference time what I'm trying to get across is when you actually look at these things they will give you Pro small probabilities that it's everywhere because it's doing this normalization okay now one thing that people do by the way is something they call label smoothing they take this and they push a small amount of mass everywhere else right so you take the one and you kind of say like I'm going to put a small tiny amount of mass everywhere else and that's basically to account for the fact that your labels are often wrong right you're you contain you know even very popular well-studied benchmarks will have you know three percent of their labels be wrong or something so you can imagine how you would kind of smooth and and why that would be bad when you're training a system if you're like it's definitely not a cat you're like no no there's a small possibility it's a cat I should admit that possibility that's a very different statement if you imagine those two okay awesome so what happens here well the great thing is is like this follows exactly what we've been doing the whole time so we we now introduce the this is also sometimes a cross entropy term right which is equal to this guy but this follows basically our basic recipe which is I said Pui uh equals K Times log P hat uh of Y this equals minus log pyi okay so this is the ground truth label okay so this is in case we don't do smoothing basically our loss to minimize the cross entropy is the same as minimizing the log of our expected probability and that thing has a very fancy name that's a logit and that's the you'll see these negative log likelihood things all over machine learning packages that's what they're doing okay and this equals just so we're super clear it equals minus log of x Theta I dot X over sum X Theta J dot x j goes from 1 to K okay and this is what we minimize that's it and so how do you solve this model just run gradient descent all right that's it right any questions about this um what part of that is to launch it again oh the logit is a log probability great question so this is the this is the terminology for a log probability so this thing comes up the reason I call that is you'll probably encounter that term I just wanted you to be familiar with it and you don't usually predict you don't actually write out the probability functions you don't take the X you actually just take the log of those probabilities and that's what you actually minimize and so you you will use them and they're in log scale so you'll often see like machine learning codes spit out these like you know negative seven negative four or three point two one that's what they that's what they are they're logits you'll see that term logic everywhere yes please uh because the otherwise it would be a maximization yeah it's just to make the function sign so that we have a minimization wonderful question yeah please remember your minimizing yeah what it doesn't look like it has any dependence oh awesome question what a wonderful question yeah it's hidden right here this Theta I this I is the ground truth I right that's what this is this statement is this is the ground no it's just like you picked out that one and then you get a your your loss function kind of like perfectly encodes it if you put in the label smoothing then in fact it's not just one Theta I that's there they each get a weighting associated with them as well but here that was the trick this Yi is the same as the actual ground wonderful wonderful question yeah great question yeah yeah so so here it's the Theta X and then I'm I'm taking exponential but you would also have like an X point over X spin yeah but here for this model I don't do that so if so what you're saying is an alternate model where you take all the logistic regressions and you just say like what's the probability of each one and then you compare them and I'm saying no you do it in terms of the logits here and this is how you bake them off all at the same time the difference between them is not super critical but this is the one we use but it's a great point that they're different so I did kind of by sleight of hand here I said oh this is like you're doing a logistic regression but this again is like saying so if you think about logistic regression as there's a yes class and a no class in the yes class I have the weight is X above Theta T the X this is each thing that this is the weight of every one of those possible worlds and then I'm summing over all of them and so that's why this is consistent with logistic regression imagine a null world that occurred here that said it's either the class one or it's none of the classes and so it's feature I'm just going to default to one because I don't care it could be any scalar the knife gets you exactly back to logistic regression does that make the connection clear yeah so they really are the same I just derived them in a slightly different way wonderful question please um yeah so uh if you've seen so this is a new function I guess for us potentially we have seen it because of the discussion we just had and another guys it actually is the binary cross entropy is the logistic function and so this is a generalization of that and if you've ever seen entropy or cross entropy which I think you know it doesn't matter if you've seen or not this is it right it's just a functional form that we care about it's a kind of a distance between probability distributions I care whatever reason I say this is not because I think there's something mystical here so if you haven't seen entry before I guess this would be mysterious and potentially even if you have seen entropy before because entropy is a mysterious thing um but this is the loss function that you use and it it's the reason you use it is because it generalizes in the way we just talked about I don't think you need to know any of that so please X and Y right there's no Y in this diagram but yeah uh so this here x is a two-dimensional Vector this is a little bit of a weird plot right X is cat as a two-dimensional Vector because I only have two Dimensions to draw on and so I put them there and that's why this picture looks a little bit different than maybe you're thinking about it as like a function or a curve or something but that's that's the difference higher Dimensions but it's exactly right so in higher Dimensions what you would expect is rather than lines which are one-dimensional you'd expect you know D minus one dimensional big planes that were separating everything out hyperplanes and then they would be live on one side or the other and you would care about the distance effectively there and so I'm drawing like the Contour representation of the function right yeah awesome question and then there's no the Y's are encoded as we were just discussing by which index I use right foreign oh awesome question yeah so we'll talk about PCA when I come back to join you and why we use that PCA is a method so the two things PCA fix for you is if you have X's that have different scales or have different meanings like you know if all your temperatures are between 80 and 82 degrees but they're really significant PCA is a way of centering and whitening your data meaning like subtracting off the mean and standardizing and normalizing that and so that's a technique that is very very commonly used in statistical analysis and so we'll talk about that and how to provably find that and what does justifications for here when you're doing things like image analysis actually the the methods have been more toward raw features and raw pixels over the last couple of years where you the things that we're all excited about is to try and have no hand coding in the pipeline you can talk philosophically about why we're obsessed with this but basically the newest models just take the image raw and they try to have as minimal what we call they call it inductive bias but I won't Define that term but as minimal kind of information about the model to learn from them so one weird fact that got me very excited a year or two ago was that we have one model for text and we were using that same model and getting nearly state-of-the-art accuracy on text and images and audio in a bunch of different places and that is the thing that's really interesting and it starts from the raw pixels and it learns The Edge detectors and all the stuff we used to do by hand so that's been the trend here we'll talk a little bit about more about featureprep because that stuff doesn't always work and when it breaks you want to kind of have a library to fall back on wonderful question but that'll be in like I guess week five four five oh my God what a great question yeah what if you can't draw this pretty picture it doesn't make sense yeah that's possible so what if your features are bad right what if it turns out that like your features about cats were like they sleep on couches you're like well dog sometimes sleeps on couches then you couldn't possibly separate the dogs and the cats it's a stupid example but like your features could be could be so weak that they're not able to actually separate your class uh imagine putting a lot more features in and that's why these models that have bigger and bigger Dimensions come in to separate automatically all the different classes but now your real question is like okay well okay there's a fix but I have my features and I train what happens you just make misclassifications and that's actually the default you have a small number of features and then you fit those features and you know you do the best you can so like if a cat jumped over here there's a cat that was here you just misclassified and you'd get it wrong right you go from here over to here and then you'd be toast so that does happen quite a bit sometimes due to label error and sometimes like there's a subfield of machine learning um that is kind of obsessed with this and my students write papers in it there's a great Benchmark called Wilds from Percy and Chelsea and a couple other folks on the faculty that have these places where machine learning picks out the wrong feature systematically like you take a bird that normally is on water and you photograph it with a land background and the Machine learning model is like oh that's a land burn not a water bird there's a lot of that going on so that that absolute 100 happens wonderful question any other questions all right so just to recap what do we do today we went through this exponential family of models and now we've hopefully tied in a bow the fact that we like had this method to our Madness about doing binary and then real value uh we you know we went real valued in binary because we had seen fitting lines we did classification and we tied them all up in these exponential family models we talked about why inference and learning were you know basically the the same in these models that let us generalize to a whole host of them this is the Workhorse of supervised machine learning but the questions that you're asking now are exactly the right questions where do these features come from what if the features don't fit the data how do I get more expressive things you're going to see things about kernels and sem and neural Nets in the next couple of lectures and that will tell you how do you pick your features and how do you get to these more expressive models and that will form the bulk of supervised machine learning then the next section will come to unsupervised and we'll forget the whys and figure out what structure can we get there and that's when we'll get into questions like PCA we'll get into these questions about what are called exponential family models or em models where you have a supervised predictor and an unsupervised thing and this allows you to do some pretty wild stuff like you know fit data from the from quasars and stuff and we'll we'll walk through all that stuff and the fact that we'll also have a thing on self-supervision which is a new lecture uh just this time I guess my student gave it last year but thank you thanks so much for your time attention have a wonderful rest of your week"
"Stanford CS229 Machine Learning I Gaussian discriminant analysis, Naive Bayes I 2022 I Lecture 5","Stanford CS229 Machine Learning I Gaussian discriminant analysis, Naive Bayes I 2022 I Lecture 5

from today I guess um you're gonna see me for at least a few weeks um we're gonna cover um like some more like super starting algorithms and we're going to talk about deep learning and then uh I'll pass on to Chris to talk about Express learning so so I think I'm going to be in charge in the next three or four weeks um and I'm going to use the board partly because I think there is a little bit more memory on the board right like uh like I can write you can you can review things that I can see I I wrote like a like 10 minutes ago even I don't know whether that's you know the best for everyone I think in the past I've surveyed students and someone prefer the board someone prefers the zoom the the iPad so I'll give it a try uh with this but any comments or any kind of suggestions are welcome and we we're open to change the format as well so um but for today at least I'm going to use the board um and I think the the video is able to capture the board almost the same as the the um the iPad I hope um okay so um okay so I'm going to talk about um the so-called generative learning algorithms so the next two lectures will be about this I'm going to Define what does it mean by generative learning algorithms and there are two type of General learning algorithms that we are going to cover one is called gaussian discriminative analysis I guess these are all new words that I have to Define as I'm introducing these things GDA and another type of algorithms is called naive Bayes okay so I guess let me get started so um I'll start by defining what do I mean by generative learning algorithms to kind of Define these terms I think it's useful to compare with what Chris has introduced in the last two weeks so in the last two weeks I think the type of algorithms Chris introduced we call them discriminated discriminative learning algorithms so discriminative learning algorithms so the reason we call them discriminative learning algorithms is the following so in some sense the definition is that if you model or you parametrize the relationship the conditional relationship of Y given X then we call this discriminative learning algorithms and I think you know if you recall this is the the type of algorithm the type of kind of models we considered in the last two weeks right so we model y as a linear function of X maybe Y is linear function of X Plus gaussian noise or maybe Y is linear function of X Plus exponential family or something like that right so for example uh in the most General format in the last two ways can be summarized as you think of the X the Y given X parameters where Theta this is a distribution of Y given X parametric space Theta you write this as some exponential family exponential there's some distributions in exponential family with some parameter with some input ETA and this ETA is a linear function of X right for example you can say this is a gaussian distribution which means ETA and that's just the standard linear regression right so so this is why we call them um discriminative learning algorithms and today we're going to talk about a so-called generative version generative learning algorithms the the basic idea is that you are going to model or parameters here model means is a word means basically means parameterized or you have a mathematical model for the conditional for the drawn distribution P of X comma y a joint distribution of X and Y you know using the simple chain rule you can write this as x given y times Q of Y so you model both these two quantities is this the there's some light is flashing right some should I are you bothered by it or not I'm fine with it just okay no worries okay um I think it flashes after every minute or something uh anyway so you model the drawing distribution by modeling each part of this tool and particularly you model the drawing distribution by modeling the distribution of Y and the distribution of X condition y 0 X and Y are not symmetric Y is the label X is the input and typically they'll have very different meanings y could be something like you know like the price of the house and X is uh the features the all like what you know about the houses right so like the the square feet you know the Lost size so and so forth so um and recall that axis the input or the and and this is maybe something like a label or some kind of class right if you have classification this is about cast class like maybe positive sentiment negative sentiment and so so basically this is the distribution of the input given the label and this is the distribution of the label itself and sometimes we also call this you know a prior for the label a prior for the class or the label because this is what you believe like for example suppose Y is one means positive sentiment Y is zero means you know negative sentiment right suppose you are classifying the text then this is a distribution over two labels where positive and negative and this is the the prior that you have for how many positive examples all negative samples uh are in your data set and and then um so after you uh you mode on paramax this you can learn these two distributions right you can learn the distributions learn P of x and y and p of Y we're going to say how do we learn them and after you learn both of this you are going to still solve the classification problem your goal is still the same you are trying to classify you are still trying to compute so this is the test time you are still trying to compute for example P of Y given X right you're still trying to classify what's the chance of each of the label or you probably want to get the max of y g Max you know we are going to talk about exactly um you care about um but essentially you still care about the relationship of Y uh conditional X and how do you get this you get this by base rule foreign meaning so recall that PO for example y given X is equals to P of x given y times P of Y over P of x so you know this quantity you know this quantity and your you know this I'm assuming you already learned this too right and how do you know the denominator then you can just write this as the denominator is really just the uh you take sum over y P of x given y times P of Y uh maybe just uh for the same quotation let's call this y Prime right so so this is the the standard the um total law of probabilities when you complete the marginal probability of P of X used as the denominator and then you also compute use these two volumes um on the top actually in many cases you don't necessarily have to compute the PO Box we will see exactly how it works but roughly speaking after you know these two things you can know uh after you know x given Y and Y you can know y gonna ask by doing some base rule because by the way feel free to stop me at any point you know just to reach handle us to speak um maybe first it's good so let me repeat the question is it true that the discriminative learning algorithms cannot work on non-iscriminal understand right um no I think the discriminate learning algorithms can also work with other possible uh distributions here so as long as you specify y given X and your parameterize that by some parameter Theta you can in theory you can still learn them using similar type of methodology I'm going to discuss the methodology as well uh um um but if you have expanded family then it's gonna be um there are several benefits for example you know Chris discuss this many properties nice properties of expenditure family if you don't have them then um you don't you cannot use those properties you have to use something else or you can you have to rely on optimization or sometimes it's challenging um depending on the cases but in principle you can you can have other distributions here yeah maybe let's just do this order yeah yeah this is just a general framework so the difference between the two that I can guess is actually you will have parameters because I'm going to parametrize these two distributions and and learn them so um yeah I'm going to talk about that as well foreign so this is a yes so I'm also going to discuss uh the differences you know why like what's the high level differences why you want to do this but I think it's easier to discuss those you know once I tell you a little more about concretely how this works um but so far you are right basically this is a somewhat seemingly Securities way to get P of YG Max right it's not direct right um um yeah I think I'm going to discuss discuss that you know discuss the differences you know probably later in the lecture just because you know it's easier when I have some examples okay any other questions oh would it be possible to write a little bigger please oh sure yeah that's a great suggestion I think that's probably also useful for the recording as well um and also feel free to remind me again because I this this happens to in the past as well like every time after like a few lectures I I stopped writing big you know even after a few minutes sometimes maybe um okay so this is just a very high level introduction um so and just uh so we're gonna talk about two instantiations of this general idea so one is called this one so the difference is it's really just in one case it's a continuous ax and the other case is discrete discrete X so and this continuous guys look it's called This is the gaussian discriminate analysis and the discrete acts we are going to focus on our application which is the the span filtering right so and today I think we're going to mostly talk about the continuous case and next lecture um I'm going to talk about the the spam filtering all right so so so now one example how how do we instantiate this plant um so so for GTA what you do is you say I'm going to suppose X in Rd um I'm going to drop the convention just because here I'm not going to use the uh the bias at least you know in the modeling part I'm not going to use the bias um don't worry too much about it you know it's just though we don't have the X series one I know it doesn't really matter that much so um and I'm so the main thing is I'm going to assume you can say this is assumption you can say this is a modeling assumption I'm going to assume P of x given y is a gaussian gaussian distribution so what does that really mean that really means that you write this you know you say x given Y is following some gaussian distribution with some mean and said Can converts and here note that X is a high dimensional Vector so I'm going to have a high dimensional multivariate gaussian disclosure so and we submit and be some covalent Sigma so um I think it used like a uh it's probably useful for me to briefly that that word um kind of digress a little bit to briefly talk about some Basics about Matic varied gaussian description these are just some very quick review if you haven't seen this um but I'm assuming that you know something about one-dimensional gaussian distribution so just a very quick uh digression so if you have a multi-dimensional gaussian random variable foreign so what happens is that you suppose you have some random variable Z sample from this gaussian distribution which means mu and sigma covarian Sigma so here mu is a d dimensional vector and sigma is a matrix is the so-called covariance Matrix so and the definition so the property you know you need to know is that you know as the name suggests the expectation of the Z is supposed to be the mean of the uh the mean parameter right so and the covarence of the random variable Z which is defined to be the expectation of Z minus expectation Z times Z minus expectation Z transpose this is you know is the The covariance Matrix so so this is how the you know you generate the C from this gaussian distribution parameters by these two parameters mu and sigma and the the resulting random variable Z would have these two properties the mean is Mu and covariance is sigma and and you only need two set of parameters to describe unique like a gaussian distribution um and you also know that um you also know the density of discussion distribution so the density of the gaussian distribution is something like this I don't expect you to remember the formula because I now I remember it after I teach teachers so many times but you know before I taught it I don't think I remembered in my graduate school um but the the formula is that I'm not sure whether you can see this um something like this um and here this is the is the determinant okay great I see some questions um sure maybe I'll start with the one one what does the denominator say what is the denominator so this is 2 pi to the power of theta over 2 and the determinant of Sigma so power of half and I'll write even bigger oh this is just the oh one why I'm doing this oh this is just a definition of the code bar in case you don't know the definition so thank you yeah um yeah I'm just using that as a to indicate this definition um and and by the way this formula you know I don't think we really have to remember it um the most important thing is we have a constant times some exponential of some quadratic form of Z oh Z is a vector yeah that's a good question so this is why this is a little more complicated than God than than one dimensional case right if you are familiar with the randomizable case then this will be still a scared of meal and this will be a long neck together sometimes people write a sigma square right so and the sigma will be just the um the uh um so for one master case this is just a virus and now it's the so-called coal virus and I guess you know if you are um so so in some sense if you look at Sigma of i j this is really just the the correlation between this will be uh the expectation of z z I say the ice chord minus UI times ZJ minus mu J in some sense the entry of the covariance Matrix is capturing the correlation between two coordinates of course you have to remove the mean to match the correlation the right way but you match the correlation of the two coordinates of this random variable I saw some other questions [Music] um yes so x and z are the same thing here I use Z because I want to be my abstract so in a few in later I'm going to have a little more I'm going to I have to change this a little bit uh yeah but this is just the for abstraction I use a different variable the second term is Sigma here so here because there are scalars so that's why I didn't have the transpose so this is a scalar this is a scalar right but and and here the reason why I have transpose is because then I need making Matrix so um I don't know I think some of you probably are familiar with this so I don't want to spend a lot of time on this but some of you probably not very familiar so I I think I used to have some let me show you some other pictures to get a little bit more sense on the covers um let me see how do I connect to this Maybe do they know this um how do I signals anything to them I don't see they are capturing the video the the screen um anyway um anyway these are these slides so it should be oh yeah okay great so it used to be the case that I make slides for this part of the talk I just mix three slides but then I realized that maybe it's just easier for me to show you the lecture notes because then you know where to find them again right so I'm not being lazy here it's just the uh you know it's also easier for me of course um okay so so these are some visualizations of the density function here so the first set of the just look at the figures this is a two-dimensional case right you have for these two and uh and you visualize the density of this um of the gaussian distribution so density you know always look like something like this right so and these are the cases where uh the co-virus is analytic it means that when the covariance is identity it means that so that when the covariance is identity Matrix one one one it means that there's no correlation between any pairs of coordinates right like I and J with ing another same they have no correlation and and the results the um the the kind of the shape of this um discussion is always spherical so basically when it's identity it means that you have equal strength in all the um like like a basically you just have like a on the same strength in all the directions because because like every Dimension like looks the same in some sense um so um so basically that's that's when you um uh see this kind of like very spherical shape of the density function and then you can uh the one thing is that the size of this density function depends on the the scalar in terms of the analytics right so if you have identity then I think this is uh uh the leftmost one but if you make the covalents two times bigger then your density function will be uh supported on in some sense like a most exploring a larger region so that I think that's the uh the last one um and then in the middle one you have a smaller uh covert so in some sense the covalents um is um describing how at least how large this like how large this uh um this the shape of this density function is and also it's declaring two things right one is the the size and the other thing is that what's the correlation what's the kind of orientation of the shape so maybe one way to think about this is that if you look at um the second like a rows of the figures so these are cases where the covariance Matrix are no longer identity and and they're for example in the the third figure here um so you have some correlations between two directions and then you see this the this uh this ellipsoid kind of shape is rotated into that direction just because in that direction the two corners are more correlated so it's more likely that these two chords are simultaneously bigger or smaller so that's why in that direction you have you have more kind of like mass in that direction and you can see that the the difference between these three figures is that the correlation along that direction is bigger and bigger in the first one there's no correlation in that special Direction axis equals to y direction and the second one you have a little correlation so that's why it Tails towards the direction and then in the third one you have more correlation um so so it's even more skilled in some sense any questions so I guess uh maybe another way to look at this is that you can look at the Contours right the um the like of this thing so so basically you look at the uh uh the the level set of of this density function levels that means that the set of um the basically the the set of like points with the equal or equal density right so and then you can see that you get this kind of ellipsoids and the same thing right so so um um so if you have more correlations then this ellipsoid will become you know more tail towards one special Direction so for example here I guess if you can see my pointer so this means that in this direction like a so I think if this is X1 this is X2 right so if you see this kind of Contours then it means that uh X1 X2 are equally are likely to be simultaneously bigger or smaller and so that's why they have correlation uh and uh and if you see in in this one then you're gonna have some reverse type of things right so if X1 is bigger than X2 is likely to be smaller and that's that's when you see this kind of shape um there's no need to you know exactly you know like understand this right so like uh um it's just some kind of rough intuition you know in reality you don't necessarily have to exactly you know uh visualize all of this but these are any questions you know I know sometimes this could be confusing sometimes this could be very enlightening I don't know like depending on but if we ask any questions okay okay so um okay so I guess we'll move on back to the more messy stuff um Okay cool so so I've introduced the multivari gaussian discussion and now I'm going to go back to the gaussian discriminate discriminative analysis so we are going to parameter as x given y as a gaussian distribution so so by the way I think I forgot to mention that here in both of these two cases the Y is always discrete um you can you can make white non-discrate as well but here we're only looking at a case where y the label is always discrete so now let's let me continue with the GDA so so as I said why is this great and we are only going to assume that there are two labels say one zero and one zero could mean for example um I guess I don't know why oh I I think I missed one small thing here but let me just so I guess uh one of the running examples we used to have for this uh uh for this uh gdr application is that you can think of like you have some kind of like um I guess for example cancer classification so like a benign a monument classification so you have some maybe X1 X2 two dimensional uh inputs and you see a bunch of data like this so so these are cases where you have in eye cancer think of maybe X1 and X2 as a measurement right of the patients right so maybe blood pressure or some size of certain kind of tumors and uh and and for every patient or every case you have you know whether this is a benign cancer or not right these are the the Bad Case right malignant cancers so and and the question is that you want to kind of classify this right examples into two classes right so that in the future if you see one more example one more example here maybe you want to know whether this is a benign one or not and the label is basically here let's call this label zero on this label one so that's kind of the the the the target applications we are thinking about so now I'm going to parameterize what is x given y right excellent y and I need to specify two cases where one one case is that what is the X distribution of x given y zero and the other cases there was the distribution of excellent Y is one I'm going to make both of this gaussian distribution so I'm going to assume that x given y 0 is a gaussian distribution and the gaussian distribution has mean mu zero and covalent Sigma so here I mean the high dimension okay so mu zero is in Rd in this case these two and sigma is in r d by D and for the other one my modeling assumption is the same I'm going to assume this as Mu 1. and sigma Okay so the same covariance that's just for convenience I can use different covariance Matrix but here I have to use a different mean because clearly if you fade a gaussian distribution to this part bunch of points and you have a gaussian distribution for these kind of points you're going to have different mean right so that's why I'm gonna have mu 1 here and mu 0 here so mu 1 mu zero Sigma these are the parameters yeah so this is mostly just for convenience and for like you can make them not the same Converse Matrix then um in terms of like the optimization in terms of learning this uh these things is going to be more complicated um so it's still learnable um at least with some Advanced Techniques but it's going to be more complicated so so here is really in some sense a simplification uh simplified assumption this oh um oh yeah so that's a good question so this is like this so you're given this event y 0 it was the distribution of x [Music] and that's a good question right so uh so how do you model the accident y right so in many cases you have you have many many different choices right you have different covariants you know you have different means or you can even model them in different ways so here I think you know at least you know um if you look at the data you see that the distribution of X you know Y is one and X gamma y zero sounds quite different so probably it makes sense to model them separately right if you model the whole thing as a John Golson I think it doesn't look like a ghost that's pretty much the reason okay who so so are we done so we haven't done with the modeling yet because we only model X given y right before remember that we also have to model P of Y right you need to P of y and p of X and Y to know the joint probability distribution and then you can use the base rule so how do we model the P of Y this is relatively easier because P of Y is only a distribution over two possible choices right why I only have two choices zero and one so basically you just have to have two parameters right so one parameter is supposed to model P of Y is one let's call this V and then you have P of Y is user zero let's call this one minus V what's the sum of these two has to be zero so sum of these two has to be one so um and so in some sense you say Y is from this Bernoulli distribution with parameter V this is just another way to say this okay so basically in summarize what are the parameters so the parameters I guess this goes back to the question someone asked this question right so we do have to have parameters even for the general learning algorithm and the parameters are mu zero mu one Sigma and Phi and our goal our next step would be we want to learn these parameters so that we know P of x equals y and p of Y so that we can compute P of Y and X okay so the next part is about fitting parameters okay so how do we learn the parameters from data right so we learn the parameters the general principle is maximum likelihood I think probably you know Chris has talked about this word maximum likelihood in you know probably once or twice in the previous lectures but here the maximum likelihood is a little bit different I'm going to compare what's the difference between this likelihood from the uh from the one that we discussed before so first let me Define what maximum likelihood here means in this setting so by maximum likelihood I mean I first have to Define likelihood so likelihood is this basically the chance of seeing a data given the parameters so it's a function of the parameter so if you have these parameters V mu 1 mu zero mu zero mu 1 Sigma you can Define the likelihood of these parameters this is the chance of seeing all your data foreign given the parameters so you hypothetically think that all the data generated from this dispute distribution and and you look at the what's the density of the of your data under these parameters sorry oh it's even bigger okay yeah sure okay so and let me let me also clarify the notation here so x superscript i I think probably Chris defined this right so we're going to use this through other uh throughout the uh the lecture lectures so x i y i this is the ice example so so we have this data set with other examples I'm only I'm looking at the likelihood of all of these examples under the parameter V 0 mu zero mu one Sigma so for every parameter you have a likelihood and this likelihood uh can have this sounds like kind of complicated but actually you can somewhat simplify it a little bit because um these examples are independent right so you assume that your data are drawn independently from you know uh each examples are drawn independently so then you can factorize this so this is the product of the likelihood of all the examples because you use Independence P of x i y i giving and you can even factorize this a little bit more to say that you can use the chain rule to get P of x i given y i and the parameter the parameters n times P of y i given the parameters note that not all everything depends on on everything else um okay let me I don't think I have a different color so um so here you can have some simplification because y i the distribution of Y only depends on Phi right the MU 1 mu 2 mu zero mu 1 and sigma are described are describing the conditional probability right so y only depends on fee so you don't have to write here these things right because they are the there is no such dependencies and also the same thing x i condition y I only depends on mu 0 mu 1 Sigma it doesn't depend on C so you don't have to necessary write V here even though you write it it's the same so okay so this is the um the so-called maximum likelihood and what you do is you want to say I'm going to maximize maximize this IL V mu zero mu one at the same so basically you say I'm going to maximize so um so the the Learned parameters will be the maximizer of this likelihood function all right if you need to find the maximum you want to find the parameters such that the likelihood is the is the largest so this is the so-called maximum likelihood in our context so uh why the question is why you make this independent assumptions um I think in short you know if I I have a very short answer I think this is almost like always assumed in all machine learning settings even in most advanced settings and and the reason is that um there I think there are multiple reasons you know you can say this is uh in some sense you know one thing that you can imagine is that you do clock data somewhat independently from a very large pool that's probably the simplest way to say it of course there are cases that this Independence is known to for example if you have interruptions for example suppose I first get some data from you and then I do something and then I get some other data from you and maybe these data are not no longer independent or maybe um the second time you provide me data you also look at the first time sometimes you know there's something about this dependencies especially in reinforcement learning like where you have interactions so in those cases we will drop this kind of Independence assumption but in most of the cases we do assume the data are sampled from a large pool independent 2. please find the stuff going right C is a scale yes you are right and the Visa scalar and is also a scalar in zero that's a good question do you have the freedom to change feet yeah uh we are on the fee is a parameter right so you're going to learn feet You're Gonna Learn what is the right thing from date so so how do you learn free so you have to find out the maximizer of this and the maximizer will be when we for example some training that's better using pictures [Music] um you are exactly right you are ahead of me yeah so and but we are going to prove that we are going to show that that's actually the solution so there's a reason for that no you have the very good intuition right the fee is pretty much the proportion of the the positive examples or the proportional positive examples right but we haven't actually so that's actually the case if you use this black adults is right so the maximum likelihood is you are maximizing the chance of the data given the parameters so the parameters are just some you you look at all possible parameters and you see which one which parameter can give the light the most likelihood oh right and uh so yeah like this so so this is the maximum likelihood this is the methodology we're going to use for generative learning algorithm not only today's algorithm but also for next lecture where we have other settings we still maximize select and just to compare this with the discriminative learning algorithm there we also use maximum code and but the meaning of the the phrase maximum likelihood could could be a little bit different so so here you are maximizing the so-called joint the The Joint density right of both X and Y you're maximizing the probability to see um the pairs of X and Y but for discriminative algorithms so for discriminative algorithm so what you do instead is that you're maximizing the so-called conditional likelihood even though many cases people just drop the word conditional when that is clear from the context so in the conditional likelihood is this probability of seeing the the family of labels conditioned on the inputs and the parameter state so and you can also factorize this you can factorize this as so here I'm using Theta as a generic parameter just to be abstract you know because I'm talking about the abstract setting right so you can think of this data as the linear linear model family so and you can still factorize this you can factorize this into using the independence but whatever you do you always condition X so X is considered to be a in some sense a deterministic quantity You observe you don't know how X is generated we don't care about how X is generated you just care about how Y is generated conditional you see X so um and part of the reason why you do this is that you only model y given X you need to model what's the distribution of X so there's actually no way you can do the does this maximum likelihood above in the in the in the in the discriminative sets because the only thing you model is why you can act that's the only quality that you have to parameterize form for so um so so you just go with whatever you have in some sense um right and it turns out that these two are are indeed different you know there are some relationships and there are differences you know which we'll discuss uh uh after I introduce some more uh more examples I think any questions so far this is what we yeah exactly and and maybe you know I think you know um I think this is you know probably the best discuss you know after I give some examples of on the concrete examples but in some sense you can see the differences between these two kind of algorithms and the differences between these two type of assumptions is that here you have more assumptions on the all the data right you you are making some assumptions on both X and Y and here you're only making assumptions on YG Max so so it's really about you know home like the differences will be about you know how many assumptions you impose on the on the structure of your data you know how much like in some sense like there is a whole universe right so you you cannot model everything right so so like so you choose some part of the the qualities you can model and and the decision here is you model both X and Y and decision here is human model part of it and that will cause some differences in certain cases that's a great question so the question was that you know here for the genitive algorithms we have the generative learning algorithms we have assumptions on the features right would that be a problem if you generalize to you know other examples so it depends uh it depends on whether your assumptions are correct or not in some sense right if your exact assumptions on the on the features is kind of gaussian then actually it would provide you more generalization generalization because your assumptions are correct and I need input structures and when your assumption is wrong then it would cause problems so um so actually this is um yeah and this is basically like the main differences right so you um for different algorithms you know sometimes you can have it like as I said you have a lot of variables in this world right because can you probably can even pick some other you know like uh like so basically you have a you can try to Model A lot of mechanisms or you can try to only model a part of it and and what's the decision you know often it's a trade-off if you model too much then you are risking to to model them incorrectly right and if you model two less then you don't have enough um like you don't leverage enough prior knowledge in substance right like if for example if you really know this is a gaussian you probably should not should leverage the prior knowledge um but but you may you may be wrong so it depends the cases yeah this is great questions um I know I'll discuss a little bit more about this more mathematical levels [Music] yeah so so I think the question is you know uh given to me and covariance you know like if you don't know anything else it's the ghost in the best is that question yes I think that's a that's a great question so um so typically you know you are basically right if you don't know anything else then you probably should just model them like if you don't really know me me and covers but on the other hand you know um to be fair you know you know more than the me and the covers right so for example if you really want you can compute the the third order uh correlations between these data points between these coordinates right so in theory you can also because you have so many data if you have a lot of data you probably can model other higher uh higher moments of the data I'm not sure you know the definition of mobile so you look at the higher correlations between the coordinates of the data if you have a lot of data um but gaussian is pretty reasonable assumption and it's still used very often um uh sometimes you people use transformations of gaussian so here we assume they are gaussian sometimes people use like you can transform the Gauss in certain ways but gulpture is pretty pretty reasonable assumption so how do I do the test when Y is continuous is that question yeah so we'll uncover that but pretty much you follow the same methodology you're going to have a different prior or different distribution for white P of Y right so maybe you can model P of Y by say gaussian again you know if you want I'll even have my variables to describe the description the accuracy oh oh so so I guess the question so um in short you know why the why is this critical continuous in some sense is uh mostly decided by your data set right so I think typically if your data set is really discrete right so if you really just have like benign cancer or not right you probably don't want to make it continuous for the same reason as you said you know why you want to make it more complicated right so you have more parameters to learn right but sometimes it's just like your your why is really continuous there's no way you can discretize them or you know meaningful way so um right and also to be fair the the parameters to model Y is often much smaller you have a much smaller number of parameters to model y than the number of parameters to model X for example here if you come if you count so you only need one parameters model y right so even this is continuous you only need one real number to model y probably if you have say for example why is the gaussian description but it's one dimensional right so you only need one parameter but for X it's a high dimensional thing so you have to always use more parameters so so typically it's not a big issue audio make a judgment of how many distributions yeah yeah I think that's a great question how how do you make this judgment how many kind of distributions right so the easiest answer is that you always use two as long as you have two labels uh if you just have two type of things you want to classify you just always have two different discussions two gaussians um of course you may want to go more advanced you know to say even for for the maybe for the benign Kaisers right it's not like really really like all the deny countries are the same right maybe there's two sub populations right so so it does probably require a little bit domain knowledge or maybe a trials and hours you know you could try to generalize this okay so uh let me proceed with the okay so I discuss a lot of methodology so far so now the next goal is how do I maximize this how do I maximize the likelihood right so you need to you need to be able to do this you know in power quality right so to guide the parameters right so this is about computation partly right so what we do is we you know one choice is that you just write down this function uh in your computer and you run some optimization algorithm but you want to do a little bit more than that in math because that will simplify your uh your implementation right so so we're going to simplify this formula so that we can and and actually we're going to do a lot of math so that you the you don't really need a laptop to your computer to compute the parameters to to run optimization algorithm for this right so um so what we're going to do is that um so the first thing is that we know that if you do a Arc Max so we are we care about the arc Max right the the maximizer of this of DC so the the maximizer is the same if you transform your loss function with any continuous monotone function so even either log the maximizer will be the same and tip for the for the purpose of this course this course we Define this using a little L this is the law of likelihood okay while we are doing this no it sounds like we just introduced something even more uh more symbols the reason is that this will make the product to a sub so log of this product log of a bunch of Paradox of range of terms will be equals to the the sum of the log of each of the terms so this will be sum of the log of these two terms and the log of the product these two terms will be the sum of the log of each of them so we log of P of x i given y i um plus the log of p y i given I guess I don't have to write P here for this purpose right so everything becomes a sub and that's that's very important actually because even you do this numerically it's very important to take the log because all of these numbers you know if you do it empirical you see like there are either very small or very big just because you know we call that when you have gaussian distribution there's exponential here so it's kind of pretty easy to see the density function it's pretty easy to be either very big or very small they are not never on the the best scale like you can imagine but if you take the log of it the scale will be much nicer so so the log of the density will be on like some reasonable kind of scaling so that you can numerically use uh and also it becomes a sub so you don't have to do the product so and then how do we proceed you know one option is that you again you can still do the numerical stuff where you can do optimization algorithm to to get the minimizer but here we can actually analytically compute the the maximizer so so the maximizer here how do you compute it um what you do is you I'm going to continue here so um so you really so how do you find the maximizer so there's a small fact I guess probably you learn from the um the Calculus class so if Theta is a maximizer then that means that of some function f Theta I'm being abstract here um so then it means that the gradient of the function at Theta evaluate as Theta is equals to zero so if you are in the maximizer you have to satisfy the gradient zero and actually in many cases this if after is convex then this is if and only if if it's not convex this is still um this is still a necessary condition so for us it's actually convex so it's a it's a necessary and sufficient condition um but for other cases this might be just only a unnecessary condition so because you have this then you can solve the the equation so you can try to solve the equation I'm going back to this is a small abstract fact now I'm going back to this case so basically you say the gradient of the loss with respects to all the parameters should be zero right so like with back to three mu 1 mu 2 and uh and and sigma should all be zero so basically you just have to say by the gradients is zero and this really just means that the partial derivative with back to each of the parameter is zero so now you have four equations and you can try to find the solutions for these equations and and this is I think homework one Q 1 T so in that homework we're asking you to first of all you have to compute what is each of these is right so you have to have an analytical formula for each of this right so what is the the derivative of L with respect to Phi you have to do some calculation to see what's the derivative you have to plug in all of our definition of this p is two P's and you get the the loss function as a function of v a new derived the derivative the derivative respect to Phi and then write out that this is zero and you solve the equations so that's homework q1d and this is a little bit complicated to some extent you know but not it's still manageable and this is you know they are even more complicated things than this in machine learning this is still you know but but at the first time it would be a little bit complicated because all of this has a little bit kind of like complex formulas um right so and what I'm going to do next is that I'm going to tell you what's the solution of this directly so you know this answer of the homework question uh and I'm going to proceed I'm going to interpret while the solution makes some sense you will see the solutions actually make a lot of sense intuitively um and and I'm going to proceed with that oh my god did I switch to that oh my God this is new zero this is thank you so yeah so what's the dimensions of this yeah that's that's a wonderful question so I think often this is a confusion that is pretty often like a um just because sometimes in math they have different things so in the in this class uh the derivative of with back to the the parameter any parameter will have the same shape as the parameter so so this is a one-dimensional parameter so so that means that this is a scalar this derivative is scale and this is the D dimensional Vector mu zero so that means the derivative is Rd and this is in Rd by D ude so this is a zero ISO vector and this is a zero Matrix Okay cool so I need to erase something I guess okay so what other Solutions you know so the solutions are okay I'm going to first Define some notations so uh select u0 to be all the examples that are positive these are the index for the policy of examples indices and wait my back this is U1 u0 is the indices of negative examples okay so under the mle solution the solution will be the following so Phi is equals to U1 over n where n is the total number of examples which is equals to u0 plus U1 so what is this this is really just the fraction of positive examples right so this is the fee you learn so Phi is supposed to be the probability of Y is one right that's your modeling choice and it turns out that if you learn it from data it will be exactly the fraction of positive examples in the data so so this is the most likely fee that can join with your data which is exactly the same fraction as in empirical data um and and of course one minus V will be the fraction of negative examples in the data question okay how do I remember this I have to burn this in my head so one minus V is equals to this is the fraction of active examples yes one U sub 1 is it is the is a set so this set contains all the indices such that y i is equals to what so this is the indices of positive examples Okay so so for example suppose you have here right so this will be the set you want this will be the set u0 and and how do you decide what is fee the maximum likelihood feed will be just you can't common examples in total one two three four five six and maybe 10 examples and you say four of them are positive so that's why fifth is going to be so in this case P will be 4 over the total number of examples 10. and for some reason I'm going to write this p as the photos I'm going to write C also just to this is mostly just for mathematical cosmetically uh you want to make it look a little bit nicer in some sense or more consistent with the other equations I'm going to write next so you can also write this as the following let me explain this notation so this is so-called indicator function so indicator function I'm going to write that one of E I think I think the homework we write it like this you know people different people have different type of brackets but it's the same as long as they defined so one e this is equals to um this is the so-called Indian function for the event e so it's equals to one if e happens and is equals to zero otherwise so let me check whether this um so in this case right so this indicator of y equals one just means that if Y is equal to 1 is equal to one this indicator of that is equal to one otherwise it's going to be equal to zero so so basically this indicator is only one when the label is positive and I'm taking the sum over all examples so that's why I'm basically counting how many examples satisfy y i is equal to one so so that's why this whole thing is just equals to u u y it's probably useful to understand this notation because I'm going to have a little more conflict in formulas than this so this is a four marking subsystem this is just uh I guess on in my mind this is capital uh which is not the Greek lighter mu um yeah I guess in the um yeah when you do it handwriting is not that obvious yeah but there are completely differences right this is a set another is a parameter no relationship at all um why do I started oh let's oh I see I see so that's a good question yeah thanks for us so the absolute value is um maybe I should Define this so this is the side so when you have a set I'm using this as the size of the cardinality of the set like how many items are in the set um this is the notation yeah yeah that's a that's a good question maybe you should take a note on this I think last I was asking this last time as well Okay cool so I'm going to continue with the telling you the solution of this mle okay foreign this is the mle for the parameter mu mu zero this is equals to 1 over this is u0 the SEC the number of negative examples times the sum of all the excise in the set use mu u0 so so this is the sum over all the positive examples and I'm taking some of the the the input vectors the the feature vectors x i so basically this is just the the average the empirical average of X I's you know of a negative axis right so I'm looking at all the negative examples I'm going to take the empirical average of the excise in it and that turns out to be the best estimate for the means of that class oh I see I see yeah that uh good question so it doesn't mean anything empirical average is the same average um yeah there is you know just just think of it as the average there's some reason why I use that just because some other cases here sometimes don't worry about sorry um yeah okay so any other yeah so and and now I'm just gonna I'm just telling you the answer and but this sounds like very intuitive right so like what would you guess you know what's the the best what's the best meaning for this class probably you should just use the average of all the examples at least you know if you see it you know it sounds somewhat reasonable um and um you can guys you know just because these are symmetric you know from the MU 1 is the same thing you're gonna have one over the size of the positive examples times x i the times the sum of the this right so this is the this is the average of positive access okay so and we're going to write this as so I'm going to use this indicator function to write them uh in a slightly different way so I'm continuing here so if you look at this formula you can write this as the u0 the size is equals to as we argued the indicator of y i is equal to zero this is the number of examples where y i is equal to zero because Y is equal to zero means the indicator is equal to what right that's what indicator is saying right indicator is saying is if the indicator is y only if the event is happening so that's why this this is one when Y is zero so this this is why so that's why this denominator is the same as the size of the negative examples so and then the the numerator can be written as equal to zero so you first have this indicator only selecting those examples that are negative and then you take multiply x i and for the second part for the MU 1 is the same you just replace Zero by one so you have and you have you select all the positive examples and you multiply x i so you know why I'm why I'm writing it like this you know one reason is that you know it looks you know a little bit kind of like more um systematic I'm not sure whether you agree with that maybe you don't um another thing is that you know um I think you see this kind of formulas you know pretty often for other cases as well so so it's probably good to unify them in some months in some sense but you know but you don't have to remember any of this you know the I think this this way is the best way to remember them or and any kind of interpreter so you just treat this as a cosmetic changes some cosmetic changes of the former okay and next I'm going to have Sigma so the solution for the mle for Sigma is like this this may sound a little bit complicated okay so let me try to so this is the what is this meal this meal is the meal we have completed above so you have to use the meal you completed about to compute Sigma so these are the means you complete above a mean y i could be mu 1 or mu zero depending on what what's what's y i um and when we do interpret this is that you just look at you can expand the sum into two cases one case is the Y is zero and the other case is y is one so when y i is zero so you have the so those are the Y those are the I's that are in the set u zero and this is x i minus mu zero times x i minus mu zero transpose and then you have you look at the those cases where Y is equal to one and then this mu y i becomes mu 1 and you get this so this makes it a little bit easier to interpret because this if you compare this with the this is kind of the covers but the covariance evaluated on the empirical data so this is the sorry on the on the data set we have seen empirical is empirical is a word that's used to stress that you you are seeing the data sample data so that's I kept using that but you we don't have to use that word so so this is the covariance covariance of excise for those excise in the set u0 for those negative examples so this is the covariance of negative examples and this is the covariance of the positive examples and and it turns out that the the average of the in this sense is is the is the best guys for Sigma it's the it's the sigma that gives you the maximum likelihood Okay so I've got all of this uh parameters so far right so um and now you're going to ask to prove all of these are true and but suppose we already got all of these parameters we can complete them in near Miracle right by plugging this formula because you just plug in all the excise you have all of the data you plug in you get all of these parameters so you've got all the parameters so that's the so-called learning process you learn the parameters and now the next question is how do you make predictions on a new example right you get the parameters how do you make predictions because so if you assume they are different I don't think the formula will be this I I and actually if they are different you can you don't even have a analytical form for the solution of the mle like like the just you cannot Solve IT analytically so here is kind of like for some reason because we are making all of this simplifying assumption you can solve the minimize the maximizer of the mle but it actually is not always the case you can write analytically and when Sigma are different for the two subpopulation you don't have that analytical solution Okay so okay so now we're talking about prediction so let's get it if you want to open some water but you want to understand you know what what is the deny cancer or not right that's your final goal that's that's the final goal of the of the problem so and the way that we do it is that you say I'm gonna output the most likely why so I'm going to Output Arc Max the maximizer of P of Y given to X and the parameter feed mu zero one Etc and note here that this are the solutions of the anole so these are not aperture parameters so I in some sense you can even say I'm obvious notation a little bit just for Simplicity so here this female one mu zero Sigma R those solutions that are computed from this farmers okay this is a matrix right this is a vector this is a vector new theories of vector mu zero is the mean of the gaussian is a d dimensional vector so did I say something here oh okay I guess I erase it right so you assume y given x given y is from this you know from Y is maybe zero this is from discussion with mu zero and sigma so mu zero is a d dimensional Vector Sigma is a matrix no that's T C is the is a scalar is the probability of Y is equal to one and mu zero is the prop is the mean of the mu 0 is the mean of x given y zero and mu 1 is the mean of x given Y is one how to do this yep Okay cool so back to here so this is my methodology I'm going to take the mle so how do I compute this so it turns out that this you know of course I have to use the base rule to get y gonna X because I only know x square and Y I only know why but I don't know what is YG Max so one thing is I have to use phase rule so let me do the base rule for you and it's actually simpler than you may think so because here you are maximizing over y right you are trying to Output which Y is more likely right well it's more likely to be denied cancer or not so basically this maximization problem just have two choices we are just maximizing over two possible choices so you are just taking the arc Max of the two the two point is two scalars they are both probabilities and that's what so just cover these two scalars and which one is bigger and turns out that these two scalars their sum is one because given X in y can only be zero or one so right so maybe let's suppose just for for the sake of simplicity so suppose you call this a and call this B then a plus b is equals to y right and you care about which one is bigger or the a is bigger or B is bigger so if you have a plus b is equals to one and you're taking Max of A and B then what does this really mean it really means that you are asking whether a is bigger than point five or smaller than 0.5 because you're going to choose a if a is bigger than 0.5 right because if 8 is 0 than 0.5 that means B is less than 0.5 so that's why you choose a and and you want to choose B maybe let's write this again sorry a if a is bigger than 0.5 because that means B is less than 0.5 and it's going to be equal to speed if a is less than 0.5 because that also implies B is bigger than 0.5 so so basically the question is that you just care about whether a is bigger than 0.5 or not so going back to this zone I'm doing abstract thing right so if you're going back to this then it really just means that this Arc Max is equals to 1 Y is equals to 1 if the probability of Y is equal to 1 given X and the parameters is bigger than 0.5 sorry my there's a little bit and limited space and if you zero if P of Y given Y is 1 given X the parameter is less than 0.5 so you know which also makes sense right because you know why oh in my back right so which this also makes sense because basically this is saying that if the probability of Y is one is larger then you you choose y you choose one otherwise you choose zero that's it right I'm just mathematically derived that for you um that's it so and if you look at this figure so what's the final decision what's the final kind of like boundary between these two cases this line will be the family of X such that this P of y equals to Y given X is equal to exactly 0.5 so if you if you define this family of axis right this is a family of axis such that the probability y given X is equal to exactly 0.5 and this is called the decision boundary so and on one side of boundary Y is y is more likely on the other side of the boundary y 0 is more likely and the boundary you know you just do some arbitrary type of thing or you just randomly output what you know the boundary wouldn't be very likely to show up it's very unlikely that your Pawn will be exactly the boundary so so it doesn't matter that much [Music] so maybe let me just uh maybe let me just uh quickly because we're gonna have two minutes left let me just quickly say what is this decision module is for the gaussian discriminate analysis right because here what I'm doing here is it's pretty General in sometimes you can see right I didn't really talk about what exactly the parameters are and if you really want to know it know what this P of Y given X is but you have to plug in the parameters you have to use the modeling right so and for G for gaussian describing analysis if you plug in uh you plug in um so what you do is you're following so you do p of Y is equals to one game X this is the thing you really care about so you use base rule so you say that this is equal to P of x you know Y is one and here you only depend on mu one Sigma and you have P of Y is one given and you divide this by P of X the probability of x this is the base rule that we kind of alluded to in the the beginning of the lecture so and then you do a lot of calculation which uh I think this is homework I do a lot of calculation and what you'll find out is that actually this has a relatively simple form so the form looks like 1 over 1 plus exponential minus Theta transpose X plus Theta 0 where Theta is a something of Rd so that zero is a scalar so they are functions or they depend on let me just simply say these are Depends on V mu zero mu 1 and sigma so so basically eventually you get mu all of these parameters and then use this parameter to compute Theta instead of zero and then you get y g Max and that's your probability and then you're on the computer decision boundary let me also do that real quick sorry we're running a little bit late so then you get a decision boundary so so what's the decision boundary the decision boundary is when this is equals to 0.5 and when this is equal to 0.5 is when this exponential is equals to one right then you have 1 over 2 is 0.5 right so when the exponential is equal to one this means Theta transpose X I think sorry let me see I think I need to have a parenthesis here but then we become some abstract here it doesn't really matter that much so um so it means this is equals to zero and and you'll see that you know P of Y is equals to 1 given X is larger than 0.5 is the same as Theta transpose X plus zero is larger than maybe you say larger than zero so so basically you have a a linear function of X that's your decision boundary that's why I keep drawing a line here you know from here you know from the principle you never know why this is a lot right the principle says that maybe this is a some formula of X right that is let's separate this but but the the derivation tells us that at least for this case the decision boundary is a linear function of x they were doing protocol uh so I think I'm gonna have this yeah sorry this is a title here I should have the parenthesis like a yeah I know the signs are not that important um I think yeah we are um maybe you can see with the best way you can come to me to ask the questions and I I can stay here is that the best way maybe yeah"
"Stanford CS229 Machine Learning I Naive Bayes, Laplace Smoothing I 2022 I Lecture 6","Stanford CS229 Machine Learning I Naive Bayes, Laplace Smoothing I 2022 I Lecture 6

so I guess uh last time we talked about um gaussian discrimination of analysis I'll have a very brief review and um and talk about some of the remaining points that I didn't get time to mention last time and then I'm going to move on to another case about spam filtering where we're gonna have uh discrete acts instead of continuous acts so last time we talked about gaussian discriminal analysis and the general idea is that you first model P of x given y and p of Y and what we do is that we say p of x given Y is a gaussian for y is 1 or y0 okay I guess I need to remember right bigger so x given Y is equals to zero is from some gaussian distribution with mu zero and covalent Sigma and x given Y is one is from some gaussian distribution between one and sigma and recall that we have this uh illustrative situations where you have some examples like this these are positive samples there's an active examples and you kind of believe that each of this subpopulation come from a gaussian distribution with different means but with the same covers so that's the methodology we have that's the starting point we have last time and then what we do is that we say after you have this probabilistic model we also we have this P of Y is 1 is equals to V after you have the probabilistic model then you can learn the probabilistic model from data by Noe so so we learn by maximize we take the arc Max over our parameters Phi Mu 0 mu 1 and sigma and we take the r The Arc Max of the the log likelihood you know it's the same as the likelihood you know Mac Arc Max of the log likelihood is the same as the max of the likelihood so I'm just writing the log likelihood which is the log of the probability um of x i given y i plus the sum of the log of the probability y under I guess you know technically under this parameter V mu 1 mu 2 Sigma and I guess technically I don't have to write video just because it doesn't depend on feet okay so that's the the methodology and then we skipped a lot of steps because these are homework questions um and and we told uh and they told you that the solution of this mle problem you can analytically solve it in this case because this objective function is nice enough it's a it's kind of like a quadratic function so you can solve the uh the maximization problem analytically and you get um some formula for uh these quantities right so the formula for Phi was something like um something of this form of this indicator function which is here basically the numerator here is the number of positive examples and divided by the total number of examples and we also have formulas for Mu 1 and mu 2 so mu 1 was something like the average of x i in the positive group in the positive examples so you take the average you divide by the total number of positive examples and you can also have mu zero and also Sigma so each of these has a formula which is a Formula or that depends on the data so this is how you learn the parameters uh from uh from the data and then we talk about given these parameters how do you do the test how do you do the the inference or how do you do the um the prediction right like when you get we already learned these parameters now you are given an example X how do you predict y using the parameters and given ducts right so and we said that the you're trying to compute the arc Max over the two choice of y's and you want to look at which choice of Y gives you the largest probability given X and given the parameter and these parameters are the the solutions you have computed from these formulas right so you compute which one oh sorry not bad this is one you complete which one has the largest probability and also we discussed that you know to know this actually you just have to you know in some sense you you have a decision boundary so uh the decision boundary between the two choices is those cases where is this uh those axes were that is where these two probabilities are exactly the same so where the P of Y is one given X is equal to P of y 0 given X is equal to a half right this is the decision boundary and we have computed the decision boundary which turns out to be something like a linear function so decision boundary turns out that this P of Y is equal to one given X this is equals to um this is equals to something like I guess maybe I'll just directly write out the decision boundary we found out that the decision boundary is the set of X such that uh Theta transpose X Plus Theta 0 is equals to zero where Theta and Theta zero are functions of the parameters that you have learned there are some like um specific formulas uh that um to describe this Theta instead of zero which are the homework questions but Theta says there are some functions of Phi Mu 0 mu 1 and sigma so that's why when you really make the prediction what you do is you say you find out this decision boundary this is the final of X such that Theta transpose X plus zero is equal to zero and then if this quantity is bigger than zero then you'd say it's positive and if this quantity is less than zero you say it's negative this is very quick you know Five Minutes review of the last lecture very very quick any questions if we have multiple labels yeah there will be a decision boundary for multiple labels so um and um you you basically just compute the decision boundary using this methodology you're gonna get a foreign I think if you have the same covariance the decision boundary is still linear um but if you have different models you may have different type of decisions you know it wouldn't be a half because you know you're gonna have multiple choices wise here right so so your decision module will be like uh I think I think it's going to be more complicated so you have to really decide you know when suppose this is like zero one two three or four right so then you really have to just really literally solve this right you have to know when is the case what is the region such that the maximizer of this is equals to say label two so that's that's gonna be something that describes by some linear region some kind of linear boundaries but it's going to be more of conflicted fall you know it could be depending on what probabilistic model you define right so I think if stigma are all the same the capital Sigma all the same I think it's going to be linear and if you have different segment even you have two classes but just take my one and sigma 2 then I know it's as a fact it's quadratic I saw some other questions as well any questions are welcome okay so this is just give you a quick overview a quick review of what we have we did last time um and and you will see that our new when we discussed the new problem we are going to have from similar methodology you define a probabilistic model you solve the mle you get some formula and then like you get some parameters and they use those parameters to predict what's the most likely wise okay so but before going to that I'm going to discuss one important thing which you know many people actually ask in the last lecture at the end of the last lecture they are great questions so the question I'm going to discuss next is that you know why this is different from what what we have seen in the first two weeks right so at the end of the day you have a decision boundary which is linear right so basically at least superficially it sounds like you are using a linear model to decide you are going to use this linear function to decide whether it's positive negative right you compare this with zero and if it's paused if large zero is positive otherwise is negative and this is the same as the the logistic regression that Chris talked about in the first two weeks so why these are why this is different uh from from the uh the so-called discriminative methods that Chris talk about so I think um here is the the way I think about this so um so so if you have GDA on the left and suppose you have like a logistic regression on the right um so first of all in terms of the Assumption they are different so for GDA I guess I wrote The Assumption there so you maybe I just wrote here again so this is gaussian and this is also gaussian something like this and you also have Y which is from Bernoulli and when you do logistic regression then you just literally say that P of Y is equal to one given X you your probabilistic model assumes that this has the form uh one over one plus e to the minus Theta transpose X and and recall that here when you write this in places we are saying that X zero is one right that's quiz assumption like in the first two weeks but suppose you don't have that assumption you're going to have another suppose X doesn't contain X zero then you need to have another set of zero something like this right if you contain the x0 then you are going to have a clean form but they are the same right because you know if suppose let's say for today let's say x uh you drop that convention so exciting contain x 0 then you you're gonna write it like this right and this exactly match the form here so so basically you can see that one thing is that for GDA you assume this bunch of things and you and you found out that it implies that y given X has this form right so recall that okay maybe I didn't write this but like this is equals to one over one plus e to the minus Theta transpose X plus zero right so let's say how do I write this so P of Y is equals 1 given X right so this is a this is a conclusion that we got from the probabilistic modeling right from our mathematical derivation right we conclude that y given X should have this form and in logistic regression you just directly assume it so in in GDA that's a conclusion in some sense right and and I guess you know we have to stress this many times so here you you you model The Joint probability distribution because given if you know x given Y and you know why then you also have the density of X comma y right but here you only always have y given X but you never have anything about X we only have the conditional probability so in some sense the kind of like the the main difference between these two is that on the left hand side you have more assumptions so I think this is the key you have stronger assumptions on the left hand side than on the right hand side and and in some sense the color of the journal um uh the general kind of like a phenomenal General principle about you know how do you model what what part of the world you want to model in your machine learning algorithm right so do you want to model both or you want to model only YG Max right so how do you decide when what how much you want to model the world so the the kind of the pros and cons of the the kind of the trade-off is that if you have more assumptions then typically if if this is also correct assumption if your assumption is correct then uh this pretty much implies you have better performance of course this is a not mathematical statement but I think it's kind of reasonably intuitive because if you have more assumption it means that you you are using your prior knowledge about the world right so here you are using a prior knowledge that x given Y is gaussian and if you use that prior knowledge typically if you use it correctly then you should have better performance like a like you know if I tell you everything then of course you have better performance but if I tell you a little more I suppose I'll tell you everything about mu and sigma and of course your performance is the best right if you tell you a little more about X then you should always have a little better performance so but the problem is with this is that so this is the the good thing about more assumptions but the risk is that you may have wrong assumptions right you might have make a wrong or approximate or kind of like not exactly correct assumptions right right so if you make wrong assumptions then make in most of the cases probably GDA will will be worse out for example what if the data are not really gaussian right so what if this doesn't look gaussian at all and you still make assumption that they are gaussian then you you're gonna have like a um um worse worse performance so and um another thing I want to stress is that even though superficially you see the form here is the same right so Theta transpose so when you make the GDA assumptions you get this y given X of this form but suppose you go through this left hand side and get this and you numerical you compute the source Theta so Theta instead of zero it would it wouldn't be the same Theta instead of zero um as you what you computed from logistic regression so you're gonna get a different side of like Theta and say zero so that's why it makes a difference so so this is a zero from Logistics regression is how do you get it you just directly fake your leading model using logistic regression that's how you gonna say that zero but if you go from here to here then you're gonna first learn the MU the sigma uh the MU 1 mu two a mu zero mu 1 Sigma and then you compute Theta using mu zero mu one Sigma so so that will result in a different set of theta instead of zero that's why it does make a difference um and whether it's better or not I think as I said basically depends on whether assumptions are are correct or how correct your assumptions are probably you never have can have like exactly correct assumption but maybe or something we all sometimes approximately correct then you should do GDA if your assumption is just completely wrong then you probably should do logistical question any questions so far um like we have a different data in Mortal cases so I will just be different again like is it because we're Computing in that equation of the out there directly and using those formulas and then you compute Theta as a function of mu and sigma right so so it's definitely a different process right you're not getting the Theta instead of zero in these two cases using the same process right so in one process it's kind of Securities right you first have to compute mu and then say it's data and the other case you just directly fit this using a numerical algorithm like a reading descent right is it possible that you can get a different decoration X but we have let's say the premise our shoes yep definitely you you may have if you change your assumption you may have a different equation for p of Magnum X and there's an actual interesting point I'm going to mention maybe maybe after I answer are there any other questions okay so then there's actually another interesting point I'm going to mention next is that even you change this actually it's possible you change the Assumption you still have the same formula but on the right hand side so so you can have both you can like if you change the Assumption maybe you still have the same formula maybe you still have a different formula so so this is a comfort case which I think that I think what I said is actually even more surprising right so you change the Assumption on the left hand side you still have the same y given X on the right hand side at least the same form so so this is a example so I guess here I'm looking at uh I think one dimensional so suppose X is one dimensional just you know it's not very important like exactly so so suppose you do x given Y is one is from the so-called poisson distribution um wait how do I personally distribution sorry and then you do this I must misspell this this works sorry um but you do this so you change it's no longer a gaussian it's um some other distribution and um oh I guess X is a is in because this is a personal description so it has to be integer something like integer um and and then you still have y is uh P of Y is one is equals to V right so we change our assumption and then this actually still implies that P of Y given X has this form one plus e to the minus Theta transpose X plus zero so still the form looks similar of course you know if you really numerical computers it would be a different state and say a zero because Theta and say zero here I'm just writing them as a as a generic variable right but actually they are they are they are they have meanings right Theta is a function of Lambda one and lambda zero and so that zero is also the function of unknown one and lambda zero so the form is still same um but you could have like um numerical if you if you use this model in terms of instead of GDA you're gonna get different state and zero so so a linear form doesn't necessarily mean everything right uh it also you know it also depends on how you learn this linear function so you have actually here we have already three ways to learn it where you can use this this model we can use the GDA we can use the largest equation they will all give you different Theta and Theta 0 as a as a final result and which one will be better I think you know if you compare GDA with this poisson version of the the standard floating algorithm then I guess the answer would be that you know probably depends most on whether which assumption is more correct more likely to be correct of course there's also some type thing here because here your your thing is like and but suppose you have a different model which also deal with like R like you know if we basically I'm saying like when you compare it okay forget about this axis in anything I suppose you don't care about that differences then which model will work better probably depends on whether your assumption is correct or not or which assumption is more likely to be correct and and when you compare the general following algorithm with the discriminant one I think it's the same thing right so if your generative assumption is more is likely to be correct then you should gain something from it otherwise maybe you should just use long distal question I think in some sense like at these days you know if you look at um um and also another thing is that um okay so um so and and also um maybe a little more General discussion is that um in some sense your model has two source of like a so in some sense they are like in some sense you have two sources of knowledge the model learns from two things in some sense so one thing is that your assumption and the other thing is you have data right so the assumptions means like how do I probabilistically model all of these qualities and data is really just what you see right so if you have more data of course it's good right if you have more assumption if the assumption is correct then that's good um but on the other hand suppose if for example you have already a lot of data then you have less need to use prior knowledge because your data already are very telling right so like the data is kind of sufficient for you to to kind of like extract whatever information you want then you don't really need to use prior knowledge because if you use prior knowledge you always have a risk to use it wrongly right so so basically in some sense I think the more than our trend is that you know like we're going to talk about the new artworks and deep learning um so in in two lectures the Modern Trend is that now we're in this setting that we have more and more data for many applications and and and then that's why the modern techniques you know like deep learning new artworks those techniques you know use fewer and fewer assumptions about our data so just because you know it's not really worth it right so like I have to think about how do I model my images right suppose you apply these two images right you have to have a model for X for the file image you know does it really work the effort to do that you know probably not you know of course it depends on cases but if you just want to First Cut results you don't have to model your X because modern X is very difficult you know it's very challenging and you may make mistakes so so you probably should just directly go for the more pull off like a discriminative analysis type of approach right so you just directly model y given XP accessor image right but in the in the um in some other applications for example if you think about medical applications right all or some of the other applications of machine learning where you don't have enough data in those cases I think still you have to kind of use as much prior knowledge as possible um and actually many times people even do even much more complicated modeling of your ex right so maybe you can use some other more advanced ways to model your ex because you know how does each chord of X what each coordinate of X means and you know what's their relationship and you put all of this prior knowledge into your modeling for x and then you get finally some wagon X using this machinery and then you predict and that's that's more likely to work so um yeah I guess that's the that's another thing is that if you use more assumptions then you are specializing to your application right that's another reason probably why in the modern time like people somehow don't do this kind of use these paranoids that often because I guess probably heard of me right works and one of the kind of magic about it is that it works for many many cases without much customization right if you use prior assumptions you'll use more assumptions you you are you are you have to kind of like use different assumptions for different applications because for different applications their data probably have different structures you so you have to do it one by one and that's actually what people do um a lot of times right so they look at their domain their questions and and study the structure um but these days you know like as you'll see like when people use device works because you have enough data and you just drop assumptions and you make a model kind of very general not very not specialized at all and you supply it to everything just without thinking much about what the data look like so uh yeah so I guess that's a you know you will talk more about new artworks but this is the a preview or kind of connections to what we're going to talk about next um another kind of like a big picture is that you know one of the reasons as I said you know in some sense I I was saying that you know this kind of GDA analysis is not used that often um at least not used as often as before right before you have to use these kind of things and now you can at least you have the choice to uh to try something like works and and because you have more data so um but still I think if you are able to model the ax you know in many cases you can do better so and also in some cases you just don't have why so even even for example when you have images right so sometimes you don't have why at all you just only have X and in those cases you just really have to model X because that's the only place where you can get information from it and another possibility another cases is um uh we are going to talk about languages right like uh especially like this lecture I'm going to talk about language as well but you know later when we're also going to talk about languages you know solving language problems with new artworks and they are you know you're just getting a kind of a um like a lot of like text from the from the internet right and there's no labels there's no way nobody's tell you like which like which web page is is about which right so you just have like raw attacks you don't only have acts and in those cases you really have to model X3 there's no way you can get around this so so so so still like modeling acts is is very important it's just like um for example it's less important for certain kind of applications because we have more data any questions thank you Okay cool so I guess so now I'm going to move on to um the next uh uh the next uh question so um I think the the question is this the so-called one on classification so you are trying to understand whether a piece of tax is a spam email or not right so like you have an email spam filter and you want to know whether your email is a spam or not and we're still going to do generative learning algorithm and and we're gonna have this great ax so this is another example how you do this challenge for learning algorithm your model X given Y and you execute this like a pipeline in some sense and and learn something out of it okay so um so the first thing so I'm going to you know get into a little more details you know how do we really approach this question so the first thing you probably have to what okay these are just examples um right so I guess um um so the first question uh to a first question we have to do to approach this is that how do you represent a text right attacks are symbols right like a ABCD right so um you need to make a numerical at least to to to to to to make the computer recognize them right so to in some in some naive way at least right so so the question first question is how do you change the tax to some you know acts maybe you call this feature right this is a um You can call this feature vector or or representation or something like that so you want to change this to X in some Dimension d e and then you model X so so first question is how to represent text so there are many ways to represent text so um so the way I'm going to tell you is a very naive way this is like a um a probably I shouldn't say naive like but this is like a very simple way like uh on this stage we're gonna have like a you know if you really deal with the taxi you're gonna use a more advanced deep learning based approach which we're going to cover a little bit in the in probably three or five weeks so um so this way so here the way that we do it is very simple so what you do is that um maybe I should have some so what you do is you say you have you first look at the vocabulary suppose you have a vocabulary of maybe 10K words so and suppose you say you list all of these words in your sequence based on alphabetical order I guess if if you open up a dictionary the first letter the first word is for probably always a I think the second you know are coming to some dictionary the second word is this word otherwalk I think it's a kind of animal um and the third one is uh the wolf I think it's another kind of animal something like this and then you list all the words you know and maybe at some point you're gonna have a word book and eventually the last word I think in many of the dictionary is this this thing I don't even know how to I don't even remember how to pronounce it I think I used to know when the first time I teach this lecture and then I forgot after a few years um anyway so you listed all of this and and then you say that um um you have suppose you have a piece of text right so maybe say suppose you have a sentence or maybe an email suppose this email just has one sentence uh something like I buy a book so you want to turn this into a vector and how do you turn it into a vector so the way that we do it here is that you turn it in a vector that is of dimension um this axis of Dimension d where let's say d is the size of the vocabulary T is equals to the number of words so and then on this Vector that represents um piece of uh this sentence is going to be a zero one vector so X is actually you know it's really from 0 1 to the power d there are only two choices and every entry so you have so many entries and what you do is you say if this word shows up in the sentence then you have entry one in this entry so the word a shows up in the sentence then I have one here and the artwork doesn't show up in that sense sometimes I have zero and then at some point you have this corresponding entry book the book shows up in the sentence I have one here and maybe somewhere I think there's a word probably I here in the list and then that one would also have a one that that word also corresponds to one and for all the other entries that all the other words that don't show up uh in the sentence you just fill in zero for the corresponding entries so and you call this x your representation or your feature Vector uh for uh for for this email for this sentence so basically technically I'm just going to say that x i is equals to one if and only if uh the ith word occurs in the email so there are actually many other ways to encode even before you know using deep learning techniques um but but this is probably one of the simplest one and you can see that this this representation of the sentence is in some sense like a super um I guess like simplistic because for example you don't care about the orders of the world right suppose you have another sentence I a book by right that sentence still has the same representation just exactly the same and it doesn't care about the frequency of the word for example suppose I have a sentence I buy a book and a a kind of pencil then of course the the representation will change but this two word a and a here you know will be kind of you will still have a work according one here it's because the word a shows up in this sentence but you don't care about how many times the word a shows up in the sentence so so you don't care about the frequency of the words in this sentence you just care about whether each word shows up so um and you know there are many other proper issues with this like uh what else um yeah I guess probably these two are the most important thing where you don't have to order and you don't have you don't care about the frequency um but that's what we're going to deal with because this is easy and you can um somewhere kind of like do all of the math uh with this kind of model Okay so now what's the next question the next question is that we need to build a generative model for X gaming y and we need to have a model for y and then we do mle and we solve the parameter so and so forth so [Applause] so basically I think I can just erase this and now I'm going to redefine experience right so that's what I'm gonna do okay so let's take um and how do you proceed so now because this x now is a binary Vector it's only taking 0 and 1. so you need a distribution that can generate binary vectors right so you cannot use gaussian here um and the kind of the techniques that we are using here is the so-called naive layers so what does this mean is that um this means that you just assume Max 1 up to XD or maybe I'll erase this you know are independent conditions on y so given y you just independently draw exponent after x d of course this is not realistic right this is definitely not exact you know how realistic it is I think that's subjective but at least this is not exactly how people generate emails right you are not saying that I'm going to generate a spam filter I first decide I'm going to generate generate a spamming email and the first thing I decide is I decide why and then after I decide why I just trying to reward randomly like like right that's probably not what how how people write spam emails right so and also that's not how people write the the usual like good emails so um but it turns out that many in many cases these kind of assumptions are pretty um already pretty good so in the homework actually um actually we haven't decided whether we are going to include that homework question but at least you know there are there are cases where um you can see this kind of things can be very effective right actually if you just really use this model even you make this kind of crazy assumption and you learn some parameters and use this model to expect on classified spams I think you're going to get more than 90 accuracy maybe these days you know as of like a 20 2012 maybe it's not that effective because all the the spammers they are they are adversarial right so they know what your prediction algorithm is they can change their algorithm to kind of fool you but but at least this is a reasonable one if you go back 10 years ago for sure like so um so so it's kind of interesting right so even you make you know obviously kind of not exactly the right assumption but sometimes you can still because the Assumption problems somewhat correct you know to some extent you can still get a useful kind of outcome from it and for our purpose I think here um I guess you know to some extent I'm not really that um I don't care that much about assumptions I think it's more like I'm trying to demonstrate the methodology um like how how do you like I just want to give a new example where you execute this probabilistic model this flow this this pipeline right um and show how how to solve them one example right so the question is how how does the text the lens of the text matter right so here um in so one interesting about this is that in this representation the lens of the text doesn't really matter you can encode any lens into a vector of Dimension d which is you know you can say this is a good thing or bad thing you know um um but but so so basically you can encode any lens um any sentence or any documents into a single vector and how do you how to decide which uh what is the window size you know here I don't think it matters that much maybe you just take entire email um yeah of course you know if you change the the the I think I think you should you should just include the entire email because that's the the unit you are working with right like for every email you are classifying whether it's a Spam or not you are not classifying it whether a sentence is of spam or not so that's why you treat an email as a single example okay all right x12x3. does that mean that like if you want like One X appears that tells us nothing about whether another X is likely to appears or does it mean that all X's are equally likely to appear like one y material um I think it's it's more about the it's more the first okay so so because I do like certainly not all the words are equally likely to appear right so I so basically I'm assuming that given y given you have decided that which whether this is spam email or not um every word you know is is uh is they are they are independent with each other but but they they may have different kind of probability [Applause] okay so I guess let me proceed so so what does this really mean okay now I'm going to do some math right so to kind of expand this and parametricize it so this really means that you have this P of X1 give an XT given y you write this as P of X1 given y times P of x d given y and now I I can I just need to parameterize each of this problem distribution by some parameter so and if you think about this what is this uh this is this is really just the distribution of Bernoulli distribution because X I can only have two Choice zero one so basically you just have to describe this probability by two numbers right I'm actually by one number right so so basically what you do is that you parametrize parameters of the model is you have fee on say j so there's some in the index which I'm going to explain in a moment you say this is x j is equals to one given Y is equal to one so basically for y is equal to one you're asking you know what's the probability of x j is one right and you you denote that probability as this and then once you have this you know the probability of x j is equals to zero given Y is 1 is going to be one minus v j given Y is one and this uh this um this thing is just a notation it's not like a um it's equally the same if I write J comma one uh I just like I write this subscript because it's a little bit more intuitive but it but I just I just need an index in some sense doesn't make sense so basically for every J I'm going to have a parameter for either J and and one I'm gonna have a parameter that's called Phi and this parameter is in 0 1. right so and and this parameter describe this distribution and for y is zero I'm also going to have a parameter so so I'm going to write this as J given y zero so this is the parameter for the distribution of x j given y zero yes it's between 0 and 1. this is between this is inter this is uh the bracket uh the hardback okay okay so so basically I'm saying that if I this with this parameter and this parameter I can describe all the all of this I can I can write out all of these numbers because I I describe all the quantities right because for example what is p of x j given actually 0 given y zero this is going to be equals to 1 minus V j y is zero okay so and then I also have need to have a a like something for this to parameterize the description of Y we call that we call this V before right in in the in the GDA case and not just for the for the sake of like a distinguishing it from the other fees I'm going to call it v y but this is the this serves as the same row as fee uh in before in a GDA case any questions foreign [Applause] of the parameters and you and this is the probability of the data given the parameters so what are the parameters the parameters of v y is one of the parameters and also all of these fees VJ given y 0 and v j given Y is one so basically I have V1 given y zero up to 3D given y 0 and then fee one given Y is one to Phi d y is y right these are all your parameters so on my likelihood you know here there are two um uh waste I can expand this so the first thing is that because by definition the likelihood is the product of the likelihood of each of the example because your all your examples are independent this is not naive place yet this is the examples I implement so you can just write this as probability of x i y i given all the parameters and here if you are careful then you know uh okay so it's basically all the time I guess all the parameters I'm going to write I think in my notes the notation is this so but I think this really just means this is uh just a convenience notation that denotes all of the parameters this is the same as this um okay and then you first you so so far is the same as the GDA and then you can also do the chain rule to make this x i given y i conditional parameters okay given the parameters and then times P of Y I gave me the parameters so when I use dot dot dots I just mean all the parameters of course sometimes you can drop some parameters because they are not they don't matter and then I'm going to again factorize in the dimension of X Y now I'm going to use my G like this naive face assumption right which is the factorization across the coordinates of X right we call that this is x sub I is the coordinate of X the superscript I is the iso example so so what I'm going to do is that I'm going to use that assumption for each of the examples so what I'm going to cut is uh I from 1 to n I'm going to put this in front just to make a simple make it easier um and if I'm careful I I only have to write Phi right here because the description y only depends on v y and then I'm going to factorize this thing across the coordinates so I'm going to have a product um across the corners have D coordinates and then I have P of x sub j i is the gist coordinate of the ice example given the label for that Y is example and all the parameters V j y just means I guess maybe I'll just uh so v j y this just means a shorthand for this family of parameters as a mix maybe I was just sorry this is a bad notation um so v j y just means a shorthand for this collection of parameters Okay so [Applause] um [Music] so you mean here um yeah that's right so right if you only look at Yep this J and this J yes you only care about the you just care about the j under there's the fixed j under the Y is 0 and Y Square that's great yeah right so so so we have two times that we factorize this probability so so the first time is here so here I'm using the fact that all the all the examples are independently sampled so that's why I I see the drawn probability is the overall examples is is the product of the probabilities of each of the example I know for every example um of course I first do the chain rule to get x given y That's and then I factorize this one into this product again and this is the level of the knife phase this is using native phase Okay cool so and then so I guess you can expect you know what we're going to do we're going to maximize this and we know that if you add maximize this it's the same as maximize suppose that's called this L right so max minus L this is the same as Arc Max of log l and log l is going to be a sum of the log of this probability so log l will be a sum you turn all of this to sum and you have to log in in front of the terms right so you have log p y I can v y Plus here we have a double sum sum over I from one to n sum over J from 1 to D and the log of this and then you analytically plug in all of this right so for example for this one you can you know what is this right this is equals to what is this this is equals to v y if y i is one if this is equal to one minus v y if Y is equal to zero right so and for each of this you can write them as a formula or for of the data and the and the parameters and then you you do the maximum likelihood so the maximum likelihood uh I guess I'll also just tell you the solution um so if you write look at the gradient of l is zero right this is the right this is the con it's a sufficient it's a necessary condition for for you you are being failed to be a maximizer okay I should write why and you compute this gradient and you solve this equation this is a family of equations and then this solving it gives um some formulas for the parameters on your recovering so the so the final solution will look like this v y is equals to um right this is pretty intuitive this is the fraction of positive examples actually it's the same formula as we have seen for the GDA and then fee j y is one this is the probability to see XJ is one given Y is one it turns out that this is also something simple so you look at maybe let me write down the formula and then interpret it so what is this numerator this is the number of occurrences of ice word right it's only one when the eighth word the ice example contains the J's word right so the J's word right this is x j i is one means that the J's word shows up in the ith example in and and you also require that the iso example is positive example so basically the sum is the total number of currencies of jth word in positive examples and and this is the number of positive examples so you can see that you know even though we have done a lot of like calculation and modeling identify the the formula is pretty intuitive you are in sometimes just counting it's kind of like something about counting right you're doing some statistics right you're counting how many times the jsword shows up in positive examples and you divide that by how many total parts of examples they are right so for example suppose like the word book shows up in 10 positive examples and they are they are like a medium positive samples that means this is five over a minute which kind of means that book doesn't seem to have much correlation with past examples if it is this number is smaller if this number is small it means that it's unlikely to see the J's word get impossible so when it's unlikely you know it's unlikely because partly because the data they don't show up so okay and for the negative examples is the same so you just the right to um it's kind of symmetric so feel of J given y 0 is equals to on something like I guess you can guess what the what it is right basically just change every the value of y to zero and any questions [Applause] okay so um okay so basically we are done with this um at least uh we are almost done there's one small thing that we have to address but in terms of the isometer parameter we are done so and we got a parameter another prediction time as before right so we'll do the prediction as as usual you want to compute P of Y is 1 given X right and if this number is larger than 0.5 then you say this is a positive example if this number is less than 0.5 you say this is an active example so we have to compute this and how do we compute this you know this is again as in our general methodology use the base rule and divide by P of x so this is still the same but there is one small caveat here which is that what if you have a zero divided by zero in situation so before we have gaussian the density no matter what you do the density is always non-zero at every places even though sometimes it could be very small the density could be very small but still your PX in all of these quantities are like all positive it's strictly positive at least you're gonna get the ratio here but here there might be some cases where your P of X is just literally zero and why that can happen I guess maybe let me just give you an example so so you may think that some example just never shows up just because of some um I guess let me show the exam cases right so suppose maybe let's say suppose so suppose um maybe the word artwork never appears in in changing side I never claim that this would mean that this would mean that P if you have a but your test example content continent so let's say Test example let's call it X contains it and now I'm going to claim that P of X will be considered as 0. um why I guess let's um some will simulate this algorithm and see what the fee will we're going to compute from this so otherwise hard work is the second word right so your so J is 2. so um and you know x2j X2 I is zero for every eye this is a mathematical translation of for every example the second word never shows up that's why the x2i is zero and when the X2 is 0 and you plug in it into this formula that tries to estimate this parameter suppose let's try to ask me the parameter V two Y is one right this Parliament intuity means that How likely the second word will show up in a positive example and recall that you know this formula is really about you know the total number of occurrences that is where shows up and divided by total number of positive examples so this will be zero because no occurrences of the second word divided by the total number of positive examples and this will be zero that's still fine um so far it's not a problem so zero divided by a positive number that's fine and the feed to y 0 is the same 0 over total number of negative examples this is also zero so so basically according to your uh estimate male estimate this this word otherwise just cannot show up at all right which makes sense because they didn't so it didn't show up in the tuning side you know you you just you know estimate you also say it shouldn't show up at all in this under this uh this set of parameters so but that's that's a problem because now if you compute P of x for example X it does contain the the second word then you're gonna write this as P of X so how to compute this you use the the total law of probability you say this is equals to case when Y is one plus the case when y0 and of course this is a positive number this is a positive number that's fun but this one is equals to maybe I'll just this one is equals to the using naive base this is equal to x j you know Y is one and you have some a product over J from J to d right and now I know that my X2 is one because this word does show up in this email and that means that P you're gonna have P of X2 equals to 1 given Y is one as the second term in this product right so you're going to have this term shows up in this product but this one is equals to Phi 2 Y is 1 which is equal to zero so just because of the second word you know according to your model is not supposed to show up but it does show up that means that this example just does have zero probability and your probabilistic model so that's why this is zero and for the same reason so this one is going to be equals to y0 and also you have this term P of X2 equals to 1 given y 0. and we call that under a probabilistic model you learned you just think this word cannot show up at all right the chance is zero so that's why the chance to see this word shows up is zero so this is zero so that's why this is also zero so because the zero terms eventually I guess if you because there's a zero here there's a zero here so you get zero just eventually so basically you conclude that this example which is n't supposed to show up like this example has zero probability under this probabilistic model that you you learn so and then that's a problem because now this P of X is a zero this is zero so you have something divided by zero actually this is also zero if you think about it because this is just the one term the numerator is just one term in the decomposition of PX when the numerator is just this term and a p x is the sum of the two terms so both of these the numerator and the denominator they are both zero and you have the zero divided by zero situation and now and what you do so this is a this is the issue and this is a reasonably realistic issue because sometimes you just see a new word in your test example right you haven't seen this word at all in the tuning site so on the way we deal with this is so-called uh LaPlace and smoothing in some sense this is a way to introduce a little bit prior so that you don't 100 trust your training site so so basically what we are seeing here is that you trust your trainings are just 100 like religiously in some sense like a like a like you uh you haven't seen any word the other the word artwork right in the tuning side you just and because of that you trust it you just say this word the students show up at all that's why if you see this if this word is in this ax right so then the probability of X is to zero so so what we are going to do is we're going to say okay just uh you know maybe we shouldn't trust the data exactly we're gonna allow it to allow any new word to show up a little bit with some small chance and and it's in some sense this is a local adjustment um um by using some prior knowledge which is called LaPlace foreign just to the best way to describe this method is start with something abstract um for the moment let's forget about that for the moment and just think about the abstract question suppose you have um so simple simple example maybe you can call this example or abstraction in some sense so suppose you think about you know your estimate um the bias of the bias of a chord right suppose you have a call and this coin is biased it's not 50 50. um so how do you know the the bias of the chord or in mathematical language it really means that you have a random variable Z which is from Bernoulli um where with a parameter fee and this fee is something unknown right it's not the half and you want to know what the fee is and you want to know it by looking at some data right so you draw a few copies from this distribution and you want to know what's the um uh what's the what was feed you want to ask me feedback um by using the data and you want to solve this problem right this is still a probabilistic model we can still do the same thing where you can write out a probabilistic you can write out the mle and you can write out the you can maximize the Moe right so let's uh maybe let's try to do this just uh so suppose you have like maybe n trials and let's call this you know Z1 Z2 and z and and each of these is either one or zero something like this right or maybe I can see my nose is called tail has so I'll just continue something like this so and you want to estimate what fee is and if you follow our general principle right we'll try to write out the likelihood then the likelihood what is the likelihood foreign so the likelihood of fee right of the parameter fee um is the chance to see this data set given the parameter fee so what's the chance to see this data set the chance to see the first one is one minus V right a chance to see the second one is is one um I guess tail means I think I need to Define Teo means what tail means zero let's say has means one right so so the chance to and you have a uh the the the probability to see one is Phi and probability c0 is one minus V so that's why the probability of to see the first example is one minus V and then you time one minus FIFA second example when you multiply a bench and then you multiply fee at the end right this is the left node and and if you organize this right you count how many one minus V they are how many speeds they are this will be one minus V to the power of the number of uh uh tails and times V to pause actually if you really look at this example you will see that the formulas are very very similar actually this example is a toy case for that in some sense um okay so you get this and then you can um take the arc Max of this like log likelihood you take the arc Max of the log likelihood so the arc Max uh maybe let's call this IO fee so the arc Max of log of LV if you solve it you are going to get the following so it's going to be the number of has over the number of has plus the number of tails which also makes sense because you know this is basically empirical frequency of seeing the house I just mean like the the frequency of that you see that has in the in the tuning set right and that's your most likely estimate for uh for fee Okay so and [Applause] right so so okay so far everything seems to make sense right so but now let's consider a somewhat um kind of extreme case so what if you see all the examples you see are Tails right and you don't have a lot of like suppose you just have three Jaws Z1 Z2 and Z3 and they are all tail so according to this formula you're gonna get on the fee the best estimate for V is 0 over 0 plus 3 which is equals to zero but you should do really trust this right so if you do really trust that this this coin just never give you a hat right what does this mean this means that this coin just never give you a hat at all forever right she didn't really trust that um you know this is a little bit subjective right you mean you know if you are really really trust the data you probably can say yes but you know maybe you have some prior knowledge that most of the coins are not that crazy right so like you probably should at least have some chance to see the hat right with some chance so um so in some sense you can say so basically on the flip side you can say okay maybe this is some coincidence right it just happens that you see storytelles even this fee is a half right seeing this example seeing these cases is the problem to see this case is is at least one over eight right it's something like one rate so so maybe it's just Coincidence of the data set so maybe you shouldn't trust the data sets that much um so the so-called Law plus smoothing is is a way to in some sense incorporate a prior so that you you say look my coin shouldn't be extreme shouldn't be too extreme like this so so basically if the LaPlace moving refers to the following estimator so this if you use a plastic smoothing you're gonna have a different formula your fee will be equals to the number of heads plus 1 over a number of heads plus the number of tails plus two so this formula you know you know I didn't tell you any like a mathematical justification actually if you really look in the literature they are mathematical justifications so far I'm just determining the formula but it would solve this problem right like at least to some extent because for this particular case you are gonna get one on on the numerator and four um for the denominator so instead of getting zero you're getting one over four so still you you say okay the chance to see the head is pretty small right it's smaller than the chance to see the tail but you don't have a very extreme estimate so and and you can see that this LaPlace moving is mostly useful when you have not enough data right if you have a lot of data then this plus boosting is not doing much right maybe let me give you a kind of a example for example suppose you have a big data where the number of has let's say I guess I'm making up this right so 100 a number of tails is 60. so if you use the standard approach right if you use the the the standard approach or maybe I shouldn't call like the vanilla approach then this is like a the fee would be equals to 60 over 60 plus 100. right this is uh what is this this is like a I don't know what I say like this is something like this and then um if you use the LaPlace smoothing then this fee will be 60 plus 1 over 60 plus 100 plus 2 which will be um 61 over 162. this is 60 over one 60. and these two are just those very similar just because the one whatever you change here one two right it doesn't really matter that much because the the dominating term is the six and a hundred so so sometimes you you achieve a like you choose you achieve a kind of right balance right if you have enough data then you're prior or like your LaPlace missing is not doing much it doesn't really change much you trust your data and if you don't have enough data then La plasma thing would um would try to make it not too extreme we'll try to make your estimate not too extreme okay [Applause] so now let's go back to our problem so let me see where is the best way to provide this um oh I think I erased them oh I erased the formula but I'll write it again foreign [Applause] so going back to our spam filtering thing I guess actually there is a there is a there's one thing that is left here this is our estimate right and our other estimate was something like VJ wise one is equals to am I writing the super super I think this is I not that right so if you apply this to uh our case so you can recall that you know in this case it's kind of like you are saying that the numerator is the number of times this word shows up in a positive example and and the denominator is like the total number of positive examples right so in some sense the knowledge is that this denominator is very similar to the number of heads and tails because because the health means that this word shows up that tail means this word doesn't show up so and and here the the denominator is like the total number of times the total number of examples positive examples um and here the the the the numerator is the kind of like the has right so basically has means that the words show up in positive in a positive example and tail means that the word doesn't show up in a positive example so that's why this is like has number of heads and this is second number of has a plus number of Tails and if you use a lot plus more thing then um you're gonna add one to the denominator and you add to the numerator and you add two to the denominator so that's the that's the LA possible and the same thing for the other formulas you're gonna get add one and I2 here so that's the uh LaPlace moving and while this solves our problem it solves our problem because now we just never phone we just never estimate any parameter fee to be exactly zero right so recall that when we have this uh um artwork issue right so the issue was that these two parameters for the artwork was exactly zero um yeah let me ask um that's a fantastic question so the question was that whether you want to do the same possible thing for the fee why like fee why was the single scaler to describe the the problems of why um to describe the uh the probability of each of the class right so and you are right so suppose one class just never shows up right and you still only have like negative class of positive cost then you probably should use LaPlace missing for that but I think this is a little bit less important because you know in most of the data set you have to see positive examples and negative samples you have to see a reasonable number of maybe 50 of positive negative and then LaPlace moving wouldn't really matter that much you can still use it but it wouldn't matter that much okay so going back to this so so recall that our problem was that when you have the when you have the parameters you get this zero right you you thought that other boxes shouldn't show up at all right because it's very extreme estimate and now when you add this one and two here on on so what you're gonna happen is that instead of having zero over the number of positive examples you get zero plus one over this plus two yeah I don't have a different color sorry so I have to just modify um um I just only have black pants um but so you get this and then this will be at least no longer so this will be still a pretty small number right this is one over the number of positive example plus two so but at least this is bigger than zero so and the same thing for this right so you're gonna get plus one our on this plus two and this is bigger than zero so if both of these are bigger than zero then when you evaluate this formula you are not going to evaluate to zero so your P of X will be some positive number so then you can get a a number of the P of bike effects and maybe just a very small extension give you have two minutes so two or three minutes so if you have more than two classes you can um this is just a for your interest um so suppose you have like a maybe a dice something like that right instead of like a coin so suppose you have Z which is from something like zero one up to K minus one so you have K choices then uh the LA plus the general LaPlace smoothing would be something like if you don't do LaPlace moving what you're gonna have is that the pro the chance you estimate this to be something like the number of times z i is equals to J over the total number of examples so you count how many examples kind of end up to the choice J and you divide by the total number of examples and if you use LaPlace moving you're gonna add one to the top and you add K to the bottom where K is the number of choices so so this is just a small extension of Laplace smoothing you know I don't think this course will use it again but for your interest um okay cool I guess that's pretty much all for today any questions okay great yeah in the next lecture I guess so we're going to talk about the kernel method and then we're going to talk to deep learning"
Stanford CS229 Machine Learning I Kernels I 2022 I Lecture 7,"Stanford CS229 Machine Learning I Kernels I 2022 I Lecture 7

hey everyone uh I guess let's get started um so today we're going to talk about a kernel method so I'm going to Define what it is so in some sense this is a general technique uh for dealing with non-linear averity in your data so um um oh we're going to see more other General techniques like new light works but this is one of the techniques that we will deal with nonlinearity before deep learning took off and we'll introduce these techniques mostly in the setting of super asserting in the kind of the discriminative algorithm type of settings so um but this kind of techniques can also applies to Enterprise Learning or other settings um just with similar type of ideas okay so I guess so let me start by um I guess start with a random example so suppose you have some data I guess I'm thinking about surprise learning just a simple kind of like regression settings right so you have some acts and you have some Y and you have some data something like this I'm destroying something I'm making up some data and in the first two weeks I think Chris talked about you know you fake a linear regression right so that's that means that you have to you're filter some linear model on top of this so but you can see that you know I'm I designed this data such that you know a linear model is probably not correct right so apparently in this data you know you may want to use some something like maybe a cubic polynomial and if you have done the homework um I think there's one homework question which actually also talks about you know how do you deal with this kind of situation where your data is not linear in X right so you shouldn't you assume a linear model in X the homework is still this Wednesday right yeah so it's one of the last homework questions but you don't have to know the exactly the homework questions I'm going to actually um um basically kind of review the homework questions to some extent so how do you deal with this kind of case right so um so a simple way to do it is the following so you say I'm going to have a Q back polynomial to fade this data so basically I'm going to assume that um model is something like H state of X is a function of X parameters by Theta so it's something like theta 3 times x cubed plus Theta Square Times x squared sorry see there's two times x square plus Theta 1 x 1 plus Theta zero so here my Axis uh is a real number just for the sake of Simplicity um so I have this uh um non-linear model and um sorry any questions okay so uh you fit this right so how do you deal with this so one thing to realize is that either you have such a model that is non-linear in x but actually this is linear in Theta right it's a linear function of theta it's a nonlinear function of x so but actually when you really do linear regression it really doesn't matter whether it's linear X or not what really matters whether you are linear in your parameter because you are optimizing over the parameter space so in other words more especially so what you can do is that you can turn this into a linear a standard linear regression Problem by doing a simple change so you can say that I'm going to Define this function Phi of x um to be this function that Maps a to this vector so this function maybe you should record five of maybe let's just call it Phi Phi is a function in maths R to r four and then you can rewrite this non-linear model non-linear X um to be something more like a linear model like this is almost exactly the same as um the linear model we have talked about right so this is a vector say the ones something like this times 1 x x square X cubed which you can write this as Theta transpose times Phi of x so in some sense after you do this simple rewriting right now H Theta X linear in Theta I still linear in Theta and also is linear in the coordinate of Phi of x so this is how you make everything linear and then you can in some sense invoke um the linear regression algorithm that you have used what does that mean that means that you basically can just view this as your new input so before your input was X right now input becomes higher dimensional before input was one dimensional X right and now input becomes uh field facts which is a four-dimensional vector um and and that's it so in other words suppose before we have a data set which is X1 y1 up to x n y n where X is a scalar the input is scalar y i is always a scalar I say and now in some sense you just turn this into you can turn this into a new data set and this little set is going to be the input will be Phi of x i and the label or the output is the same of course your input dimensionality becomes formal instead of one but we have we are able to deal with high dimensional input for leading regression right so so basically you know you can just know um view this as you're new inputs or sometimes people call it features I guess probably you know we have talked about this like features sometimes is a is a word that is has multiple similar meanings in machine learning so um so so in many cases features just means inputs um but I'll clarify you know in earlier uh situations sometimes features means slightly different things um but sometimes you can just say this this is a new input this is a new output the new out to be the same and you just work on linear regression on a new data set and that's how you learn a cubic function for this data set right by turning it by kind of reducing it to a linear regression problem and I think this is a piece that one I think it's Q4 but maybe we change the order um of this of the questions a little bit um right any questions so far um [Music] uh how do I know this is better than that what yeah this transformation uh then no transformation okay so yeah so I guess it's just uh you know you you know of course eventually you have to validate your method and see how it works but the intuition so here I'm not saying it's better I'm just saying that if you believe that you should use a cubic function this is how you implement it right so whether you believe that you should use the cubic functions of a linear function that depends on the property for data maybe you should experiment with it you should probably look at data to see whether it is a linear function there are other ways to decide you know which model we want to use I think actually in two or three lectures uh later we're going to talk about how do you pick different models okay so yeah so exactly I think so this is a way to implement a cubic function uh at least for one dimension okay and and what you um gonna do is the following so um so how how do you proceed right so like you basically just uh you know repeat what we have knew about we have no about like a linear regression you just write down an algorithm uh on this data set right and and implemented linear regression algorithm on this data set I'm going to um repeat the procedures but this is basically just exactly you just invoke what we have learned uh from the first two weeks but I'm going to repeat it because uh this part I'm going to use it we use it again to demonstrate uh something else to demonstrate a kernel trick so um so how do we really do this so if you do linear regression on this new data set so I guess let's say we use green reset okay so let's use green descent and so on the loss function something like that's right you compare your label with um uh with um you know suppose you have fun you compare a label with uh the model which is V of x i and square right recall that you know if you do the standard linear regression what will happen is that um for the standard case then this will be x i and this will be x i in the most standard case and what I'm doing here is just replacing that x i by Phi of x i and then if you work within yourself what's the formula the formula would be something like you have to have a loop right so and each time you are updating your Theta um by theta plus some learning rate times the gradient and the gradient if you compute it it will be something like y I minus Theta transpose Phi of x i times V of XL and for the standard case then this two two things will just be both x i if you look up the lecture notes there will be dispose of both x i and now they becomes this Transformer transformed version of this I guess so the quest teaching this location like this is supposed to be that you update your status by Theta process that's the notation makes sense okay sounds good so you know it's in some sense just a short hand you know you know if you really want to be very kind of like if you really read a mathematical paper maybe they will call Theta t plus one is equal to Theta t plus this I'm saying that in a standard case this will be x i in the in the in the first two weeks now I just replace it yeah all right okay so um and there's some small differences which is that you know if in the standard case so now what's the dimension of theta the dimension of theta is not the same as the dimension of the X right so suppose you say X is of Dimension Rd um maybe I should use black color just to be more consistent so and po Fox is a function that Maps Rd to some other dimension it doesn't have to be exactly the same Dimension right before we map from one dimension to four dimension but you can from any Dimension to other dimension so let's call this Dimension P so p is the dimension of the so-called new feature the in the new inputs so and that means that your Theta also has to be in the same space as V because your Theta will be a linear combination of the transform inputs so Theta is also living in this space so so so basically we are updating um in this P dimensional space instead of X dimensional space and and let me also just briefly talk about the terminology so I guess so I think this is often called a feature map so and and this V of x maybe I think I should call this like something like this maybe the right notation should be this so fees as a function it maps from RD to RP and P of X I think people often call this at least in this context called these features and if you look at you know this kind of like papers or kind of research in this context often people call X the attributes just to distinguish it from the features um you know the terminology doesn't really matter that much like in most of cases you can infer the term terms if you really know what they uh they mean so um but I'm just uh giving you some kind of context when if you read the paper which says that access the attributes and fear facts the features they know what they mean but sometimes other papers would call it differently maybe some people would call Access raw features and call these features you know they are there are multiple ways to name them which is a little bit unfortunate um okay um anyway so but feature map I think people always call this feature map maybe sometimes people call this feature extractor um um but they all mean the same thing and I guess maybe let's just so my extrapolation from reading all of these papers I realized that features you know even though sometimes they mean different things often at least from my statistical learning inference of this world I think this word seems to mean it always means that something that you use a linear function on top of these things so basically if you're there's a linear thing on top of something and then that something will be called features like uh um you know just uh so so basically like uh it often refers to those variables in which you have a linear you have a your model is linear okay so are we done right so um this sounds great right so everything is so clear and simple nothing seems to be complicated you just replace x i by feel flexor so um so but we are not done so why because you know in some cases this is great right so um like for example example of the homework question if you look at it but we basically ask you exactly Implement something like this I think we ask you to implement a different algorithm we didn't ask you to implement the green infant we ask you to implement the uh the you use the the exact solver use the the um the inverse of the the time some Matrix some Greenies kind of things so um but you know but the same thing right you just apply your existing algorithm on this new data set um but but this is not done because you know because of efficiency issue but I have I think I saw a question yeah why is it why is it hot here I think this is the so that's the gradient doesn't have like a it doesn't really matter right but but if you have a half then you always agree in front of the green there's no two it's just convention I think if you I think if you read the the the the first two weeks also we have a half right typically there's a half um um okay so um all right so okay so while we are not done the reason is that sometimes there's an efficiency issue uh with this with this kind of approach and the reason is that this feature map can be very high dimensional so P can be very large in some cases for example suppose you know in this case p is four right so it's which sounds kind of pretty fun but what if your ax is High dimensional right so here x is one dimensional and p uh the features are p is four dimensional but what if you say I'm going to have a x 1 up to XD right I have D coordinates in my raw data and now if I want to create something like this um that have all the the common the cubic monomials of the of the coordinates then what you have to do is that you're gonna have something like Phi of X needs to be something like a gigantic Vector where maybe you have one here you have X1 X2 all the degree uh one um monomials of the coordinates and then you have the degree two I think x minus two maybe you should start with X Y Square x one times X1 X1 X2 up to X by x d now X2 X1 so and so forth maybe maybe x 2 x 1 can be omitted because there is a repetition but it doesn't really change the point um so you're gonna have need to have a very long Vector that lists all the possible combinations the products of these coordinates right so you're going to have like maybe X1 Cube up to eventually x d cubed something like this so and only when you have this then you can claim all the degree three polynomials can be written as a linear function of this of these features right so and so so and and then what is p what was the dimensionality of this new features so if you contact right so there is so I'm going to do some rough content so this one there's one one and for this kind of like degree one thing you have D of this right and for the grade two thing I guess uh so you're gonna have degree two thing maybe so there are d square of this let's say we ignore the reputations you know the reputations only change the constant factor which doesn't really change the point um so you have d square and then for degree three thing you you can start from X1 X1 cubed 2x D Cube and with all the combinations you're gonna have D cube of this terms I guess this is a little bit okay okay so right so then this means that um so even though you know of course every so every the good thing is every degree three polynomial can be written as you know can be written as some but the bad news is that V of x is in this RP where p is something like the D Cube um you know technically B P I see in this case you know if if I do this p is equals to 1 plus this plus d square plus d Cube um the dominating term is D Cube and this is very high dimensional uh Vector so if you think about this uh for example let's say maybe you suppose this is just a back of envelope calculation suppose these 10 to the three say a thousands then P will be a billion so and and why this is a problem this is a problem because if you run algorithm this algorithm then how many iterations how many uh how much computation you have to pay here so if we look at this so I guess if you count how how much competition we have to pay this product requires of P computation right all p in the so you are taking the product of two inner paradoxical two uh to two vectors to P dimensional vectors you have to pay P numerical like operations right P multiplications and you also have to sum the sum also has P operations so it's all of P operations for this in the product and a name it becomes scalar and you you take the difference between this and this that's fine and then you multiply by this which also uh this is scalar times this which still takes all P operations so evaluating this whole thing takes off P operations that's still kind of okay but you have to have take this out and there's a multiplicative Factor because you have to wait you know repeatedly so totally evaluating this the whole thing takes off NP um like multiplications or or additions so so you have to take all of n times V time so basically the runtime depends a lot on P it's linear in p and if p is something like 10 to the nine then your run time will be 10 to the nine um times the number of samples you know which is often prohibitive so this is very very slow on on empirically if you do let's just implement this and when D is a solid any questions so far okay so the main goal of this lecture is to find out a way to speed up this algorithm that's called the that's the kernel trick what's the kernel tricks about it's about to how to uh implement this non-linear model um using this idea better but but implementing it in a way such that you can be computationally efficient so so the final algorithm will be equivalent to this algorithm but it will just be faster and actually much much faster focus on that like whatever alternative he is going to stay as the same um the second the we are gonna we are going to try to implement this algorithm in some other ways but but UCP will you know I'm not sure whether I will say yes to like you know we will do something with fee yeah foreign so the the whole point of the the rest of the lecture is to have a faster algorithm and you know maybe just a a side of philosophical remark I think machine learning is really a lot of about machine learning is about computational efficiency even though these days sometimes you know like you can use gpus right so but uh but I think at the heart you know at least a good fraction of machine learning is about computational efficiency because many of these kind of statistical questions in some sense you can say they are studied well in some sense like in in statistics and and I at least immersion learning I think at least comparably comparatively I think we focus a little more on computational part but the first competition is only one part right so like you also have to care about statistical perspective um but I'm just saying that computation is important you know it's not just a merely uh or a separate thing that you you can you can resort to an oracle so in many things you have to really think about computation because otherwise you cannot Implement your algorithm you cannot run your algorithm on a large their side then you wouldn't see a good result um and and this is one way to speed up things um and we're gonna see other uh ways okay so um okay so how do we speed up this so here is a key observation to speed up this so so the observation is that um the Theta can always be represented as you know you may found this a little kind of like a surprising at the first start but I'm going to um explain so this Vector Theta can always be represented as a linear combination of the features for some scalar where this beta I is the the scalars for the for the uh linear combination so for some later one up to Beta n and each of these beta is in r uh and okay I guess there's a condition here so if Theta 0 is 0. so I guess I'm going to change just uh for the sake of Simplicity I'm just going to decide my initialization for this algorithm is that Theta is equal to zero it's initialized to zero so then I'm not going to have this condition so I'm going to claim my algorithm I'm going to say that my algorithm to start with Theta is zero you know initialization probably doesn't matter that much right so because either way you're going to update a lot of times right so um so if you start with zero then in this algorithm no matter what time stata can always be represented as a linear combination of our features so and why this is you know useful what will be this useful you will see but roughly speaking the reason why this is useful is because Theta is a p dimensional vector right and I'm in a regime where p is probably maybe just uh I'm thinking of so I'm in a regime where regime uh is that P is larger than n so let's say p is really really big extremely big right even bigger than n so so Theta is a p dimensional Vector which is extremely big but now you represent this vector by the scalars and you have unscalers so so in some sense you have you reduce the degree of freedom in your application at least right so before we have to represent store P numbers but now you only have to store in numbers if this claim is true so so in some sense you will see that you know I'll tell you more about details in some sense you'll see that the way we speed up this is that we never store Theta explicitly we all we always store the the representations of data right the beta ice is my surrogate for for phosphate and that's enough for many of my computation like is isn't the sum their escape this is a scalar but this is a vector right this is a pdmi stone vector so this is scalar so so but you but you don't have to store this because these are already there in some sense these are not changing even right so only the beta will be changing where beta is a surrogate Force Theta in some sense so so um so you don't have to store how data changes and that implicitly tells you how stata will change over time we'll see exactly how this works um right so but maybe before going to that let me just uh um show you why this this is true why why this is true the reason is actually relatively simple um if you look at this so the reason is that pretty much you know in short in a nutshell the reason is that every time you update you always update Theta bioscaler this is this whole thing is a scale this parenthesis is a scalar whatever scalar it is you always update about scalar times this vector v of x i so basically every time you update your update is of this kind of form it's a linear combination field of exercise so that's why you keep being keep having it for um if you want to prove that you know um you know more formally I think the kind of the statement you have to do the induction so let me also try to do that so by induction so first of all you check at iteration zero so at uh iteration zero so indeed our Theta is equal to zero that's my choice of initialization and this is indeed equals to a sum of linear combinations of the the feature vectors right because I just have to choose my Beta I to be zero exactly right um that's easy and maybe I can also just uh to build up intuition let's also look at iteration one even though I don't think this step is necessary if you really care about the formal proof this is just for intuition and iteration one then what you do you you update Theta to be equals to the over Theta which is year zero plus um this gradient which is Alpha times uh I from 1 to n y i minus Theta transpose Phi of x i feel facts I and this thing you know um is a scalar actually is equal to Y because Theta is zero right Theta was zero in the previous division so this will be just y i times V of x i right Theta was zero and you plug in 0 here you get this so this is indeed a linear combination of the feature vectors right so this is a vector this is a scalar and so Alpha times y I will be my Beta I in this round right so so this thing plus this thing together will be my maybe I'll just write Alpha here so if I write Alpha here then this whole thing will be my Beta I add iteration one okay and you know for the future steps you know I wouldn't I wouldn't be able to explicitly write beta I that carefully but I'm going to use the induction right so so suppose I'd iteration t I already know that Theta is equals to something like sum of beta I Phi of x i I'm already this is my inductive hypothesis and then a next iteration you can see that Theta is updated to b equals to theta plus Alpha okay I'm just going to copy this formula again actually if you just I just want to prove the induction I don't even have to plug in what Theta is because if I just care about induction right so I know this is a linear combination field of x i and this is a complete the combination field of x i then the sum of them will be linear combination of V of x i but I'm going to do a little bit more detail steps because these steps will be useful for me as well uh so I'm just going to plug in everything so I'm going to plug I'm going to replace Theta by my inductive hypothesis and then I'm going to replace this data by my inductive hypothesis as well so I'm going to have Alpha times okay what is this so Theta is equals to uh let me see one moment there's something okay I guess let me maybe let me just skip this okay so and this is equals to just the sum of faded eye plus Alpha y i minus Theta transpose Phi of x i I didn't do anything really complicated it's just a super simple uh manipulation right I just and now this whole thing is a scalar whatever scalar it is this will be my new beta I right in the next step any questions [Music] yeah this doesn't work for randomization and but you don't need to use but actually in a position zero is actually probably the best um maybe maybe there's some reason for this but maybe let's discuss that uh offline so initialization um it does matter a little bit but but let's say is we just take initialization fees zero Okay so okay so now let's proceed with my plan my plan was that I'm going to replace so so so so far I just don't want to prove the claim right the claim is that I can always represent say that by Beta and now I'm going to just only mention beta so so basically my plan is that I was just only gonna maintain beta but not say so so that means that I'm going to start from so so P parameters before it was P Paris and now it becomes n parameters and if p is very large then this this means some saving so so that means that I have to understand how the beta is changing right so I have told you that beta exists right so when beta is equal to this but I want to have an update of data that only so here right now beta depends on Theta so you found to compute beta the new beta I have to know the old Theta which is uh which kind of defeats the purpose right so if you have to know what's the existing Theta then um then you have to compute it right so then um you raise the P time so what I'm going to do is I'm going to find out how does beta depend on beta itself without going through a site so that's uh what I'm gonna do so where should I Maybe yeah I'll write down the left hand side I guess maybe I'll just write somewhere here so that there's some locality uh maybe I can erase this so so this is the update for beta I so how do I update beta I so beta I is supposed to be equals to Beta I plus Alpha times this way times this so this is my rule for my starting point I know that beta is equals to Alpha times y i minus Theta transpose Phi of x I but this rule is not great because it does have sailor here so I have to maintain Theta I'm going to get rid of data by plugging the definition of theta in terms of beta so this is equals to Beta I plus Alpha times y i minus what's the definition of theta so if you the sorry what's the representation of theta so Theta is equals to this right this is the relationship between Theta and the beta this is a j because I'm having some inside and this transpose times Phi of x i and parenthesis okay so um and then I'm going to continue so reorganize this a little bit so I'm going to get beta I Plus Alpha times y i minus I'm going to um put this inside so sum over J from 1 to n beta J Phi of x j times Phi of x i this means in your product right so a transpose B I guess this is just a inner product with speed that's what I'm using so and this is I and this is J okay so have I done okay so what I have achieved I got rid of theta so now it's a it's of the rule from beta to Beta itself right so if you know the old beta you can compute new bit that's what this is saying but did I really save any time not yet right because if you want to compute the new beta from the old beta you still have to do this in the product this inner product between two P dimensional vectors still takes P time and you still have the sum Over N indices right so still of MP time so I haven't really saved much just because the fees still shows up here however here is the Magic The Magic is that somehow you can compute so so this still takes off this is still of MP time right and this is uh so far not really much saving there are two things we can notice to to make it faster so one thing is that this inner product um can be Pro can be pre-processed okay so you don't have to compute every time right because this is just something that doesn't really change over time in your in your algorithm but over time I mean in in the as your algorithm is running right this quantity is not changing at all right so for every IJ right the beta will be changing right so but this wouldn't wouldn't be changing so and another more important thing is that so so this means that you can do it once and you don't have to complete it every time so at least you only have to do it once in the very beginning and another thing is that oftentimes this inner product actually can be computed faster than you thought right the trivial way to implement this is that you just compute this Vector this vector and you take inner product but actually you can do some math to speed up this in many cases um so this can be computed without even evaluating fee explicitly evaluate each of these factors you are the waste p time but sometimes you can do some math to not even evaluate the fee that's what I'm going to show on next any questions basically we are representing something in the space of RP in terms of linear combinations of vectors in order so actually uh last number of factors than the dimensional space itself so does it make it all over approximation of the initial results where is it precise um it's still precise because so far you see that I have never done any approximation right but your question your question definitely makes sense it's a very logistical question right so why you can somehow magically save money degree of Freedom right I think the reason is that um this might be a little bit big at this level but the reason is that even you had this P dimensional uh degree of Freedom key degree freedom in a third time when your data is not as large as P you cannot use all of them you could it's not it's not like you you are not fundamentally using all the dimensionality so or all the degree of freedom in the face in some sense like the if you have any samples the maximum amount of degree frame you can have is and that's why you can save even without any approximation right so here it's really to see a real implementation of the same algorithm we didn't lose any it's not like you're doing any approximation um I think no I think you have to utilize with beta0 be all of this only works with easier initialization so so state has to be tries to be zero and beta also has to be initialized to be zero just because that's the correspondence at the beginning right so then this wouldn't help you ice probably would hurt you yeah we will discuss a little but at the end like when this will be useful I think this will be useful only when p is really really big yeah but sometimes p is really a bit like like in the case um yes yes so so like um so okay so if your question is like what happens with my Beta Is Random I think in most of the cases maybe in all cases I don't know in most of cases I think you probably would get the same answer eventually like when you run this algorithm in a data space but the correspondence is only zero so there's another reason why like okay so so maybe maybe here is the exact answer to this so suppose you only care about what happens with beta is run initialized to be random then I think it still can work and it can actually still give the same solution in most of cases is basically zero initialized initialize to be zero but that's that's a different reason um that's a there's a different reason for this to be true because just because you're doing some complex compensation eventually you always converge to the same solution probably that's what you are only um kind of you you mean as well but this correspondence between sailor and beta you know on this level it only works for beta is zero that makes sense um okay any other questions [Music] we still have to calculate like we're pre-processing it we're actually a single combination right right how is that speeding anything else right so so so so that that means you only have to do any Square pairs right you know it doesn't you know how does that so that still is a lot of time I agree but the the difference is that if you do it on the Fly then you have to do this all IG pair for every iteration so I'm saying that comparatively this is faster I'm not saying absolutely is very fast so so suppose you do this inner product every time then you still have to do it for every inj because here you have a j here you have some over J and also you have to update for every eye so so you basically have to do all of these pairs for every iteration so I'm saying that at least you can save that at least you don't have to do it for every it will be higher than the number how many iterations will we end up having um the number of iterations I think that's a little bit tricky like but but let's say you know let's say you have two iterations right so before you have t times this number right and now you just save that t oh right that that's the only thing okay the the biggest saving is probably come from coming from here which I'm gonna show right now like like a each of these inner products actually cheaper much cheaper than than than we typically think okay so why that's the case so um of course this is not like a this is not a universal statement right it's not like forever fee you can do this but for many of the the fees that we designed or that that makes sense that intuitively makes sense then you can speed up and actually later on this becomes a principle you only design free such that this is fast you don't you don't care about any other fees um but you know but let me say Let me let me not get into there let me just talk about this particular what right so for this fee I'm going to show you why this can be fast and and the reason is very simple it's just that you can do some math to to make your own formula easier so so for the fee that I Define here so fee of x times Phi of Z uh this what is this I'm taking I'm abstracting a little bit I have x and z just the two two things right you can think of this as x i this as x j um I'm just using more abstract notation so this is just the inner product two vectors you know one vector is one um X1 up to x d x y Square so and so forth you know and the other one is a column Vector which is one Z One Z two up to z d and Z1 Square so and so forth right something like this so and you just take the anchovies product and take the sum right so this times this will be one this bunch of things times these bunch of things will be sum of x i z i i from 1 to D okay so now let's do the degree two parts right the degree two monomials so this will be so what are this coin this these quantities right here the degree two things so they're all of the form x i and x j right and here you have this bunch of these they're all of the form z i z J right and you take the corresponding Paradox and take the sum so basically what happens is that you Loop over all the possible choices of I and J and you take x i x j times z i z J and then you do the you do it for the degree three part which is the same so you Loop over so all the degree three parts there of the form x i x j x k and on the other side you have the form z i z j z k and you take the sum over all possible combinations of i j k so this is from 1 which is from 1 to B okay so and now let's simplify this and so you can simplify this by So This One X this one we don't simplify this is already pretty simple um you can notice no otherwise you can simplify x times Z but this doesn't really change anything it's still the same computation but then for the second term what we can do is that we can we have a two sub right we can factorize the two sum so you can write this as you first take the sum over I you look at all the terms that depends on I that's a z x i and z i and then you also have the part that depends on J so j z J so in some sense I'm just using uh I've checked speaking I'm using the fact that you know if you have sum of UI times WJ where I is from 1 to D J is from 1 to D then this is equals to sum of UI times sum of w j right so that's just the formula I'm using an UI corresponds to x i z i and u w j corresponds to x j that's how I use this abstract formula so um okay and then I can do the same thing for the third one which will be equals to again you factorize based on i j k you collect all the terms that depends on I which is x i z i and you collect all the terms it depends on J which is x j z J and you collect all the terms it depends on K which is uh x k c k so okay what's good about this the good what's good about this is that you know you can see this one and this one are actually the same thing you use you are just changing the the way you index the terms but but anyway you are you are taking a loop some over all the terms right doesn't matter what indices you use and and the same for all of this all of these are all the same all of these are equals to X transpose X in the product we see so you find essentially what you get is that um essentially it's like a you know what you got is just a x inner particle Z Plus X inner product with z square plus X in the Paradox with z cubed and why this is helping you in computation the reason is that now it takes of D time to compute X commas inner product with Z right that's that's of D time but now P right this is just the your your raw feature like your input dimension and then after you get this you can take the power the second power this one is a scalar right after you get X in the power C you just take a scalar Square you get this and you take a scalar Cube you get this right so so the whole runtime is really just X the time for doing this Plus I think four or plus some constant because you just take the power and you take the cell so so the total runtime is total time is also of d like a look or I'm ignoring the constant you know I'm assuming D is big any questions so I guess like if we choose a fee smartly and appropriately then we will we might not even need to compute the whole do we we might not even need to put our data set right right so here you don't need right so here you don't have to implicitly save fee because okay so exactly that's a good question so basically what so what happens right now so basically you compute this quantity but you don't have to know what fears you just complete this quantity using the using the circuit and you compute them you compute all of this in advance and then you run this algorithm so let me write down this more formally so formally what you do is you let me also introduce some notation to kind of abstractify this because it will be also useful um especially if you read other papers and the related works so um there is a notation people call this Define the so-called kernel function is defined to be this precise at this quantity we care about the inner product between features this is called kernel function so this kernel function as the definition shows is something like that takes in two vectors and output a single scalar and and the algorithm is just that um so if you use the kernel method basically we have show you the steps but now I'm going to group everything together so basically the final algorithm is the following um maybe here so what you do is you say you first compute you're pretty precise all of the inner product so you you compute K of x i x j recall that this is defined to be the inner Paradox of these two things so you compute this for all I and J from one to n okay um and then you say you have a loop so I guess you start with say beta0 and so beta beta is a vector in RN so beta is the collection of beta 1 after beta n and you start so you start with better zero and then you do a loop and this Loop will just be something like uh for every I from 1 to n I'm going to use this update rule for beta so what does that mean that means that I'm going to update beta I to be with I plus Alpha times y i minus sum of beta J sum of beta J times this inner product by this inner product which is something I've computed and I denoted that by K of x i and x j right so that's my algorithm and and now we can take another kind of like accounting to see what's the run time for this particular polynomial feature so the runtime is let's see okay so I guess you know you probably can also got a runtime so to compute this we are using the formula like this right to combination part that we're using this formula so for every pair you need all of the type so this is O of t for IJ so of N squared D in total hmm [Applause] right and here my runtime would be um let me see [Applause] I guess I would I don't have it on my nose somehow I don't know why anyway but I can do it on the fly so you already know this number so basically you need to pay n times to a time of end time to compute this sum and you also have to update uh each of the this and each of the I right so for every eye you have to pay and time so totally this is all of n Square time and square time per iteration so the catch is that there's no P involved anymore whatsoever so you just only have n and D um and if your n is small enough then in some case you know as if n Square for example is less than NP then you are winning right recall that before when we run this every iteration we have to take n times P duration and now every iteration we have to take an Square iteration so if n is less than P then you are winning and in many cases n is much smaller than P just because P could be a D to the three right that's just a lot of it so [Applause] um at least I'm not saying this is universally better when I'm actually I'm only discussed you know when this chronometer is better especially as you can see it's better when p is very big and small right but in those cases you do save a lot of time can you do update one day at each time so I think that's that would that would be called uh coordinate within this is right because each of the beta I should be considered as a as a coordinate of your parameter but that's just terminology I think if you update each of them one by one um You probably have to use smaller Alpha but in theory you should it should still work if you small enough Alpha but I don't think you would gain much by doing maybe you can gain a little bit but but it wouldn't be a fundamental difference you know maybe it will be faster a little bit any other questions or what I'm adding squared or per iteration how many tuitions I need that's a little bit hard to decide because it depends on the problem and um but so so that's why we only compare per iterations so far run time on my part if each calculation C runtime like the total is over and b square or N squared B plus o of N squared D times I think I think I see what it means so so if you really care about the total the final final run time I think you would call it something like and Square D plus of you know and squared times T where T is the number of iterations but what is T you know the reason why I don't discuss like that is because t uh is a little bit tricky like it's a little bit uh problem more problem dependent um so so so that's why and um yeah but it was a good guess for tea it really depends on the problem sometimes it's pretty small sometimes it's it could be a little bit larger um so so suppose you ignore the the pre-processing time then you can compare roughly speaking and compare the per iteration runtime so maybe maybe the first implicitly right so there's no way to exactly compare everything exactly so um unless you you have more specifications of the problem but roughly speaking though I think the idea is that here there's no P showing up so so so so it has to be better in some sense it's it's kind of pretty easy to be better than so for example p is really really large I suppose p is like 10 billion right so whatever you do here you probably it's going to be better than something that has p in it yeah so um what so I think and and this observation that you only have to compute the inner product of this of the features um it sometimes has more profound kind of implementation Implement uh implication on the the thing is that you know when we realized that this inner Paradise the only thing you care about then you start to wonder what do you really have to think about what it feels so maybe you just have to start with the kernel and then as the field so basically so here is the um a change of mind side you know that researchers have kind of in some sense done in for this kernel method so so so we started with the fee and we defined a kernel but you can also start with the kernel so you say I'm going to define a function like this and then as long as there exists a fee then you are done because you don't have to know what fee is right so you don't have to know the kernel is the inner part of something but you but you really have to care about those fees and the way it's not implemented it's not used at all in your algorithm right so so roughly speaking I think one way to deal with do this is you just Define K and you get you just forget about V and different points um [Music] P dimensional space this p is greater than M so isn't that this will be over 50 in this case so yeah yeah yeah I think that's uh that's a great question um you don't necessarily have to overfit but this is um um the short answer is that you don't have to you don't necessarily have to overfit um because uh okay I guess you know I'm I'm using languages that we haven't discussed in this course like you like the norm of your solution may not be that big like even though the parameter of parameters is a lot but you can have a small Norm solution and that still can help you generalize but this is something that uh I don't think we're gonna discuss in very much detail in this course even though we're going to touch on this a little bit but not much okay so so basically let me just say you know let me continue with the the this change of mindset uh in some sense so what what people do um is that you just forgot about a fee and you just don't only work with the kernel because the all the algorithms you know you don't have to even know what fee is so so basically you know one thing at least that is tempted to do is that you can use the Define okay and then you just run this algorithm because after you define okay you can run this algorithm right but of course you know you cannot just use a arbitrary hey because if you use the arbor trick hey then your algorithm doesn't have interpretation you don't know what you are really doing it's just the algorithm right so what you really want is that you want okay that satisfies this for some fee but you don't have to know what exactly fee is so so basically I think so that means that you have to understand in what cases you can Define of K and that algorithm still makes sense so so basically people call this if K is a valid kernel foreign if if um there exists of a function fee such that K of X comma Z is equals to the inner product so so basically you can just design any function K as long as you guarantee that the existence of Phi as I always guaranteed that this is a very kernel then you can try to run that algorithm and you know your algorithm is really just doing a linear linear model on top of the the feature fee but you don't have to know what V is once we get these final because we'll apply them to feed that free version of the data and then we'll get the thetas and formulas that's a fantastic question that's that's great so yeah I think I I think I missed this small part I forgot okay my back so but yeah but I also you know um like yeah this is exactly you know what let me talk about this so the test time you also don't need the fee that's the thing so so the test time by test time you mean I mean if you if you give me a new data point x right so give a new data point x so the question is how do you compute how to compute this thing and the question was that you know it sounds like you have to have to compute feed in the Theta right um from beta and then you you compute this but actually you don't have to because you can just plug in the formula so Theta transpose field of X you use the representation as much as possible and and use math so so you try to replace Theta by this linear combination by the I times Phi of x i so I suppose Fairfax and now it will regroup you found that this is still about inner Paradox of two six so this becomes sum of beta I times the kernel function applied on x i and X so if your kernel function is evaluated very fast then you don't have to know what Phi is okay that's a quick question thanks for reminding me right so if you know okay you can do the training you can do the test um and the only thing you have to guarantee is that this K is a valid kernel so that you are doing something sensible um by doing something sensible it means that if you know it's a valid kernel you know that you are doing linear regression with the fee as the feature so in some sense like a there is a if you design a good fee a good feature is kind of almost the same as designing a good k because I know you don't know what fee is good right or not right so probably you should just go directly designing K and advanced K is a very kernel you just run it and see whether it's good or not [Applause] what is x i is samples or like training samples this is the treatment samples yes that's a great question so X eyes are still the chaining samples and an X is a test example so it's interesting that in the test time you still have to look at the training example to do this to do this test it's not like you have you just have to remember right right so so like in a typical case you just have to store Theta and then you don't you can forget about the tuning site but now you cannot forget to change that so and I guess I have like 15 minutes so I'll briefly discuss you know when do you know this is a valid kernel because this is so you need to have a way to you know if you want to use this kind of like equivalence and just forget about hey you have to have some way to tell whether your kernel is valid right in some sense so um and there are some mathematical characterization of when the kernel is valid this is uh um um uh let me write down the theorem see which sport did I which one should I erase maybe this one okay foreign there's a necessary and sufficient condition um so [Applause] so necessary condition so so if K is valid this implies that okay maybe let's let me let me first Define some notation so in the in the literature people also have you can Define the following suppose you have X1 after accent these are undated ones Define the so-called we still use the same thing in the same notation I don't know why people do this but so now this is with the abuse of notation you define a matrix K I call it kernel Matrix and this Matrix is a unbatant dimensional Matrix where k i j is equals to this K is the kernel function applied on this the parents of data I know this is a little confusing I don't know why people keep doing this like this is a on the left hand side this is a matrix you define a matrix based on the function the kernel function so every kernel function you can use that to Define Matrix and this Matrix is basically the evaluation of this kernel function on particular data points um you know if I think probably you know we should just call this came another thing like I'm whatever you call it by so it's just there are two different things but people keep to seems to use the same notation for the fault um and um what you know is that so maybe I'll just uh probably talk about the the full condition given that we don't have a lot of time so unnecessary and sufficient condition is that if the k so K is a is a valid kernel function so that means I'm talking about a function but now the Matrix foreign this is equivalent to for every X1 an accent if you choose any examples then the kernel Matrix K this is a matrix that's defined like this is PSD is positive sum definite um one side of this claim is pretty easy to show because from the value kernel to this I think it's really pretty much just a simple calculation so you just plug in the definition that K is the inner product of two feature functions feature vectors and then you can pretty much verify this current this Matrix PST the reverse direction is a little bit kind of tricky and and also my statement of theorem I think I'm missing some regulatory condition if you if you really care about the exact math but but this is like up to a small manner regulatory condition you have to say this K is boundator like a continuous kind of things um right so basically after you have this theorem then the workflow is that [Applause] so in some sense kind of the workflow is kind of like you first design okay a kernel function right there's a kernel function okay and then you verify K is valid and how do you verify maybe you can use the theorem above but that doesn't really mean that it's easy because you still have to use this theorem in some way try to prove that the kernel makes with PSD but there are some ways to verify this or you verify by either by using a serum or just by constructing all by constructing the explicit Feature Feature map that make a valid so you do something like this and then you just run your algorithm run on you run this algorithm that we defined somewhere I guess here um with k and and they are so but here we are using so how do we get this algorithm we are using the regression loss right we are using the square loss and linear model right but you can also kind of like use other uh starting point for example suppose you start with logistic regression with the feature and then you can do the same kind of like operations like we have done and then arrive at a different algorithm in the kernel space so in some sense this is called kernel trick so basically a kernel trick or sometimes if you call this kind of kernelite kernelized it means that you turn an algorithm so you turn an algorithm uh uh into into this algorithm like algorithm about Phi Phi X into a algorithm uh about okay this kernel function so so you may start with logistic regression with VX as the feature and then you can do you know I'm not I didn't tell you how to do it but you can do the same type of kind of derivations as we have done today to get another algorithm um that only uses K and the algorithm would look something like this but not exactly the same a little bit different and and if you can do this then you you say but not all the algorithms can be turned it can be can be can be done in this way like a like a not all the algorithms is a mean about this so-called kernel trick so some algorithms is possible some algorithms not possible um I think in the homework we have this perception algorithm for homework too um which can be turned uh into into kernel uh kernel can be kernelized uh I guess I'm already kernelized um or you can apply the kernel trick and now just the question I think you can apply chronologic um but some of the other algorithms if you use L1 organization if you have heard of it we have I know we haven't talked about L1 organization but some of the other algorithms cannot be colonized just because your algorithm so okay and also maybe one thing why in what case you can kernelize this so like a kind of the key is that everything can be written like the only way you use the features are about the person of product between two data points the features that do two data points if your algorithm somehow can be turned into a way such that the only thing you care about is this then you can kernelize it because just replace this by the kernel but if your algorithm cannot be written in a way such that the entire algorithm only cares about this inner product then you wouldn't be able to chronologize um right okay and then let me also briefly say a few words about some of the other kernels right so only show you one kernel on some of the other kernels include um they are actually not that many you know sometimes um they are not the many general ones but sometimes for like the particular applications you can design particular kernels that looks um that they are useful for you so um so some of the kernels are something like for example uh we have defined this you know k x z which is something like you can for example this is a something that is very similar to what we have so KX is equals to X in the product we see plus C to the power k so for any choice of c and k uh this is a kernel um and actually you can write this as something like um for case two I guess I actually know so far okay it's two actually I even know what's the feature the feature is something like this C square root 2C X1 square root 2 c x two so and so forth and X1 square up to XD Square you don't have to know exactly what is this like I don't even remember but but but for this case you know for case two you can write the explicitly what the the features are and for other case you can also write explicitly what what they are um but you don't have to carry that much about it right you only have to know the existence of it and then we run it you just use this kernel and another kernel is called gaussian kernel which is something like exponential minus x minus Z Square over to Sigma Square here this is two norms so and this one you can also write it as an inner product between C of X and Phi of C but this fee will be much more complicated actually you can construct oh sorry yeah you can construct a expensive fee such that this is true but this construction will be very complicated and also another thing is that this has to so fee has to be infinite dimension so sometimes you can fee has to be even infinite dimensional vectors you know I I know we we didn't talk about what exactly the infinite dimensional Vector really mean but but that's kind of the idea so you have to really use a lot of Dimensions um to um to to express this kernel but you don't really have to know what what's the what are exactly the fees are because when you run the algorithm you just don't care about this right so so in some sense this so this is the way that we can deal with info and dimensional features so so if your features that it's not a problem like as long as your inner product between two features can be computed efficiently then the dimensionality of the features just don't matter like uh it can be very large or it could be like infinite it doesn't really matter foreign people think of this you know features as a similarity metric where you can think of this this function as a way to measure x and z um of course you know that's just the in some sense that's the in interpretation or intuition um so um when x and z are similar I think at least in this case you know when accessory are similar you look at a larger number so that's why you can think of as a similarity Magic um all right so maybe a final comment is that you know how do you think about this kernel trick in the modern by more than I mean like in the last five years or last 10 years so you can see that like the runtime here right so they always depends on N Square actually it's always N squared so so you save a lot uh in terms of the P the dep dependency you just even get rid of the P dependency completely but you what you lose is that you get N squared instead of n recall that before when you have um like the vanilla way to implement it you get n times P the p is there but earn the power of only is only what right so but now the the power on N is two so which means that you know if your own is very large then this n square is actually a bad thing right and square is much worse than n so you lose a lot in terms of the dependency on n and this is probably one of the reasons why kernel method is not um used a lot you know these states you know just you know that's that's one reason I don't think it's a fundamental reason is I don't think it's the the most important reason but it's a it's a it's a it's a it's a it's one of the reasons so the reason is that in these states you know your data set is at least in some applications and it could be like a million and when you have a million square that's like 10 that's like a a trillion right so that's just prohibitive so um so that's the the problem but of course there are also also other ways to speed up this a little bit like if you really uh care about the runtime so that's the second reason why the kernel method is not used as often as before is that um all of this requires a design you have to design your K or fee right so whatever you do is it's really a hot so-called handcrafted feature right you do Define a function fee yourself um you know you can you can choose them but it is it's by human design I already designed okay so when you do um at least one way to interpret uh what new right work will do is that um we I know we didn't discuss new artworks but I assume some of you know a little bit but um generally you know on High level um at least one way to interpret why the right work is better is is is that you can say new network is actually in some sense learning this function fee so so as you will see I think when you have new artworks in some sense you can view this as a model as something like Theta transpose fee of X but this Phi of x is parametrized by maybe let's call them some parameter w so so you can view the video networks like this so it's a still a linear function on top of some feature but this feature itself is learnable based on the data so so you have a learnable feature instead of like handcrafted feature and that's one you know one of the many intuitions why new artworks can can do better and and but we do see this very often so these days we have a lot of this kind of Feature Feature extractors that are learned by data um and more and more more of this and and if you use them then you just um um like it's much much better than just a random uh much much better than the kind of those polynomial features we designed um on like in this lecture um okay I guess that's all I want to say for today um thanks"
Stanford CS229 Machine Learning I Neural Networks 1 I 2022 I Lecture 8,"Stanford CS229 Machine Learning I Neural Networks 1 I 2022 I Lecture 8

hello everybody hi my name is Masha um some of you may have met me already as part of office hours and seen me post on Ed and things like that I'm really excited to be giving the lectures today it's going to be in a slightly different format than tenu's or Chris's so feel free to give me feedback on that afterwards on Ed or by email whatever you like but the topic today is actually kind of fun um so we're going to start our foray a little bit into deep learning so neural networks um I'm assuming everyone here has heard of neural networks before anyone who hasn't yeah that's what I thought pretty much so it'll be fun to see how the ideas of what's happening in the state of the art actually come back to all the things we've been talking about so far so ideas and linear regression ideas and logistic regression all of this will connect to how neural networks work so before I get into you know the mathie notationing type of material I wanted to start with some motivation uh who here has heard of gpt3 yeah a lot of you so it made big waves a couple years ago is a huge huge model but it was able to do really impressive things like in this case here it came up with a poem on its own right and the hope is that these deep learning models learn to be so expressive that they can do creative things like creative writing more recently this was very very recent um has anyone heard of Dali 2 a few of you yeah so this is very very recent and so dally 2 is able to generate images and the prompt here is an astronaut riding a horse as a pencil drawing and you can see the the main one that I picked out and then a bunch of different versions of what else it can come up with so deep learning is very very powerful for better or worse but it also has so much potential to do such amazing things right obviously this is cool but there's also applications in medicine applications and education applications in things like autonomous driving which could hopefully make our roads safer so there's lots of potential here and that's sort of the backdrop to where we're going to start we're going to talk about the origin story of all these things so we're going to start with supervised learning learning with non-linear models so so far most of what we talked about has been linear models and now we're getting into the non-linear territory and I'll talk a little bit about what that means and then we'll get started with neural networks we'll figure out what the heck they are how we can Define them and that next lecture we'll actually talk about how you can optimize them and I believe Tangy will be giving that lecture any questions before I get started cool all right so first let's think about linear regression so we've seen this a bunch and so we can get started with a data set that we might have right so we might have some excise and Y eyes so some inputs and outputs in our data sets and say our data set is of size n right and uh we know for linear regression that we have a prediction that we can make according to something like this right so we have a linear function that we're using to predict and this function depends on our inputs X can anyone help me out with what our cost function might look like in terms of y's and H's for linear regression okay H minus y okay I heard somewhere some over I yeah this looks good to me this look good to everyone familiar hopefully we've seen this all before awesome okay so we have a prediction here and we have our label and what we can do is we can also write this directly in terms of our parameters so our parameters here are Theta and B cool I just plugged it in makes sense so far right and what we can do is run things like gradient descent or stochastic gradient descent in order to optimize this cool so last lecture you talked about a slightly different set of models you talked about kernel models and so with kernel models we still have a similar setup right so we have our xi's our y eyes right and then what is our H Theta of X look like so what does our prediction look like with kernel models does anyone remember from last class hint it's very similar to what we had before yeah and what's 5 of x sorry the feature map exactly so what's interesting about this setup is we're still linear in parameters right but we're non-linear in the inputs because Phi of X can be any non-linear function that you discussed last time so what if we want to be non-linear in both the parameters and in the inputs so generally speaking what if we want to do something like this say our H Theta of X is anything non-linear so let's say Theta 1 cubed x 2 plus maybe square root in there Theta 5 x 4 and maybe square root the whole thing right so this type of model could be a lot more expressive potentially right but we also want to think about how can we make this computationally tractable how can we make this useful okay so we have some non-linear model and by the way all these notes will be up online afterwards so if you don't finish writing something please don't worry about it it will be up and if you want to follow along I should have mentioned this earlier but there is a template up as well um so it's going to be what I'm writing on so the blank version cool so let's go back to our um our non-linear model now we can assume that are excise are in Rd so some Vector of features or inputs and our y i is some scalar so just an R right this is pretty standard we've been looking at this type of formulation a whole bunch and then our H Theta is a mapping from r d which is our inputs to R which is the dimensionality of our outputs so the cost function we're going to think about here for our now non-linear model it's going to look very familiar so for one example I we're going to have j i of theta is equal to the square of the difference between the class label and the prediction so y i minus H Theta of x i and all of that squared so that's the cost for one example if we want to get the cost for the whole entire data set we're going to average so what this is going to look like is we're going to have JF Theta 1 over n n is the size of our data set from I equals 1 to n and then here we have j i of theta so this is for entire data set okay so this constant is a little different from before this is a common convention in deep learning It's usually the this is called the mean squared error so it's usually the average the constant really doesn't matter your Optimizer is going to be the same regardless of what constant you're going to have out front of that sum does that make sense quo all right so now we're going to talk about how do we oh yeah question why is it there foreign so n is the size of your data set so you're just this is the mean squared error so you're averaging over all the squared errors in your data set so the reason the constant doesn't matter is when you take the gradient your X that's going to result in your minimum is going to be the same regardless of whether it's 1 over n or one over two you asked why do we use it at all sometimes it's helpful for scaling but other than that there's no real magic behind this it's just a convenient thing to do makes sense to average over your errors um yeah it's a nice way to scale things thank you no problem cool so what we want to do once we have this cost function is we want to minimize it right one way to do that we can use gradient descent and so this notation here we're assigning this kind of like coding notation you can think of we're assigning the right side to the left side okay can anyone tell me why this is gradient descent what is written here and not like stochastic gradient descent what makes it gradient descent yeah exactly so you're considering the whole data set here and the reason for that is if we write out what's actually going on here our J of theta is so that 1 over n is here and we have I equals 1 2N over j i uh sorry j i that's Theta right so we're reasoning over the whole data set so each update here considers the entire data set that we have cool all right so here's stochastic gradient descent so the idea here is we're considering now only one single example every time we update Theta right so we have some hyper parameter Alpha um Alpha same as for gradient descent some number greater than zero um and uh we're basically initializing our parameters Theta randomly at the start and then we go through some number of iterations we sample uh some example from our data set and we do this with replacement and we continue on until either we converge to something we're happy with or we reach our and it are maximum number of iterations so this is with replacement um I'm gonna briefly sketch out what SGD usually looks like more commonly in deep learning settings just so you guys have an idea so this is one variant here is going to be another one and so the variant that's in algorithm one that's going to be in your notes this other one is not but I just think it's helpful to know some of the terminology when you go into the foreign deep learning as well okay so we go through a for Loop where um let's say we're indexing at k we have one tip and Epoch so Epoch is basically a term to mean you've gone through your entire data set and so in deep learning you'll often see or if you read deep learning papers you'll see oh we trained for blonde number of epochs that's what that means that's how many times the optimization is basically gone through the data set so for k equals one to an Epoch we can Shuffle the data and then for J from one to n eater here we might not have enough time or desire to go through the entire data set so maybe we decide that we want to go through 500 examples out of the data set and call that an epoch that's also fine so we have 4J equals one to n itter we're doing the same type of update and here we have no replacement in this inner for loop with the J index because we don't want to look at the same example twice in the data set does this make sense yeah pretty much pretty much and slightly different terminology cool [Applause] yeah so you're basically like having this number by any form you mean like n times for like 40 itself like if the leadership like and Banks and this other one the latter so the question is what does an Epoch mean and uh basically it's if you go through the entire data set it's how many times you go through the entire data set any other questions here cool I'll talk about the last version of gradient descent for today and this is mini batch gradient descent so with mini batch gradient descent the main difference is you're considering b or a batch number of gradients at a time the reason we want to do this is with things like gpus and parallelism this actually speeds up computation so uh we can compute B gradients at the same time or simultaneously as opposed to doing them sequentially and that can be quite a bit faster so let me write out some of that so we're Computing B gradients and they look like maybe grad J J1 F Theta all the way two grad JB of theta and we do this simultaneously cool so one question you might be thinking about how do you choose B and very often you choose B empirically so you test things out you look at your validation set things like that and you'll talk about evaluation a bit later as well but one way to choose B is choose the maximum B that your GPU memory can handle and that's sort of a good way to speed up what you're doing but the trade-off is usually and this might come up in your homework as well um usually the lower the B the better the performance of the algorithm and I'm not going to talk too much about why this is the case this is also active research but just to give you some idea of how folks go about choosing these numbers any questions about mini batch gradient descent yeah I guess this is exactly the same as small American Pacific so that you're just doing physical I think a lot of badge as a whole stool yes yes precisely any other questions yep [Music] yeah so with replacement means uh say we picked example two we can pick example two again in the future whereas with out replacement in the corner case it means if we picked example two within that J for Loop we will not pick example two again until we get to the next epoch you're just randomly choosing one example it can happen to be the same example cannot does that make sense in the middle one you randomly pick examples until you reach and it Earth whereas in the left one you're more trying to go through your entire data set and then do that again any other questions on anything gradient descent related yeah we actually do not Pierce nowhere with voice replacement so with the mini batch no because uh you're considering say say your batch size is 10 or 64 or whatever you don't want multiple examples in that to be the same you want all the examples to be different between batches um you can you can yeah um sometimes you don't sometimes you do um it's sort of like a design choice but you can in this case when it's saying without replacement it means that within one batch you don't have doubles no worries other questions okay all right so next we're actually going to Define some neural networks so we talked a little bit about how we optimize these things but we didn't really get into for example well how do we Define the neural network yet or how do we compute the gradients right like when we talk about multiple gradients how do we actually compute these things and we need to be able to do this in a way that's computationally efficient and progress in machine learning really took off in the last you know 10 20 years because of advancements in Hardware because we're able to paralyze on gpus and make these things so much faster and also algorithmic developments as well of course cool so uh we're going to talk about neural networks today and how we Define them the back propagation which is how do we compute these gradients that will be covered next lecture okay so this example came up at the very start I think maybe first lecture or something like that but the example here we're looking to predict housing prices right and we looked at how we can do this with linear regression with linear models and say with our linear model given the size of a house we have some data where we have some you know size and prices and we can plot them on this plot and our model maybe looks something like this with something like linear regression okay what's a problem with this why might this not work so well any thoughts yeah that's a good first one so prediction might have a non-linear relationship with the input right and we only have a linear so maybe that model doesn't capture the relationship that well what's another reason this is not great yes exactly the second issue with this is that prices can't be negative and when we have a linear model like we do they can right nothing's preventing them from being negative can anyone think of like the simplest thing you can do to fix this problem yeah yes you can fix the intercept to be zero so what does that mean for negative numbers so fixing the intercept to be zero would be this right yeah which also doesn't make sense so one thing we can do let me write out these four issues first but I will show you a solution in just a second does anyone know a function that can fix this for us so I say that later okay any other ones yeah Riley's has anyone heard of Rollies one person with you okay so we're gonna talk about railings in just a second okay so the issues um that we talked about here are the prediction might have a non-linear relationship with the input and the second issue is we can have negative braces okay so are what we want are relu to look like or what we want this function to look like is something like this so basically for all things that are in the negative side we map to zero so that's what our value is going to be and what that looks like in math terms we have our regular prediction but we want the maximum between what you know linear regression would output as a prediction and zero and then our parameters here are going to be W and B so this is value this is how it's usually written and this is the notation that we will use so you can say that value is a function of T here and then really we can just write our prediction as this value function which is WX plus b and uh does anyone know what relu is what it's called in deep learning terms yes yes any other so like what category of functions is it in deep learning yeah exactly activation function so our value is an activation function can anyone tell me is value linear okay raise hands if you think it's linear okay non-linear yeah yeah it is definitely non-linear the maximum crates or non-linearity there okay so this is our non-linearity in deep learning or often is it's also sometimes called one neuron okay so going back to our housing price prediction setup yeah questions yes activation functions are by definition non-linear we'll talk at the end a little bit about what happens if they are linear other questions cool all right so let's set up our high dimensional input example right so far we've uh especially in this plot we've been looking at one input one output so scalar to scalar so what if we have more features so high dimensional input and this is the case when we have X B in Rd and Y B still scalar so we're still predicting housing prices for example so our new terminology for our prediction is value of wtx plus b okay and so our X is just going to be stacked features or inputs so we have X1 all the way to XD and this is in Rd and then our weights our weight vector W is going to be in what dimension can anyone tell me based off of how I've written it D that's right because we're making a DOT product with x and then our B is called our bias and it is going to be scalar what we want to do in deep learning is we want to Stack these neurons so output of activation functions is going to be the input to the next one and this is what creates that expressivity of deep learning models basically they have a bunch of non-linearities that are stacked one on top of the other and this becomes a super flexible framework that can represent a lot of different domains okay so now let's make the housing price prediction problem a little bit more concrete so let's say our X is going to be in R4 and say besides PSI size size or a square footage which is X1 we're also going to consider things like number of bedrooms in the house um what's another thing maybe we can consider the ZIP code that the house is in maybe it's close to a Subway or something right and the last thing we'll consider is maybe something like wealth of the neighborhood so these are our features or inputs and uh what we might want to do is compute some intermediate variables so what I mean here is maybe there are some things that combine some of these ideas um and help us make a prediction for what the price of the house might be so one example is maybe the maximum family size that a particular house can accommodate so what would the maximum family size potentially depend on out of the four inputs that we have size and number of bedrooms yep I would agree with that so we can also think of other variables like maybe how walkable the neighborhood is and that might depend on the zip code for example and the last one is maybe School quality in the neighborhood that this house is in and this will be our A3 so A1 through A3 are some intermediate variables that we think might be helpful to make predictions about housing prices okay well so how could we maybe write these out in terms of math notation well let's use our values that we just found out about and do value of some linear combination of the features that we think might make sense in this context so maximum family size maybe we have um combo like someone said there of the size which is X1 and then maybe we add the number of bedrooms which is X2 and we have some bias term which is going to be theta 3 here so Theta 1 Theta 2 theta 3 are all parameters and then we can do the same thing for the rest of these intermediate variables so we can say Theta 4 so walkability depends on X3 which is zip code and then we have another bias term and then finally A3 so we have value of theta 6 um so A3 is walkability maybe that depends on um or sorry A3 is school quality so that depends maybe on zip code which is X3 and maybe on wealth of the neighborhood which is X4 and then we have some bias here as well okay so these are the intermediate variables that we think might be helpful here okay and the last thing that we might want to do here is is um let me sorry let me just change this notation so it's not confusing for the notes later these are all W's and the biases are going to be B's thank you these are b2's a few ones make sure oh sorry okay no I actually had it right the first time ignore me so these are these were all Theta parameters at the end of the day it doesn't matter I just want to make sure that it matches your notes so it's not confusing later so one two three Theta four Theta 5. okay so this is actually one layer that we defined here okay and finally once we have these intermediate variables we're actually going to make the output we're going to construct the output right and our output is our H Theta of X which is going to be if we follow this construction we're going to write value of and here we're going to make combinations a linear combination of these intermediate variables a right so we're going to have Theta 9 A1 Plus Theta 10 A2 plus Theta 11 A3 plus Theta 12. okay one thing so this is going to be our end goal or end prediction here one thing that usually happens in deep learning is we actually for the output we don't use a relu so it's sort of like convention nothing is necessarily stopping you from doing that is just by convention usually we just have a linear layer at the end cool so now that we look at this diagram that's here on the on your right so we have all the things that I talked about right we have the size the number of bedrooms the ZIP code the wealth and it's going into these intermediate variables right so this is A1 A2 A3 okay and the weights that we're considering that are relating um sort of taking from the first set of inputs X to a so for example here we have Theta 1 here we're going to have Theta 2 and so on this makes sense and this structure that we yeah sorry questions uh right now they're all scalar everything we're talking about right now is scalar so whenever we have the subscript it's usually scalar yep um towards the end you're just using those intermediate videos it's possible that some of the original variable files still be useful so is there any way you can kind of transport them yeah I mean if you look at a two for example if you set the um say this is X3 is positive and you set Theta four to one and Theta 5 to 0 then you transfer over all the information from three pretty cool okay but this structure we came up with the structure based off of our knowledge right this was not something that was determined by some algorithm we just came up with it because it seemed to make sense so this is called prior knowledge and infusing your model with it basically but what if we want to be a little more General what if we don't want or don't have the prior knowledge to do this in a way that results in good performance so this is getting into something called fully connected neural network and what this means is we no longer think about these ideas of family size walkability School quality we don't know what those intermediate variables might be but maybe they depend on all of the inputs so each intermediate variable will depend on every single input so this is going to get messy but I'll try to use different colors so every single variable here will depend on all the ones that came previously and this is a much more General way of thinking about this right we don't have to infuse this prior knowledge into the system we can let the neural network figure it out so what this looks like mathematically is maybe we would have like A1 uh b equal value of you know some weight X1 plus some weight X2 plus some weight X3 plus another weight X4 and plus the bias term and we can do this for A2 and so on and so forth does that make sense cool so what this looks like if we start looking at Vector notation now more this might look like this so we have A1 is equal to their value of of W1 this higher index notation in square brackets is going to refer to layers so this would be the first layer and this would be the second layer in this network so we have W1 layer one transpose with all the inputs X now this is a vector so now we're doing a DOT product and we add some bias term okay can anyone tell me what dimensionality W uh W1 in the first layer is yeah yes that's right and that's because our X is dimensionality four and our bias is still a scalar okay and then we can do the same thing we can say this still first layer okay and last one same thing okay and then finally we have our prediction and the prediction now is going to use weights from the second layer and it's going to operate on those intermediate variables a okay so we're going to have W2 and here we have B2 and now W2 is going to be of what dimension yeah I heard it somewhere yes because a is of Dimension 3 here and the bias still scalar okay so this is a two layer neural network and this is the same thing as saying we have one hidden layer so intermediate variables are referred to in deep learning as hidden units and the associated layers are hidden layers okay any questions on that yeah uh yes great question great question um so you're going to be limited by compute but aside from that it's a lot of experimentation so for example gpt3 has a lot a lot of layers but it's also dealing with a lot of data if you have just a little bit of data and just a few examples you probably don't want to use a very big model and I think you'll talk a little bit about why shortly any other questions yeah different hidden units if they're all relying on the same input so the hope is that the network learns different representations but technically nothing is really stopping it from exactly what you said just replicating ideas but it often doesn't um and it's trying to learn these A's in the best way that can help the neural network make a prediction H Theta of X right so this ends up working actually quite well yeah for example um [Music] [Music] yeah so that's actually an active research area as well it's called interpretability in deep learning and either figuring out if there is any or figuring out how we can induce there to be some level of prior knowledge so some interpretability I think there's a question at the back yep yes that you're going to be applying to what yes so w are the weights that you're going to be applying to the x's and then W so those are W1 with the square bracket at the top and then W2 with a square bracket at the top is going to be the weights you're applying to the A's yep all right okay so this is some notation yes yes question is just linear and so what's the point of the neural network versus just a writing um yeah so that's a great question you're completely right if you get rid of the values things are just going to be linear their values are what make uh the neural network be more expressive so those non-linearities those activation functions are really the heart of the neural network okay cool I'm going to move on in the interest of time so we have a two-layer neural network and we just talked about uh all these things the only difference here is that we're changing notation a little bit so we're introducing the Z which is just a linear combination of x's with our weights and our biases and then we still have our A's the number of A's here is M so this is the number of um hidden units that we're considering so we had three in the last example more generally we'll have m um and then yeah that's otherwise this is exactly what we just wrote out before this make sense to everyone any questions on this okay so we're going to talk about vectorization we're going to make this even more vectorized right so we had like a lot of notation a lot of indices here we want to get rid of as much of that as possible one to make things cleaner and not have to write all these indices everywhere and two which is the more important reason vectorization actually makes things faster it makes things uh better able to be paralyzed on for example gpus so what we can do first is think about those weights in our first layer right those W's with the square square bracket one and what we can do is we can transpose them so they're rows and stack them in a matrix and so what we're looking here at here is M by D so m is our hidden unit dimension and D is our input dimension so these are all the weights in our first layer and then what we can do is we can actually write out those equations from before in this vectorized form where for example we can write out Z1 if we take the first row of this Z1 will be W1 of 1 transpose X1 plus B1 1 right which is that first row that we had before okay and but this looks way way nicer and this is our vectorized notation any questions on this so it's the exact same thing just more compactly written cool [Applause] foreign this is actually called preactivation so this is before we applied our values and we have these Z's this is couple capital W I'll do it with those little bars at the top um and it's all our weights from our first layer we take the X and we add our biases which are also stacked together and all of this is going to be of Dimension M which is the same Dimension as our hidden units now we want to get A's out of this and to do so we need to apply the relu to every element in the Z vector so our A's are A1 through a m and we want to apply relu to each one of these Z's we're actually going to abuse notation here a little bit and we're just going to write this by definition to be value of Z so it's an element-wise operation and we're going to refer to it the same way as we would for a scalar okay and then what we can do is we can write our second layer weights in the second layer we only have a scalar output that we want so we only have one weight Vector to include here that we're going to transpose to be a row and this is going to be in dimension 1 by m any questions on that part okay and we're still going to have our bias in the second layer which is still scalar and so our final output here is going to be this W2 Matrix times a uh dot productive with a and the bias term and what I said before is that vectorization helps us paralyze things on gpus okay so we talked about two-layer neural networks what if we have more layers so notation just follows right so before we stopped with one hidden layer which is a two layer neural network and now we're going to have R minus 1 hidden layers which is an R layered neural network and like I mentioned before so all the hidden units will have values whereas the hidden layers whereas the prediction just by convention will not and we'll refer to these big W's as weight matrices and these bias terms will just be called bias nothing really changed and these A's will be hidden units one thing to note is the dimensionality of these A's right so what is the dimensionality of a k so a layer k and that's going to be we're going to refer to that as MK okay and so can anyone tell me what the dimensionality is of w layer one so I say that louder D cross k what do folks think so it's K cross d is that looking yeah yes yes there we go okay so it's M1 cross D so D is our input so we want this uh Matrix to do matrix multiplication with x x is of Dimension D so we want that last Dimension to be D the First Dimension is the dimension we want as the output the output will be A1 A1 is of Dimension M1 does that make sense okay and then so just for practice what would W2 be yeah [Music] this would be an R layer network with r minus one hidden units what is the dimensionality of W2 can anyone help me M2 cross M1 because the input will be a one that's what's being multiplied with W2 and the output will be A2 so we want M2 output okay cool and so this can more generally we can write this for WK we're going to have our um MK cross MK minus 1. and b k is just going to be um MK dimension any questions here no okay awesome so we got this question earlier why do we need an activation function relu can anyone remind me why do we need it yeah so it's our non-linearity it's what makes what we're doing here non-linear but for fun let's see what happens if we don't have the relu at all or we have it be just identity this is a one sorry okay so we have our hidden layer A1 and then say we only have one hidden layer so our output here is going to be W2 of a plus B2 okay and then if we actually substitute in for A1 what are we going to get so we're going to have W2 of W1 X plus b okay all of that a oh sorry that was a plus B2 right and then we can actually expand it out and we're going to get watch my math so I don't mess this up I'm gonna get W2 w1x Plus W2 B1 plus B2 so what does this really look like well we can Define these to be say this is W tilde and say this is B tilde right the the second term doesn't depend on X at all and the first term does okay so this is just a linear function of X as we would have in linear regression essentially everything would collapse and we're here linear in these parameters any questions about this so if we lose the value we lose the non-linear expressivity of the neural network yep yeah you can definitely have other ones um so in the notes I think we mentioned the sigmoid activation function and the tan H activation function I would probably say value is the most common but it really depends on your application the kinds of outputs you might want things like that um there are definitely other ones and there's also b-sides uh just you know the kinds of outputs that you might want these activation functions have certain properties when you um try to compute gradients with them and things like that and that's a little bit beyond the scope of this lecture but if you have questions about this later come come by and I'll be happy to chat any other questions on this yep [Music] perhaps the closest drink yes why are you still landed so commonly used despite it being quite close to me it works well oh yeah good good good uh Point yes you're it you're right thank you I meant it in the sense that the parameters are going to be linear here yep other questions okay okay one last new ish idea I want to talk about and that is connection between neural networks and kernel methods so kernel methods we talked about this very briefly at the beginning of this lecture so the kernel method output or prediction is going to look like H Theta of x Theta transpose Phi of x so we're linear in parameters here but not an X yeah everyone agrees okay so looking at that penultimate layer AR minus one what if we write it as Phi B of x where uh sorry 5 beta where our beta is going to be all our parameters so our parameter is W1 um all the way to the penultimate layer w r minus 1. and also our biases which are 1 all the way to VR minus one okay and then we can write the prediction from our neural network as W of R so our last layer Matrix multiplied with Phi Beta of x if we fix our parameters so we're fixing beta here and we can add our biasterm okay so really this looks pretty much the same right the only difference is within the neural network Phi Beta is actually learned so the algorithm is looking for the best possible features for this data whereas in the kernel methods we choose the kernel so there's more of that prior knowledge and prior structure that we're infusing into the algorithm whereas here there's flexibility in that these five beta parameters can actually be learned to best fit the data and because of this sort of structural similarity the penultimate layer output a r minus 1 is sometimes called the features or the representation within a neural network and so if you ever hear terms like representation learning or something like that it's talking about those hidden layers within the neural network and what we're learning there any questions here yeah at the back uh fire e beta um that's all supposed to be one line right yes so the the whatever that Greek letter is called is the only subscript uh so it's WR Matrix multiplied with five underscore beta of X plus all of that plus the biastern BR part of the is not like a subscript at all no okay thank you yeah so other questions no all right um so today we talked about two different things we talked about supervised learning with non-linear models um and what that might look like what the cost function looks like and the second thing we talked about are neural networks how do we construct them we first started with two layer neural networks and then expanded that notation to be our layer neural networks and next time tangyu is going to talk about back propagation so how do we actually optimize how do we perform stochastic grade in descent or any kind of gradient descent in this framework so how do we compute the gradients for the cost function and for the neural network any last questions okay"
Stanford CS229 Machine Learning I Neural Networks 2 (backprop) I 2022 I Lecture 9,"Stanford CS229 Machine Learning I Neural Networks 2 (backprop) I 2022 I Lecture 9

so I guess the last time um Masha talked about um on deep learning uh the introduced deep learning new networks and today we are going to talk about back propagation which is probably the most important thing in deep learning like how do you complete a gradient and implement this algorithm of course there are many other kind of like decisions you have to make um in deep learning and also they are um like a like this kind of like back propagation this Computing the gradient becomes kind of standardized these days like you that don't necessarily have to implement your own like a greeting computation process where you can just order the gradient is done by the so-called Auto differentiation algorithm but this is actually the only algorithmic part in deep learning so that's why we still teach it and also in some sense it's still important because um this idea of computing gradient automatically you know actually has many implications in other areas for example suppose you want to study the so-called Knight learning I'm not sure why you heard of this word like which is basically built upon this idea that you can do auto differentiation for almost any computation you want and and this kind of same ideas also shows up in other cases and also I know that um some of the previous courses right also covered uh the back propagation I think cs221 does cover back propagation in some sense so what I'm going to do is that um so in in the past and you know like in the last four years when I teach this you know I have a lot of derivations you know with all of these indices and you compute the gradient of you do the change in very detailed way and this year I'm going to try a slightly different approach where I'm going to make this a little bit of more packaged in some sense like kind of divide into kind of like a sub modules um in some sense it's a little more abstract in some other senses you know it's it's cleaner because you don't have to deal with all the small indices like like all the like sometimes there are five minutes is like you have to keep track and now everything is kind of in Vector form so and and this is not like entirely new it's not like I'm just doing experiment because in the last time I did both of the two versions right I have the the very fine grain kind of like derivations I also have the vectorized form um and I think I got some feedback from you that the the fun good derivation is not that useful because either you know you you feel like it's kind of messy and I'm doing all of these computations like on the board and you kind of know how to do it it's very messy or if you haven't seen it before it's kind of hard to follow right so so if you haven't seen all of them like if you haven't seen kind of bad propagation at all like even for a very very simple case um and you want to do the the very kind of like a low level stuff I'll probably take a look at lecture notes afterwards you know like a look it's kind of like independent what that teach here doesn't depend on anything there doesn't require any background but but it's it's more on the a little more abstract level to some extent like I would say it's more like on the vectorized level so everything is true as vectors so if you want to do the more low level details you know you can take a look at the lecture notes which which I think that those part is a lot easier and and mostly covered by some of the other courses as well so that's why I'm making this decision you know don't don't worry if you don't see anything like if this my comments doesn't make sense here just you know if you haven't seen any bad propagation so this is this this I'm I hope at least uh this is still like a very like the materials I'm going to cover today still is completely fine if you don't have you haven't seen anything about um bad propagation um okay so um okay so let's get into the uh more details so I guess um so basically the so-called back propagation this is just a a word that describes um like it's a terminology in some sense back problem that um is a kind of a technique for us to compute a gradient of of the loss function for Network so recall that last time I think we talked about this you know um um like you have a loss function for example you have a loss function J of theta which is maybe let's just look at one example J subscript this is the Lost function of JS example and um and this is something like y j minus H Theta x j square right so this loss function can be other loss function here I'm just only using the square loss for Simplicity and this is a new network and recall that you know we talk about ICD stochastic winning descent in when you do deal with new artworks you always have to compute this gradient so the the algorithm is something like this right minus Alpha times J um the gradient of the loss function on some particular example J K maybe a random example um at the parliament Theta so basically today what we're going to do is that so basically today how to compute this a gradient so this is the the main thing today and once you know how to compute these gradient you can implement this algorithm and there's a generic way to compute a gradient so what I'm going to do is I'm going to start with a with a very generic theorem and the theorem I'm not going to prove it you know it's you don't have to know how to prove it but but I think it's kind of good to know the existence of this here so the theorem is saying that I'm going to write down a statement but a theorem is basically saying that if this loss function J by the way I'm going to focus only one particular example today right because if you know how to compute the gradient for one example then you know how to configure green for all the examples they're all the same so so the point is so and the theorem I'm going to write down today is that if you know how to compute the loss itself like in your efficient way then in almost all situations you know how to compute the gradient of the loss and in a almost same amount of time it's kind of like a very in some sense you know if you heard of it the first time it's kind of striking um so let me let me write down the theorem so I need to First specify some of the Minor Details which don't matter that much um so anyway this theorem is going to be stated somewhat informally so so let me Define this so-called notion of um differentiable um circuits or these differentiable Networks so and um and let's say this a differential circuits or differentiable networks are compositions of sequence of let's say um arithmetic and arithmetic spelling it correctly sorry operations and Elementary functions by the way my definition here is kind of like a little bit hungry as you can see but the point is you know you will see the point the point is not exactly about the the details it's more about the the form of the theorem so by arithmetic operations Elementary functions I kind of mean that for example things like you know um maybe addition you know subtraction you know product you know Division and and some of the elementary functions you can handle you know actually most of the elementary functions you can handle so maybe cosine sine so and so forth right exponential log logarithmic maybe relu maybe sigmoid you know there are many many of these functions right so and suppose you have this is my definition of differential circuits or differential networks or differential networks and you can see a new network is often one of this right because it will have a new network we have a lot of Matrix specifications and it's really just no matter how many Matrix modification you have it's really just some complex conversations of all of this you know like operations right so it's in some sense everything you compute you know are some combinations of these things right no matter is whether a new network or something else okay but but I'm going to insist that they are differentiable because you know I'm going to have have to I'm going to differentiate on the output of this network so here is my theorem um so [Music] maybe I should be I think the blue color is a little better right is that right okay so um so and this is informally stated no it's not very far from the formal version like it's just a formal version requires some um some minor details about the differentiability so and so forth so like the claim is that suppose you have a differential circuit [Music] of say size and and means how many kind of like basic operations you are using so this is my size the size means the number of basic operations and suppose this circuit computes computers a real valued function so I'm going to stress this this is a real value function of that Maps let's say l dimension to one dimension um I'm going to stress that this theorem only works when you have one one output so like the function currently have one output but it can have multiple inputs so then suppose you have such a circuit then the gradient of this function that you are computing the gradient so what is the gradient the gradient at an important is a vector because you have IO odd inputs where the gradient is L dimensional right so so the gradient is an L dimensional vector the gradient at this particular Point let's say x x is just the abstract point and this can be computed in time time off and um I think I guess technically this is O of M plus d but I guess um um I'm not sure why I'm on my nose this is typo I think oh I see so okay I guess time off and by our circuit of size often and here I have a implicit assumption because you know implicitly assuming like is bigger than D because if n is not less is less than D it's a little bit sorry a is bigger than l because you know if you have a circuit the circuit if it you know most of the circuits should read like all the inputs right so so you probably need at least an hour time to read all the inputs so that's why I'm assuming n is bigger than L if n is not bigger than L then this has to be slightly change changed but the the message doesn't change so okay anyway what's the main message the main message is that if you can compute a function f by a circle of size n then its gradient can also be computed in a similar amount of time and and you just pay a constant Factor more and I think this constant is literally in most cases this constant is just one uh of course when you really talk about the absolute constant it has to depend on how some of the small details like how to really implement this you know in the computer so so that's why I'm hiding a constant but in some sense you can feel you can view these Concepts even one um so um okay I guess okay sorry um I think whether the concept smart I think also depends on how do you count right so whether you assume you know you are the computer function f so maybe okay so so basically the constant would be two if you have to compute F and The Greener five right so so basically there's a so if you know the function f already then this constant would be one if you don't know the function f then you have to first evaluate F and then do the gradient and then this constant will be two I'll I'll discuss this a little bit uh later as well but anyway for the moment you can just think of this you know you almost have the same amount of time you can only use the same amount amount of time to complete the gradient so gradient is never much difficult more difficult than Computing the loss itself so and and this is very general because it doesn't have to be a new network right it could be any almost anything like this [Applause] so and if you want to you know instantiate the serum you know for um for new artworks of the lots of networks then F you know in this theorem what correspond to the loss function on a particular example J J's example and and Theta is the the variable acts here right so and the gradient of f corresponds to the gradient of the loss here right so and and what is the what is L right so L is the number of inputs to this function f so here what is the input what's this variable in a car about the variable care about is the parameter Theta so L is going to be equals to corresponds to the number of parameters and an n so what is the the time to compute um this loss function the time to complete a loss function is kind of like the same as the number of parameters so if you think about you know you have a new network of you know a million parameters and how much time you have to use to compute the final loss function where you basically have to give your input to your network and go through the entire network and do all of these operations and basically eventually the amount of time you have to spend is also similar to the number of parameters for evaluating the loss but only is the time to evaluate the loss so that means that the time for the gradient so Computing gradient also takes according to the theorem this also takes off on time so basically you only have to take all of number of parameters time to configure it any questions so far like how is the dimension of the input already so um they sound pretty different possibly so here my does that depends on how you map the apply the theorem to to this setting so here I'm going to view this as a function of the parameters so the parameter is my input to this function I'm going to differentiate with back to the inputs right so so the the theorem generation is a generic theorem right so like there's some function and you are differentiate with respect the input of the function right it depends on how you use this theorem so if you use this theorem you say the Theta corresponds to the X there right so then what is L I always dimension of the the X right there so so L is dimension of the Theta here any other questions okay so I guess the the plan for the next rest of the lecture is that we're going to show you how this works for Newark works I'm not going to show you for the how this works for our general circuits um it's actually not much more different different if you do it for the general circuits but it's just like the whole thing is um you need to have a lot of dragon to really do prove this like with the general circuits because you have to consider how that all the generalities right so so basically I'm going to show the concrete example formula it works how do you complete a gradient um and and when you see the the concrete examples for the right works you cannot see how you do it for the for the for those for General circuits um which I'll probably discuss you know if I have time at the end and um another thing like before I talk about the theorem here um I guess I like to say that this is also this theorem is also the basics for many of the so-called second order method so so basis and also for example for meta learning I guess we haven't introduced meta learning but I think the point that the product is kind of relevant is that um um so like I'm actually I'm not going to talk about what my learning is in this course just because it requires um it requires um uh more kind of like um backgrounds but but the general idea is that you can use this um serum twice so that you can do something about a second order derivative but there is a there is a um there's a specific setting you can do so basically I think let me just give you a sense on what I mean here this is not kind of like a compared comprehensive because sometimes you're going to use these theorem in different ways but this is one way to use this serum in a clever way to get something more than what it offers so um so for example suppose in the same setting foreign [Music] you can claim that for any vector v of Dimension L this is the same as your input dimension of x then this the hyacin the second order derivative which is a matrix this is a i o by L Matrix times this vector v can be computed in of n plus L time and that's I guess I said only is kind of like bigger than l so this is still kind of like an often type so so you can see that this is kind of like you know now it's kind of a more magical right because if you come on to compute this Matrix typical it takes even just to memorize this Matrix right even you just want to kind of compute each entry in suppose each entry of this Matrix takes off one time then you always find L Square time to complete this Matrix so so certainly if you want to do to to get all of n plus L time your algorithm cannot just first compute this Matrix and then to take the Matrix Vector product that will be two inefficient because just Computing this major itself will take L Square time so however um if you compute the whole thing without going through without doing the Matrix first and then take the Matrix Vector product and complete the whole thing all together then there is a way to speed it up and you can do it in all of In Time well and it's bigger than L basically right so basically you just take off and type [Music] right so Newton's method you know depending on which Nutri you're talking about the the vanilla Newton's method requires this Matrix then it's l Square time so we only know how to do Matrix Vector python Vector product so so so that's why I think you know at least there are a bunch of papers in the last few years which tries to implement the original Newton's method without Computing the hesit so they try to use the highest and Vector product to have a different way to implement original Newton's method and and that you know was somewhat successful you know like I think you have you can have some new some algorithms that tries to only use the highest inductor product um to to implement the approximate version of the Newton's method right so and and actually this you know this corollary um is actually pretty easy to prove and and uh and and when you prove it you know you can see how it's actually the proof also tells you how to implement this so so basically you know like uh um um the way to prove it is that you just say I'm going to Define new function G of x this new function is going to be the gradient times V so this is a real valued function because this is a function that Maps all dimensional Vector to a real Vector you know if the output is a real Vector even though you have like there's some it's sorry the author is a real value scalar right even though these are vectors you take the inner part that becomes scalable and then so if you use the theorem so by the theorem you know that g of x can be computed in of and type just because you know the ingredient can be computed so that's why after the ingredients you take the inner product you spend another of an of an actually of n plus L time just to be precise because you know computer gradient Takes Over N time and then Computing the the the the the inner Paradox L time you get all of n plus L Type and then you use the use the theorem again Ong G so before we are using the serum on F and you say the number F can be computed now you're going to use the theoremong G so you view G as the new F right so you you view the f as the what's the right way to say it like like a like so this G will be the F in the theorem right then you verify whether G is satisfy the condition that's true because G can be computed in all of n plus L time and then that means the number of G can also be configured in it can be also be computed uh of n plus L10 right so this means that novela G also computed can be computed in of n plus L time because you can do this twice right and what is the the nubla G like the gradient of G of x if you do the computation what's the gradient of this it's really just you take another gradient here um I guess this probably requires a little bit you know you can verify this you know using turn rules offline but then the gradient of G is really the gradient of the inner product of f x and V and this is actually the Hyacinth of f x times V foreign and and you can do this kind of things like for many other things where you for example if you define any functions of the gradient you can still to take the derivative right for example I think in my learning sometimes you have to have of maybe some special function of the gradient and you have to take another gradient of it and that's that's sometimes you know a some module used in metal learning and then you can apply this again where you can say okay this because the green is computable in often time this function probably just a very simple function so the whole thing can be computable in often time and then you can take another derivative again so so that's that's when you know I'm not going to go into more details but that's how people are using using it in metal learning as well so okay anyway so this part is Advanced material we're not going to test it in any of the exams or homeworks but just uh I feel like this is a good to know uh to some extent um just because this uh this is used in many other more advanced situations any other questions Okay cool so I guess now next I'm what I'm going to do is that I'm going to um discuss a concrete case right in right work I'm going to start with two layer Network and then I'm going to talk about deep new Nets and um and basically I'm going to apply the auto differentiation right the the proof of the theorem is basically you have to give Auto differentiation algorithm or also called back propagation so so um basically apply the back propagation to the specific instance right new Networks Okay so I guess so let me set up some um so before talking about your artworks maybe let me briefly discuss a preliminary this is the chain rule I'm somewhat assuming that this is covered in most of the Calculus class um but I'm gonna have to review this in here partly because I'm going to have potentially different locations from what you you know not exactly the same notations from what you learn from the calculus cost you know just because different calculus book has different notations so um for the purpose of this course what I'm going to do is that suppose I have a suppose J is a function of Theta 1 up to Theta p I'm trying to use as close notations as our final use case even though this kind of like this part is supposed to be abstract right so all the notations are just some symbols right I'm trying to use similar notations just to avoid too many confusions so suppose FJ is a function of a bunch of parameters a bunch of like a variables and then um suppose you have some intermediate variables Suppose there is a um okay so how does this function works this function works by um by I guess maybe what is the this function is defined in this following form so you have some intermediate variables so maybe let's say they are J1 which is a function of that the inputs Theta 1 up to zero p and also you have up to j k which are J some functions of so I guess you know I'm not sure whether this farmers make any sense so I'm kind of using the j i I suppose the function or under the variable of the of the output of a function I think this is pretty kind of like typical in emotional math books right so so you can view J as a variable and then this J is a function of the state of up to Theta p and which function it is you just use J1 to describe that function so um does it make sense like yeah so and then just because I don't really necessarily care about exactly what the function is so I'm just going to give a name and this name you just call the same thing as the variable so and then your J is a function of you know these intermediate variables so and once you have this kind of like a two-layer setup then suppose you care about the derivative the chain rule is saying that if you care about the the partial derivative of the the final output with respect to some input Theta I then how do you compute this derivative you can do the chain rule so what you do is you say you're going to enumerate over all intermediate variables so take the sum over J from 1 to K and for each intermediate variables you first compute the derivative which is back to the intermediate variables and then you multiply this derivative with the derivative of any mini variables with respect to the variables you care about Theta I um and just to clarify the dimension so this is in R and everything is in everything is a scalar so far so okay so this is the the chain rule just uh in in the language of this course any questions okay so now I'm going to talk about a concrete case and uh and maybe let me also Define some notations so um so so maybe one important thing to note is that every time at least in the context of this course every time you have this partial derivative thing like we insist that the this quantity the quantity on the top is a real valued function so so we are not considering anything about multi-evaluate outputs not because you know you cannot consider them it's more because you know typically in machine learning if you are taking derivatives derivatives of a material outputs uh function multivariable function a computational is kind of like a it's a problem so um so so so so so so in some sense I think in the machine learning people try to avoid that like at least at least on the algorithm level of course in analysis probably you have to think about that that notion but on the algorithm level you always like 90 of the time you always try to take derivative of a real valued function you never try to take derivatives of multi outputs multivariable functions and in this course we only have to think about the case where this is a real value function so that's why in the notation so the notation here is that so suppose J is a real value function valued variable or function then um of course it's kind of clear what what this would mean by so if you take this with back to some variable Theta that just means the partial derivative that's easy but what if you have a vector so suppose if a so this is easy this is just a so if Theta is a real value of vector this is just the partial derivative which is easy and what if a is is it the green one is not great maybe I'll use the black one so if a is a some variable in dimension d then uh then this would be also in dimension D that's our notation I'm just clarifying the dimensionality right so and and if so basically this would be just a a collection of scalars you know the partial derivative of J with respect to A1 up to the partial derivative of J with respect to a d which is dimension d and we are also gonna we are also going to get into the situation where a is a matrix maybe a is a matrix of D1 D2 and now what does this notation mean becomes a little bit tricky because sometimes in different literature they are a little bit different um on conventions for this um course what you want to do is just that this has the same shape as a so this is also this is just a um all the partial derivatives and and this is in the same as this has exactly the same shape as a itself so basically in in this course notation the the this kind of derivatives or gradient whatever you call it right so they always have the same shape as the original variable um all right Okay so um I think I need to okay so now let me talk about the two layer Network this is just a review I think Masha has talked about this so um so you have something like so if you the loss function is evalued by the following so if you evaluate the loss function what you have to do you have to first compute the uh the output of the model and then you compare with the label and you come to a loss right so basically the computation to evaluate loss function is the sequence of operations so you first compute the so-called intermediate layers um by doing something like this this is the first layer weights times the input on the data times plus some bias variables and say let's say this is of some Dimension and so and then you apply some anchovies value and you still maintain the same dimension and then you apply another layer maybe let's say another layer that's called o the output of the null layer is called o so you apply W2 a plus B2 and this layer because the output this is the output layer so you want to make this one dimensional so you get R1 um and often sometimes this is called H the Del facts right this is the model output and then you compute a loss the loss is something about half times y minus o squared by the way I'm here only having one example X and Y X is the input Y is the output okay so that's just a quick review of the two layer networks I think I'm quite sure that we are using the same notation as measure um hopefully so um okay so now the question is how do I compute the gradient with Vector W1 W2 B1 B2 bye in general how many of the samples for all of them or like that's another layer right so so that's when you that's about how you implement this algorithm right so if every time you just like take one example you just have to do it for one example if every time you take a batch then you have to do it for a batch but uh for the purpose of this this lecture you know this is a good question but for the first of this lecture uh we only have one example because eventually you just do all the for all even you have like 10 examples do all of them the same way you just repeat the calculation of course you can parallel parallelize all the com the gradients of all the examples but the the method is the same so okay so how do I complete the gradient of of this right like the greeting of the loss with respect to the parameters so so what I'm going to do is that I'm going to um oh no this is uh yeah this is a variable called o uh I think I probably should uh how do I yeah I should change the notes yeah like this is fine in the latex right like but uh how do I write this like so that it's not look like a zero um I think I think I think it's okay to let's see so is there any chance that I can do it on the fly to change the notation I think I should be able to what what you want me to change it to uh Sigma is going to be used it's going to be used for some other things tall okay but but just just this will make a introduce some inconsistency with the notes because in the nose this is all just to let you know okay um but I probably should change that in the notes as well yeah and also like you you see that this one doesn't show up that often hopefully okay so um so what I'm gonna do is that I'm going to um um try to use some chain rule um um with this right so how do you chain rule so um for example you just if you just look at one entry of this Matrix right this is the Matrix you look at one and two you can use the chain rule to derive the the derivative of that entry so that's the typical way that we do it you it's going to be a very complex formula but you can still do it right so you you use the chain rule multiple times and then compute the derivative of that entry and then eventually you get a lot of formulas and then you try to regroup those formulas into a nice form so that's that's actually written in the lecture notes you know if you're interested you can look at them um it's pretty much a sprute force computation um of course if you do that Brute Force computation multiple times then you get eleven like a little bit kind of like expiration you can do it faster in the future so um what I'm going to do is that I'm going to try to keep everything as a vectorized notation and and still I'm going to chain rule but I'm going to do a more kind of like a vectorizing version of the chain rule so so because of that I'm going to have this kind of abstraction so so this is uh in some sense I call this chain rule from Matrix modification I guess there's no unique name for this this is just a um invited by me but it's really a simple fact so so I'm going to um suppose so Z is equals to W Times U plus b where let's say so w is a matrix of Dimension I say I'm by D and U is a vector of Dimension T and B is a you know you don't have to do really necessarily care about the dimensions because you know the at least as long as they match then it's fun so and then I have some function applied on top of Z so so this is my abstraction right so you can see this is a little bit like this right because if you map this Z to this Z and this W1 to this W and X to U and B to B12 uh B1 to B then it's kind of like that it's a kind of abstraction of a part of the problem so um and then I'm going to claim that then the derivative with respect to the W which is what I care about at least if you map it back to here we talk about derivative derivative with respect to w so then this is going to be equals to what is equal equal to a derivative with respect to j z times U transpose and maybe just to kind of like um make sure you are convinced that the notation the dimensions match so this is supposed to be in R to the m by D because the derivative as I said should have the same Dimension as the original variable W so w is in M by D then this is M by D and this is in RN because Z is a m-dimensional vector so that's why this is AR and this is um in our 1 times d right because U is of Dimension d and a new transpose is of Dimension One by D I guess all the vectors are column vectors so if if it's R I know dimensional Vector it's really M by one so all the vectors in this course is column vectors so so that's why this whole thing uh kind of match Dimension right because this is a column Vector this is the row Vector you take the outer product of them you get a matrix actually this is a rank one Matrix that's an actual interesting observation the gradient with respect to the rate Matrix for one example is always a rank for Matrix for one example not for all the examples no not for the the full grid if you just have one example the gradient for the weight Matrix is typically one quad Matrix and if you look at and also we know the gradient with Vector B is going to be equals the gradient with respect to Z so this doesn't solve everything because you still don't know what what this quantity is right this quantity is unknown here and this is going to the same content here is unknown but but at least it solves the local part right it's kind of like chain rule right it says that if you want to derived take the derivative we'll explain to W then you only have to know the derivative derivative with respect to intermediate variable Z and the next next I'm going to talk about how to take the derivative with back to Z so but this is kind of a decomposition right so it says that if you want to know the the derivatives with derivative with respect to W then you only have to know the derivative with vacancy um this is just what the formula tells me um but I'm I'm but here I'm verifying that it does make sense because this is a one by D Matrix one by D vector or one by the Matrix and this is I'm the national Vector of M times 1 dimensional Vector because I view all the vectors as column vectors so that's why I'm by 1 times 1 by D will give you m by D why is the transpose that just because oh I think the fundamental reason is you do the calculation is exactly this um but but it just happens to match you know it has to match if the calculation is correct right right so and also you know maybe that's a reasonable way to memorize it you have to make the dimension match so that's why it's the case so um okay how do you prove this so um proving this you have to go low level you have to do it for every entry so I'm going to show it once and then later I'm going to have more abstractions like this and I'm not going to prove it for you so for this one I'm going to do a quick proof um it's really just a derivation so so what you do is you just use the chain rule so so what you do is you look at the derivative derivative with respect to any entry wij so and how do you do this you say you use the the most basic version of the chain rule you you Loop over all the possible intermediate variables so what are the intermediate variables these are these right sounds like it's very nice to use Z as the intermediate variables so I'm going to Loop over all possible intermediate variables K from 1 to M um and each of the ZR is one of the intermediate variables or z k so DJ over dzk and then d z k over d w i j right and then I'm going to plug in the definition of decay so so then this uh what's the definition of c currency case there's um it's defined by this message specification what is the case dimension of here it's the case dimension of this the case dimension of this is going to be something like the definition of Matrix fabrication wk1 times U1 plus W K2 times U2 up dot until w KD times UD plus this P the b k and you look at the partial derivative of this with respect to w i j so how to do this it's kind of like so first one you know to make this non-zero you have to make sure that this variable is show up in the top right if the variable doesn't even show up on the top there's no partial derivative the partial derivative will be zero so so this is only non-zero only if this wig does show up in the top so when w i j shows shows up on the top only if um K is equal to I right so that because only w k is something show up on the top and w i j so only I I and J are the same are I and K are same then w i j can show up on the top so so that's why you only have to care about those cases where K is equal to I so you just uh the entire sum is gone so you only have to care about z i and then so you have maybe like I'll just do it slowly so this is wi1 times E1 plus up to W ID times UD plus b i over d w i j okay so w i j only show up once here on the top linearly so w i j show up here on the top in this term um somewhere in the middle right so there's a middle term which is look like w i j times u j that's the term that's W where w i j shows up and the derivative of this respect w i j is equal to u j so that's why this is equal um this is equal to sorry times u j right so so that's my that's my DJ over dwij and and then I have to group all of this into a matrix form so if you're verified that so basically you group all of these answers into a matrix form it will be like this so you're gonna get all of this DJ over dzi into this D J over D is equal and in this UJ term will be grouped into this U transpose so so I guess I'm using a very simple fact the simple fact is that if something maybe let's say um x i j is equals to AI times BJ then that means Matrix X is equals to A times B transpose that's the that's the simple fact I'm using right so if some if the entry of a matrix is equal to some AI times P J then you can write it as Matrix form where this Matrix X is equal to a times B transpose a is a vector B is a vector any questions foreign this abstraction it's so called vectorized form of the chain rule to a problem here so what I got is that so what's the mapping the mapping is that Z maps to Z and the W one maps to w x map to U right and J maps to J so that's how I use this abstraction so and after I use this objection what I got is that I got W1 DJ over dw1 is equals to um DJ over DZ times U transpose will be X transpose act transpose okay so of course this is not done right because we want to compute those but we we kind of like we have a reduction in some sense so we we did some partial work right our goal is to conclude this but now we said that you don't have to compute this DJ over DC and next I'm going to show you how to compute DJ over DZ so it's kind of like you are kind of peeling off every a layer by layer in some sense of course you can also do the same thing for B I guess the B is always easier so this will be to see this okay so so next question is how do you do the [Music] okay let's see whether maybe I should use the Newport foreign so so next I'm going to compute this right from right the Z is DC so how do I do this I'm going to have another abstraction so um this is the abstract problem so note that my Z the the relationship between J and between j and z is through this a and W2 right so there's some complicated dependencies between j and z so I'm going to abstract one part of it just to make our um kind of derivation clean so the abstraction would be that you you think of um suppose you have a variable a which is Sigma of C and then J is a function of a so a sigma is this entry wise Sigma is entry wise kind of like a activation function right Sigma is the value basically so I'm going to claim that in this case you can you know that your target the question you care about DJ over DZ is going to be equals to DJ over D A times Sigma Prime Z and this is a so I guess let me explain annotation here so this is our and dimensional vector this is also n-dimensional Vector let's say in this abstract in z a they are all n dimensional vector so then this all M dimensional vector and this is the so-called entry wise entry-wise product right so I'm taking two undivision vectors I take an untrust product and I get the derivative I care about so okay and and then you can see that the only thing you have to carry you have to compute next is what is DJ over d a right because DJ over D is still I know I'm not going to do this proof for this it's actually even easier than the other one you just have to expand and to the channel [Applause] okay so now next let me try to foreign so how do I deal with DJ over da I'm going to again abstractify this part of computation in some abstract form so my attraction is that um so the abstraction is that um I guess I'm going to have a is equals to W Times U plus b and J is equals to a function of a right um so why this is a reasonable abstraction because a maps to this a w Maps this W2 and this EU Maps wait am I doing something sorry my bad I have to use a different okay maybe let's call this talk okay that's a perfect place to call it okay so tall okay so why this is a useful abstraction this is because the my thing in my mind is that tall means the the tall above and on the W here means the W2 above and B here means the B to above and and J means the J so know that the difference here is that this W now means the second layer right so and if you make this mapping right so then what you care about is that you care about DJ over d u here because you or I guess I didn't say what you corresponds to so U corresponds to a right so U corresponds to a because a is what is Multiplied with the Matrix right so DJ over d u so that's what I care about so you can see that even though this abstraction is very similar to this abstraction the difference is that here I care about DJ over d u I care about the derivative with respects the input of the Matrix fabrication and before I carve our derivative with respect to the the matrices The Matrix in the matrix multiplication so right so so that's why it's a little bit different and you're going to have a different formula for it of course because you are taking derivative with back to um you know Y is respect to W and the other is with respect to U so okay so what's the formula for this the formula for this is um if you write it in The Matrix form it's w transpose times DJ over d B D sort of so I guess if you check the dimensionality then this one okay I don't know what the dimension here so I guess let me specify the dimension so CW is maybe something like um I don't know let me come up with the maybe R times and right so an U is of Dimension m and Tau then has to be intervention on so Dimension R sorry so then this is in dimension r and this double transpose is in dimension n by r and that's why they can be multiplied together of course you know okay I guess another thing is that if you don't want to remember all of these equations you want to remember is so first of all you don't have to remember all of them second if you want to remember them and you want to kind of cheat a little bit you can just view the mask scalars and you can see then so for example this one would make a lot of sense if there are scalups right that's just the the tribute to rule and this one makes a lot of sense if they're all scalars because you want to take the derivative U with respect to U then W have to show up as the coefficient so so everything make a lot of sense if it are scalars and the only tricky thing is that if there are matrices then you have to figure out what's the right transpose if you might left multiply right multiplies and so forth Okay so okay so once I have this thing then I can um apply this to the special case above right if you apply it then what you got is that you with this mapping that you got you DJ over da is equals to W2 transpose times uh DJ over dtau right that's because um right just I'm just replacing the note I'm just applying this General thing to this case and now you see that there's only one thing that is missing what is DJ over detail and that's trivial right because DJ over D tall is just this is really just the very last thing j is just the Y minus Tau Square so DJ already told like everyone can compute this is just a I think minus y minus top so so what you really do you know eventually then what you really do eventually is that you first compute this maybe this is step one and then this is step two you compute DJ over da then then where DJ over Dia is used and then DJ over Dia is used to here so then you do this step three and then you get DJ over DZ DJ over DZ then used it um here this is four wait uh maybe four is here then you get the derivative derivative with sector W1 so we really implemented you have to do it backwards you know when you do the derivation you know it's reasonable you know we can do here anyway but um I guess I'm doing it in the in this way from W to from the lower layer to the top layer so from the the first layer to the second layer but when you really do the implementation you have to first complete this and this and one two three four uh I guess I didn't compute the derivative with respect to all the parameters so so I'm going to compute 0 to respect to W and B1 what if you want to compute the derivative with respect to W2 maybe that's a good question to see whether it's somehow digest this what if you want to do the DJ or DWT sure [Music] that the toe with respect to W2 and so and which for example which abstraction you need like I have lost three things right one two and three which might notice yes it's cracked so I guess um the the I think you're gonna use this because W2 is the Matrix right you care about derivative with Vector Matrix then you want to use this so so basically so if you care about this then you view this W2 is the W here so I guess then you need a different mapping so I guess you need to use the first abstraction well first dilemma and and then you're gonna say W2 corresponds to a w and the U now corresponds to What U corresponds to the a here and and B1 B2 corresponds to the B um and and Z corresponds to sorry not Z that so the Tau corresponds to Z so this is in the the right hand side is in the abstraction and the left hand side is in the in the real case I care about so so this means that this is DJ over the tall uh times I I had U transpose here now I had a transpose here so I'm going to have a transpose and I also know DJ over D B2 is going to be equals to DJ over D top makes sense foreign because everything is a vector-wise notation um and now what if you do it for multiple layers you'll see that everything is to stay it's the same so you basically you're just gonna repeatedly use these two landmarks so the three landmarks are three abstractions um I guess maybe I should number them in some way maybe let's call this Lemma one and then here I'm using lemon one and maybe let's call this lemma2 and this is Lemma 3. so so I'm going to for the Deep networks I'm just going to use Lemma one two three repeatedly and I'm going to get the gradient so that's the last thing in this core in this lecture foreign Networks so suppose you have multiple layers of new light works you know um so then I guess using the same notation as last lecture the first layer is this the second layer is some value times D1 sorry this is still the first layer this is the activation and then you do the second layer you have W2 A1 plus P2 dot dot and you get a R minus one layer which is value of C R minus one and then I say you have the auth layer the r which is equals to WR times a r minus 1. plus b r and then finally you have a loss I guess I'll just write the logs here given that the loss will be J will be a half times y minus ZR Square okay so that's my computation of the of the loss function you know a sequence of matrix multiplication and and the activation functions so now I'm going to try to compute the derivatives the the partial gradient so first of all I'm going to compute DJ over d w k for some case later so how do you do that yeah this is a little bit awkward um I guess maybe it's okay to I I know all the you all know this is deep nuts right so better yeah nothing different this is the the same thing as in last time I'm just recording the notations so and I want to compute a derivative with respect to case Matrix so maybe that's a derivative respect to W2 I think right so Source K is two now how do you do it so if I do this with respect to W2 then um the thing is that you want to use the landmark right so the landmark um level one I guess maybe that's the most relevant because number one is trying to take derivative with respects with some w so you just do some pattern matching you want to apply number one so so what's the the abstraction here that I'm talking here is that if you um okay so how do you do the pattern matching so I guess you say that what's the definition of c w k WK is involved in this computation in the following way so WK is involved in the following way c k is equal to WK times a k minus 1 plus b k this is hot WK is involved and then you say I don't care about what happens next I just abstractify the rest so then this is pretty much the same as the setting level one right so then using level one or you can you yeah using this or you know actually you can call it no you don't have to call it Lamar you can call it a formal or something right so number one tells you that if you take the derivative with respect to the Matrix is equals to the derivatives with back to z k z k is basically the output of the matrix multiplication so times the input of the matrix multiplication which is a k minus 1 here transpose okay so that means that I only have to take the derivative with respect to Z K right z k is basically some of these you know intermediate variables right so how to take derivative with back to Z K so you need to think about you know how ZK is involved in this competition right so ZK is involved so I care about this and ZK is involved in the following way I'm not sure so DK is equals to railu um okay sorry CK is involved you know directly in the following way so AK is equals to value of c k and then J the loss is a function of a k right that's the part that z k is directly involved because you first goes to the value you get a and then you you do some computation to get J and then you can use the the so-called MR2 so dilemma 2 will tell you that what is the DJ over DZ okay so it's going to be something about DJ over D AK times the the value Prime Times of c k this is gamma 2. and the third thing is how do you deal with AK well what is the derivative respect to a k so if you don't know derivative is about AK then you have to see again see how AK is involved in this whole computation so how does it how is AK involved so AK is involved because AK if you use a to complete Z again to use a to confuse Z K plus one right so if you look at that part so basically ZK plus one is equals to some w k plus 1 times a k Plus B K plus 1. this is the the time the first time AK is used and the only time AK is used and then you say the rest of the thing is abstracted as a general thing and then you say I'm going to use now I'm going to use the Lemma 3. because lemons 3 is also about Matrix for vacation right number three is also about Matrix modification but it's trying to take the derivative with respect to the input to the matrix multiplication right so a is the input right so um so using lamba 3 I'm going to say I guess maybe I'm not sure whether I should also make a explicit map so if you care about the explosive math then it means that AEK corresponds to the U there in lamba 3. right so w k plus one corresponds to W there and B K plus one corresponds to the B there and J corresponds to J I guess Z K plus 1 corresponds to talk right so I guess if you just Pat if you keep Patty matching I think it's a little bit difficult to see the map the actually the probably the right thing way to the right way to think about it is that you think about the rows of these things right so Z K plus one is the output of the Matrix verification AK is the input to the matrix multiplication and double case the The Matrix in the in the multiplication so so that's how you easily map the rows of them and then um so you get this thing is equals to w k Plus 1. transpose so and then oh sorry I think I have some is this this should be it should be so then you can see that if you look at this two sorry this is pile I must so if you look at this formula and this formula basically this is kind of like a recursion in some sense right so so here you are saying that from DJ Dak you can compute DJ ZK and here from the like a yeah basically you can just recursively use this tool to get all of them so I'll just make it explicit so um so basically with all of this formula you are basically you already complete everything um it's just I'm going to reorganize this to make it a little clearer so what you do is that you say you start from the last layer yeah the last layer here so you'll first compute I guess you know maybe let me just describe the final algorithm so you first compute the so-called forward path so you compute all all the the values of all the variables so in some sense I've already assumed that it's completed implicitly before so basically compute all the um Z1 A1 Z2 A2 so and so forth right so just by Computing all of this network evaluating the loss you get everything right so you get all the variables and then in the so-called backward path you complete the gradient so and the way you do it is that pretty much like same as the two layer Network you start with the the last one so you first can compute this with the last layer and this is Trivial because J depends on ZR just in a very trivial way so this is just the minus y minus ZR right so this is our starting point and now you recursively use both of this to get all of the DZ of the DZ over d a so you already have DJ over DZ oh sorry DJ over dzr and then you can use that to compute uh the previous one using this right so so from this you can get DJ over d a r minus one using this formula because we get this is w if I can do it on the Fly correctly yeah guys this is AR transpose DJ of d CR right so you you remove the index by one and you get to a and then you can use the a to compute the Z the D DZ over d a to come with a DZ already a DJ over DC so so this is number one this is number two number three you you get ZR minus one which is uh equals to um minus one right so so basically I have to do in this iteration you it goes from R to R minus one and then you repeat right so then you can you can repeat you can say I guess maybe I shouldn't number them by so maybe I should number this one by one this piece two 2.1 2.2 or 3.1 um in the in the third round you get a r minus two using this equation maybe let's say this equation let's call it two let's call this one so to see you using two and then you got a z version using Hua and you do this repeatedly so you get everything about a and z I never after you get everything about ANC so so basically after turning all the everything like this it's pretty easy to get the the gradient with respect to with respect to W because you can just say is using this maybe let's call this Ray you can say this is equals to right okay so I guess you can see that I do need a forward pass because in my backward path I do require a bunch of qualities for example it does require I know the quantity z r minus one so I have to save all the easy quantities A and R all of this in my memory and then in the backward Parts I'm going to use them um so and why this is called backward pass in some sense if you think about the computational flow I guess um I guess this is probably my last point that I can make today so this is kind of like um it's the reason why it's called back propagation is because you can view this as you can see that you are the the way that you eat you change the indexes you start from R and R minus one and you do R minus two or mass three so do this in a kind of a backward way so in some sense if you kind of draw a um kind of a computational graph or kind of a flow of the computation I'm not being very formal here but if you draw it in some sense then you can view this whole computation as you start with x and then you have the Matrix modification you need to use the weight W1 and B1 I'll do some matrix multiplication and you get Z1 right and then you do another metrics Buffet maybe you do another a maybe this is as I say you this you got Z1 and you've got the activation say activation and you get you get A1 and then you do a matrix multiplication uh with also W2 B2 and then you get Z2 and you'll do this repeatedly until finally you guys ZR and you've got the the last J okay so this is kind of like the forward pass and and if you think about how how to kind of like conceptually how do you organize the information in this backward algorithm and sometimes what you do is that um I'm just trying to visualize this but you know of course I've everything of all of this will be in computer so in some sense what you do is that um you first compute you first compute a derivative with back to this variable ZR okay and then you say okay how did you get the Z you get a z by some matrix multiplication right so let me say this is by matrix summation location where the input of this is a r minus one right so and um and so then you take this uh in some sense from this you compute the derivative respect to a n minus 1. right that's the that's my step two point 2.1 right and and then you just keep doing this in a backward fashion so you you figure out you know where a come from AMS when I come from Arc minus one from come from Z are minus one so that's why you do a backward pass you say this is completely from ms1 right that's my step two point okay sorry I'm not numbering this correctly so this is 2.2 that's messed upon step 2.2 right so and now keep doing this eventually eventually I got Maybe C1 right so that's that's the so-called backward back propagation is and in the middle you can also compute the derivative with resp to the W so because the derivative with Vector W is equals to something about a derivative with raised to Z so basically once you get this quantity you know you can start from this quantity to get the derivative with back to w uh R minus one I think or my spell r on minus what um and and then I you know every time you get one more DJ over DZ then you can get one more DJ over DW so from this you can get DJ over dw1 so so that's why it's called back propagation and maybe just to say one uh I know we are running out of time just one more word about this so so this is a a kind of very sequential computational graph but but actually if you have like a more complex computational graph right so which is not like as sequential as this you can do almost the same thing basically you just write all this graph and then you figure out you know how to do the back propagation and the way to do the back the general way to do that function is exactly the same you just run in the graph in the backwards the only thing you have to figure out is that how does this you know like like what's this relationship basically this Arrow right so how does the the derivative with box a depends on Z right so what's the derivative respect to the input of this of this module of this you view this modification as a as a generic module let's say right so and and the only thing you have to figure out is that how do you write this so-called backward function this backward function takes in the derivative with respect to the output of this module and output the derivative with respect to input of this module and and that's that's the so-called backward function for example if you write pie charts if you need to write a new module basically you have to implement the forward function and the backward function the forward function is how you get Z from a and backward function is how do you get the derivative with back to a given a hypothetical derivative with respect to Z the hypothetical I mean like just some vectors right you're taking some some vectors and you get some light results and this is the function you have to implement for the module and and once you have this module then you can run like a then you just everything else is systematic yeah I think I should stop here okay thanks"
"Stanford CS229 Machine Learning I Bias - Variance, Regularization I 2022 I Lecture 10","Stanford CS229 Machine Learning I Bias - Variance, Regularization I 2022 I Lecture 10

so I think I'll still spend like five minutes just briefly review um on the the bad propagation last time I think I I was running behind last time so so I didn't have time to explain this figure which I think probably would be useful as a high level summary of what's Happening uh I'm going to Omit all the details so so I guess I'm drawing this in this way like this is the forward path this is how you define a network and the loss function right so you start with some example X and then you have some I guess this is a matrix back to multiplication or Matrix Matrix modification if you have multiple examples but this is a matrix Vector message multiplication module and you take this you take X in a product of multiply with w and B and then you get some activation the pre-activation and you get some post activation you take some Matrix Vector multiplication and you get um I guess I'm using I'm matrix multiplication but actually it's Matrix Factor multiplication so uh and you get the activation and then you do this right this is the how you define the loss how you define the model I guess the output of the model I think last time we used tall which is the output of the model and then you have something that defines that defines the logs right this is the the so-called forward path and and and the in some sense the the you can summarize the the back propagation uh in a way uh like follows so basically if I draw it right so this is uh of course you know what you really do is you implement this in computer um but if you draw it in some sense you are doing it in a backward way so what you do is you say you first compute if you look at the the flow the data flow or the kind of the com the process of the back props um back prop process so you compute uh the laws with respect to uh the output first and this is often very easy this is like just you take the because the loss is something like y minus Tau Square Times a half and this one is just a very simple formula and then you compute um you can you compute the derivative of the loss with respect to a two here I only have three layers so and then you take the derivative you compute the derivative of loss with respect to Z2 and then you compute the derivative of loss with respect to uh A1 and then something like this this is the the order of the computation uh you walked and and this is kind of like it's kind of like you are actually in this networking in a backward fashion in some sense um but how do you do this each of this Arrow so this is by the the Lemma that we discussed right so so I think we have three landmarks or three abstractions right so and each of this arrow is using one of those three elements so and and now you can see what this those kind of like landmarks are for those dilemmas basically are saying that if you know DJ over dtau how do you compute d0 over d a and and there's another element which says that if you know how to compute DJ over da how do you compute DJ over DZ and and all of those slime us about this kind of relationship if you know how to compute um the derivative with respect to the output of some module right suppose this is a module towards the output of this module right so if you know how to compute DJ over D the output of the module then you want to know how to compute the derivative with with respect to input of the module so all of the three lemas are doing these things I'm not going to the details because you know we don't have enough time to review again but that's the basic idea and um and also there's another thing which is like this is only about the derivative with respect to activation you can also compute a derivative with respect to the the weights right so if you know this quantity then uh I think if you know this quantity then you know how to compute the derivative with back to the last layer weight and if you know this quantity then you know from this quantity you know how to compute the derivative with back to W2 and from this quality you know how to compute derivative with respect to W1 and also the same thing for for Beats so um and and this kind of the the last row this quantities right so they are they don't depend on for example after you get this you can compute this right and after you get this quantity you can complete these two quantities but this this Row the derivative with respect to activations you can only do it sequentially you cannot say you compute this before you do this so so this arrows kind of is kind of the the orders of the dependencies between these quantities and um and and each of these Arrow you know is basically done by one of the Lima that we discussed last time right each of the Lima is kind of dealing with the um with this any questions this is just the extension of the last five minutes of the option I didn't have enough time to elaborate on this okay so um good so now in this lecture and the lecture afterwards we are talking about um um I guess a few Concepts one concept is called generalization which is the main point of this lecture and also next lecture we're going to talk about on the concept of regularization and next action we also talk about some of the practical a Viewpoint of ml like how do you really tune your model how do you uh whatever you have to do in this whole process right so like you start with the data process and then you have to tune a model and then maybe you have to go back to change your data so on and so forth so um so basically these two lectures I think we are um um we're gonna discuss this kind of um this concept I think the generalization is probably the main thing that we are talking about here so um so generalization as you can see you know as you can guess it's really just about you know how well you are your model is performing on Einstein test examples so we're going to discuss you know how do you make sure your model can also generalize to unseen test examples so so far we only talk about training right so we have some examples which we have seen when they are tuning data sets and we fetch some model on top on them right so and now we care about whether this model will work for future unseen examples so and we are going to discuss you know um on a bunch of Concepts you know the bearings very straight off which is which is a kind of a principle when you think about how test error changes as you change model complexity and we're going to talk about some of the new phenomena people have found in deep learning which is a little bit different from the classical understanding um okay so I guess uh um that's just a very high level overview I guess I use a lot of passwords I'm not expecting everyone to follow everything so um let me um maybe be concrete um okay so I guess so let me start with some kind of basic notations and Notions so I guess some basic Notions one thing is this so-called tuning loss which you probably already know what it means you know shooting laws or sometimes it's called tuning error sometimes it's called training cost I think in in this lecture sometimes we use the word cost so they all mean uh the similar type of Concepts sometimes people use loss to refer certain kind of losses and error to refuse refers to certain other type of losses but but they are from the purpose of this lecture they are all means same thing right this is what you care about in a tuning right for example if you care about the square loss then the training loss will just be this I think we have to write down we have written down this equation of times this is the loss function you care about when you have square loss and other laws could be cross entropy laws it could be um um like a mle the maximum likelihood estimator I think that's actually one principle to derive the tuning loss right you derive the maximum likelihood estimator for uh for a data set and that you use that as your training loss you use the negative log likelihood as the tuning loss so this is basically so far what we have focused on in the last few weeks so how do you get the feeling loss and how do you really Implement on this and optimize this right so there are many ways to optimize it for example in one of the lectures we use the analytical formulas right so we we have the GDA we analytically compute what is the the minimum loss right the minimum uh uh the minimizer of the negative log likelihood and it's you know of the other lectures we are using a numerical algorithm to minimize this loss right so like we for example like in deep learning we are using stochastic wind descent um and we have talked about Newton's methods so and so forth but so far everything we have to talk about is this loss function when we try to find the minimizer of the source function or some oh you know not necessarily exactly this one but like either the but always it's always a loss function defined on the training examples okay so now um suppose you have obtained right so suppose we have some parameters Theta so suppose we have obtained um some SATA how do you evaluate what you're saying is good or not so ideally you want to model to not only perform well on the tuning data because for the tuning data you already know the prediction right why you care about letting the model to predict something you already know you already know so what you really care about is you care about you know you want to evaluate on unseen examples so that's why the test loss is defined on unseen examples um and I'm going to use this notion so suppose let's say you draw so the process is that you draw some new example X comma y from some distribution D and often this is called test distribution and and then you evaluate what's the expected loss on this new Test example so you look at L Theta which is the expected loss of and the expectation is over the randomness of this new example Juan from this test description so what's important is that this X and Y is not seen in the training it's a new Fresh example and of course here I'm defining it as expectation right so actually in places I'm taking average over the entire discussion so so if you really want to do it in parallel what this really means is that you draw a bunch of examples maybe let's call it X1 test test one you draw maybe I'm of this these are another examples you have used for training these are new examples you draw during the test time you draw them from B ID from the distribution d and then you evaluate the Empire the error on this side so and you evaluate average error on average loss on on this set on the test set because you know that if you evaluate on this test set uh it's it's pretty much just approximating this expectation you are just using empirical way to estimate the expected value right like if you want to if you want to estimate any expect an expectation of any random variable one way to do it is you just draw multiple copies from the same distribution and you take the empirical average right that's why the test set is a reasonable estimate for the um for the test error and just to be clear these test examples you know you don't you haven't seen them in a changing site they are something you draw um you know you can draw them in advance but you don't you cannot like them to be seen in your training uh the tuning process and there is a notion called generalization Gap foreign often people called this is basically talking about the difference between the test loss and the tuning loss and oftentimes you know it's not always true but oftentimes the training loss is less than a test loss when you test you found that your model is not as good as you thought before on the training set you know you know it's probably sometimes it's probably a little bit worse sometimes it's a lot worse sometimes they are very similar but generally you shouldn't expect that your test performance is just dramatically better than the training performance I mean of course in extreme cases you can design data set such that this happens but but I think in in realistic practical situations I don't think you should expect that at all um so so it's often the case that this Gap is either very close to zero or maybe a slightly negative slightly positive or it's it's much bigger than zero so you want this Gap to be as small as possible so so basically you just in some sense you care about two points is you care about the training loss and you care about the gap you want both of these two to be small if both of these two be small or if both of them are small then the sum of them will be small and that's your test by your test loss is small um that's that's the hope you hope that this both of these two are small foreign so this one is something you can control in some sense right this is what you try to optimize for right but this one is harder to control because you don't so right because you cannot say I'm going to find a data such as L Theta is small because if you do that in particularly you try to optimize Theta such that the Tesla is small then you have to see the test data set right so so so so so so that's why you cannot really easily control this because you you are not allowed to test the assets so you like you cannot choose your Theta based on the loss you can only choose Theta first and then you evaluate loss but not vice versa so that's why the the generalization Gap is something that is very hard to control at least you cannot directly control it you know and and the the point of this lecture is to discuss you know in what cases you can somewhat no this is not too big right like when this can you can hope that this is not too big um okay so and and then we also did uh before getting into more details let me also Define of two notes two kind of like commonly used uh terminology so um of course we are dealing with the case when else so we are mostly concerned about the case when L say that it's too big right so if else it is small that's great right you don't have to worry about anything so when LCD is Big the question is what what do we do to change it right like if You observe that your test loss is very big then what you can do to make it smaller that's the kind of the question you want to study so and typically when LCD is Big you can there are two on failure mode in some sense these are not supposed to be um these are not supposed to be um you know comprehensive but I think typically you are in either one of these two failure modes so one of the filler mode is called um affiliate patterns so one of the failure mode is called overfitting and so over 15 you know I'm going to discuss a lot about overfitting um but you know the first other bit is that the typical situation of overfitting is that the training loss J is small but the the test loss is big so you have this big generalization Gap so you have a discrepancy between tuning and test so that's at least that's you know that's not a definition for our fitting but that's a very um uh a a very typical um characteristic of overfitting so um for example I guess I'll probably draw this you know very often uh our job is going to figure out very often uh in in this lecture so suppose you have a um some X and some y you have some data sets I guess the running example I'm going to do is that I'm going to have some data set that lives very close to this quadratic quadratic function so the the data are approximately quadratic so X and Y so you want to be has a one-dimensional problem so given X you want to predict Y and You observe some um so you have a data set for example you have four points so each point is like this maybe this and something like this maybe something like this right so you have you see these four blue points and you want to fit a line to it or filter some curve to it and the question is what curve you're gonna affect so suppose you fetch something crazy like this let me try to see what color I'm using for this well sorry one moment let me um think about how do I use the color on your consistent way so I guess if you fit I'm going to use black follow for the model you fit so suppose your model your fat is something like this I'm drawing something crazy so this model is I intentionally make this model to pass these points exactly so this model face the data the fortunate data perfectly right so the J Theta is really small it's kind of close to zero but you can imagine this model shouldn't generalize to anything examples right so suppose you suppose you um regenerate some examples you know and you kind of believe that ANC examples also are kind of like similar um like have a quadratic relationship you generate something like this maybe somewhere here maybe somewhere here then you can see that the The Fading to the the right Point becomes very worse so much worse becomes much worse so the so the test loss is very big so so this is a typical situation of overfitting in some sense you are saying that you you fit to the data very well but you are you are on you are over fitting in the sense that you you um you only focus on the training data but you you kind of like um forget about the the test performance you know I will discuss why this will happen um I guess you can probably guess um but this is for so far I'm just defining the roughly what over facing means so it means that you are not um you you feel the tuning data but you don't you don't generalize and another um uh notion is called underfading so an undertaking basically just means that you face something like this maybe let's say you fit this suppose this is another model effect so underfating just means that both The The J the J Theta is also big so so even your model does it doesn't even do well on a tuning set and that that is basically means under fitting um so you as the word suggests that you are not fitting the data um and and whether you're in the overfitting regime all the underfating regime or in the in a nicer regime um depends a lot on different things and one kind of decision we are trying to discuss today is that you know what is the right model complexity so like whether you're going to use linear model maybe use our quadratic or maybe fifth degree polynomial or when you might work so and so forth so we're going to discuss you know uh what will happen if you change your model complexity and whether you know in what cases you may underfade in what cases you may overfit and what is the best sweet spot any questions so far and kind of like as a spoiler in some sense like we're going to discuss two um we're going to decompose the test error l Theta we're going to decompose this into two terms actually I'm not going to show it mathematically because I don't think I have enough time to do that but intuitively you're going to decompose the test error into two terms which is called one is called bias and technically it's y squared because the bias is defined as the square root of this term so plus variance so you're going to have this you're going to Define these two terms and and say that these two terms if you take the sum of them it will be the test error and these two terms has this property that's the BIOS is going to be uh an increasing function so we're going to see something like this the bias is going to be a decreasing function as the model complexity I haven't told you what the bias is what the virus is I'm just kind of giving your kind of like a a spoiler on what kind of things we are going to discuss so the bias is something like this and a virus is something like this so these are basically you are kind of like trying to figure out the underlying kind of like mechanisms um so so the mechanism is that if you change the model complexity to make it more complex then your bio environments will be bigger and the bias will be smaller and your sum of these two functions which is the test error will be something like this and then the best one will be something in the middle so so this is the kind of the uh um a quick overview of what we're going to discuss so all right okay so now I'm going to um Define bias and virus um in a little bit more formal uh ways still not very formal like like there there's um like I'm going to start with uh like it's a great it's a graduate process I'm gonna have a little more formal definition of the bias okay and I saw some uh examples so any questions so far why is crazy I mean oh this is just a maybe I should draw this is just because it's kind of a unique thing you need to define the bias to be the uh it's just the um how do I say this like it's a definition like a like a actually some people call the the biosaurus bias actually in some literature sometimes people take a square root it's just a how do you choose the right unit um yeah and I I you know when I say bias I I don't really distinguish whether it's squared or not it's so um okay so I guess um what I'm gonna do is I'm going to have a running example which is basically like this and I'm going to uh kind of like try um what what happens with linear 50 degree polynomial these are um and and kind of use this technology as a solid experiment to demonstrate these qualities so let's start with linear so this is um sometimes this is a solid experiment but but actually we have some real data experiments in the electron notes here I'm just drawing this um um so like um but I think it's um it's pretty much the same so so suppose you okay maybe I will set up just really quick so my running example is basically like what I on Drew about um so so I'm gonna have some training examples and these training examples are something like y i is equals to a quadratic function quadratic which is just this quadratic um imagine and plus a little bit noise this is a small noise so that's why these blue points are not exactly like on the quadratic it's just there is a little bits fluctuation so um and sometimes I think um I guess this quadratic sometimes they're called as H star XI just for the sake of technology I think sometimes I call this the ground shoes this is the in some sense the the true function you are trying to find out um but of course you don't know it you want to you want to try to recover it and I'm going to do a cell experiment first you know I'm going to do a few experiments I'm going to start with linear model and then I'm going to try fifth degree polynomial and then I'm going to try quadratic so um so linear model suppose you have a linear model I guess you can probably see you know what will guys what will happen so I'll draw this again so you have this four data points something like this now what happens with linear model is that you know you have these four points what's the best linear effect probably would be something like maybe this for this particular data set right so and and you can see that uh what's the what are happening here so maybe let me see how do I um maybe let me erase this for the moment I'm going to redraw this again so for linear models I guess you can see a bunch of properties right so you can see that this is a large training Arbor training loss alternate losses for consistency there's a lot changing loss because um I guess what's your prediction on a tuning data set this is your prediction for this for this ax right so so this is X1 and the prediction is here and the prediction Fox 2 is here the prediction fox 3 is here the prediction for X4 is here and you look at the distance between the production and the true label you see that the distance is pretty big right so um so the channel the Chinese Hardware is fairly big and so on and so so this is under 13. okay by our definition of under 50. because the tuning is already big and now let's uh think about so what what you should blame like why the training is back what was the what's the corporate the the corporate you know I would argue is that it's just because no only linear model can fit your data it's not just right no only linear no any linear model can can work and it's not because you don't even have enough data it's just because you know like even you have more data a linear model wouldn't work as well right so this is just because the linear model is not expressive enough so and and that's the and this is called bias so this is so this is called bias so we're in this kind of settings um things happens you like you have device so the bias is um basically like it's saying that okay the reason why I don't know exactly why people call It Best in the very first time but I think you can kind of the the kind of the relationship the thing is that you are you are imposing additional structure right so you are imposing a linear structure but the true data is not linear so it doesn't matter how many data you see if you as long as you impose this you you just insist that I just believe that this thing is is linear you're going to fail because this is the wrong belief about the relationship between Y and X so that's why this is called bias and this is not you cannot mitigate cannot be mitigated by more data as I said and a nice actually it can also not be mitigated by less noise even your data is more and by less noise data right because even you have more data and with less noise you can imagine what happens right so suppose you see a little bit more data supposed to see some more data as tuning data right and now and maybe let's say you just no suppose in the extreme case you just see everything on on this exactly on this on this quadratic line without any noise still if you think about what's the best fit for example let's say just you see all of this blue and the green points and what's the best effect the best fit probably would change a little bit that's true right it probably wouldn't be exactly this maybe it would be I guess it would be something like this maybe maybe something like this I don't know like you have to trade off right because you know whatever you fit right if you fit this then you you don't fit some of these examples if you do you know there's no any option right it's like whatever it's just because linear model cannot represent quadratic function that's it so so that's the the typical uh situation where you have a large spice and mathematically so the way you define miles so here I'm just only talking about some characteristics of having large bars so mathematically one way to define the bias you know is that you can say this is the um so buyers is I guess actually there's some approximation here depending on what exactly your model is but roughly speaking is the the the the best uh our loss uh you get you can get with even infinite data so you guys I guess you know if you suppose you have infinite data you have a data set with infinite data following the same kind of property right so like I'll generate from this quadratic plus noise then what's the best you can do and that's called Vice and and you can kind of see that you know it's probably important for buyers to be small because if its bias is large even with infinite data you cannot do anything right so um and that's the that's the problem with linear models any questions the distance um I think I think that's pretty much you know uh so for this case they're pretty much they are the same so basically so so in this in this case it's exactly true that the bias is the best linear model uh so the closest like the closest medium or the model that is closest the the linear model that is closest to the ground truth and that error that closeness is is device right because when you generate infinite data basically you just generate the ground truth the whole line right if you have no noise you are now changing the model class you're only using linear but you cannot okay so buyers would be the best okay so in some sense technical reasons to say bias is is is property of the the the the family of models right so so the linear mode the linear model family has a large price right I I think you know um yeah I think that's that's that you know we are always talking about model family right so we're talking about either linear model family the family of linear model so the family of fifth degree polynomial or the family of quadratics Okay cool so this is the bias and now let me talk about the virus um and here there's you know I'll come back to the virus for this model but here the variance is in some sense you can say it's not very important only the bias is the corporate and now I'm going to show cases where the variance is the corporate to blame for so um so I guess I'm gonna really draw this so you have 2004 points okay so now I'm going to fetch a fifth degree polynomial so the model is something like hclx is you know some Theta 5 x to the 5 plus up to instead of zero but recall that we can do this with with linear regression because you just this is still linear in the Theta right we have a homework question on this we also talk about how to do this you know with kernel methods if if you care about efficiencies right so so we are able to do feathers and an intellectual notes actually there are some visualizations of the real the real model is going to fit so here I'm just gonna draw it so if you fit your 50 degree polynomial so probably you're going to get a 50 degree polynomial can be can can go up and down so many times several times I think technically a 50 degree polynomial you can have I think four four Global four local four local maximum all good minimum four or five something like that so the higher the degree is the more times you can go up and down right so um right because if you have a quadratic the only thing you can do is this or maybe this and for cubic you can do this and for for first degree problem you can probably do something like this right so um so the exact details here don't matter so uh just the the point is that if you have high degree polynomials you can it can be more flexible right and then if you fill the data um if you fetch the the polynomial to the data then possibly you can organize something kind of Plenty flexible something like this and actually if you really look up for some like this is not required for this course but if you look up the book follow the calculus or like polynomials you know that if you have four points there's always a 50 degree problem when they pass through all of them so in some sense if you don't have enough points and your degree is high enough then you just you can always you can always make the tuning out with zero literally zero so in this case the tuning area is literally zero so so and and then why right so I guess this is expired and the the thing is that so this is over 13 so but why what's the problem here why is overfitting so why um the test is not good so in some sense the kind of the the intuition is that this kind of model fits so it's faced to the Spurs patterns to this first patterns um you know in the in a small and noise data small and noise data so so this is because you don't have enough data and your model tries to explain all of this not small perturbations small noise and because it over expands the small noise it lost it's it kind of like didn't pay enough attention to the to the more important stuff and the reason why you can over refer to the small noise the final data is because um because you you are you're so flexible right so whatever patterns you you see in these four points as long as you just have four points whatever crazy patterns you see you can always find the degree five polynomial to expand it right so so whatever patterns you see in four data points you know like you can explain it so that doesn't sound right right so like how come your model can explain like everything and anything like a random so so basically you are looking at the you are kind of like over threading to the Spurs um patterns um but instead of the the big pattern so the big pattern is this right the spur is fine inside of the fluctuations in some sense right so um and so all the other words I think you are you are explaining the noise instead of the the ground truth so um and and okay how do you make this intuition a little more formal so okay I'm not gonna go very very formal but like some more kind of like things I can say about this intuition is that this is saying that you are sensitive your model is sensitive or maybe kind of like specific to to the noise how do I formulate this like one way to kind of formulate this a little more mathematically is that you can consider you redraw on the samples and you ask right after you redraw the samples are you going to see the same model okay right so you draw some new samples with different Spurs packets right because they are Spurs because they are noise right so so if if you're if your model is specific to the spirit's curtains that means if you redraw you are going to expect this you are going to learn the new Spurs patches and you're gonna have a different model and if you're not specific or sensitive to Spurs patterns even you have a new data set you probably shouldn't change much but you should still be somewhere to say you should still out of the same model and it turns out that if you have the five degree polynomial you reach all the data sets then you will find a new model so what happens is I suppose you um read all the data set in the election notes there are some real experiments again but here I'm just gonna uh Jordan so suppose for example now you still have the same ground shoes But You observe some maybe let's say here I'm going to have something upon like this maybe developer like this maybe yeah like hit this and I'm going to try to make the pattern a little bit different then maybe you're gonna get um something different Maybe I don't know like you try to find out what's the polynomial maybe you want to guide something like this okay actually this these two are still a little bit similar but I can't do anything empirically you'll see that it will be different just because you know any small perturbations of this would change a lot but maybe you you got this so and and if you actually you can also do some local thing right suppose you move these points a little bit lower then you probably would change this function a lot so just because you are very sensitive to the data phones not increasing the number of samples right so so far I'm saying that you you don't you don't you don't you draw the same number of samples with similar well choose the same constitutional institution but just the randomness are different right you're using different noise so um right so and that's a good question that's exactly what I'm going to talk about next so so uh um okay sorry one moment before that so so basically okay just to summarize here so if if you reach all the examples and you find that a large variation between so suppose you have a so you so you have um um so you call this so basically Define the variance to be uh in some sense the variations across um models learned on different data sets so for example you draw five data sets right so each data set has four examples maybe and you try you do these experiments and you get five models learn now five different data sets so if you see a lot of differences between these models right so then that means you have large variants and and if you don't see a lot a lot of differences then you don't have a large virus that's the some of the formal definition of this you know we will have a little more formal version of this but this is the idea right so so maybe for example if you you get a new data set you get something like maybe here here they're here and maybe you're going to learn something very different maybe something like this right so and there are all some so here at least you can see this one is very different from this one because on the left hand side here you're going up here you're going down so um so so that suggests that you have large virus and and now talking about data right so suppose so so this one of the characteristic of virus is that virus is something that can be uh reduced if you have more data so um and and in some sense the virus is caused by lack of data and it can be mitigated if you have more data so let me continue here keep all of these markers in my hand otherwise I have to walk back and forth Okay so so the virus and sometimes you can say this is caused at least partially you know at least one cause is that this is caused by lack of data and um and okay of course you know it's probably you cannot say this is only caused by lots of data because you know if you have um a different model right it's a very very so in some sense there are two reasons one thing is it's like you have lack of data and the other is you have two expressive too expensive models and these two things are kind of like a relative to each other right so if you have uh uh very expressive model but your data is really really big then probably is okay on the other hand you know if you have not too many data but you have very very simple model then it's probably still okay so um and and as you can see that you know then if these are the issue the reason then you how to mitigate the virus then the mitigation is just that the medication is that either you get more data or you have small simpler model so technically you don't have more data if you have more data you should already use them already but for the for the for the um for the understanding let's see for example what happens if you have more data with this what's this thing right suppose you have more data and you still fit a fifth degree polynomial so suppose you have a a lot more data this is the ground shoes and you observe a lot of more data as you have a million data right roughly you know there's a little bit fluctuation of course so now you want to fit a 50 degree polynomial what happens will be that this is probably not entirely obvious but like okay one obvious thing is that you cannot you probably wouldn't do anything like crazy as this right because if you do this a crazy thing maybe this crazy thing goes through some parts but you cannot go through all the points right like for example right you can see here is there's a big match between this part and this point right and here you have some uh mismatch right so so this this one wouldn't give you even a small training error so this is not a best model fit on the training data so what you really will fit like if you minimize the error on the training data with this so many trillion examples then what you get will get is probably something like this more like this maybe there's still some small fractions it's not like necessarily matching exactly the ground shoes but you have a little bit smaller fluctuation but it will be something like this because because if you don't do this then you wouldn't fix the original data as good as well and and this is you know you know this is kind of like more like a quadratic but you know but a fifth degree polynomial contain the family of degree 5 problem will contains the family of quadratic function because you can just thread you can just set your Theta 5 say the four to be zero then you get a quadratic so empirically what you're going to do find is that probably this if you really look at the details this the best fit model is still degree five polynomial but the set of five Theta 4 the first few coefficients are very very small so effectively you are just very close to quadratic oil [Music] s right so so the question is like a um another possibility is that affiliate mode is that you just couldn't find this degree five polynomial right you you find because some optimization issue right maybe even though there exists one that is very good they face the data but you couldn't find it um that's that's that's probably not true for the fifth polynomial for this one entire example just because this is very simple but it could be possible uh for for some other cases where you just the the model does exist but you cannot find it so so this uh is something that we don't discuss at least in the scope of this lecture so in this lecture we are we are assuming that you can just optimization always works you always find the best model so um so if existed you can find it so um so that's why like I'm I'm okay like there's there in this case you know even you have a lot of data right and even have a very complex model say degree five polynomial or even degree 10 maybe in this case right so there's always exists one model right that works which is like something like this like like the ground shoes and and and we'll find it you know for this case definitely we will find it because it's a it's a linear regression problem you'll find the best mode right so okay cool so um and also another maybe just uh to to answer the question so um um so in some sense the the problem you are referring to is easier it's easier to detect in some sense to some extent it's not always true because at least you can detect that from the training right so here we are more talking about the United States So Okay cool so any other questions is yeah yeah so so so so here when I say more data I really mean that you have you just collect you you have more data from the same discussion like uh from the same description yeah yeah so like if if you collect more data from yeah so like in sometimes you you you you kind of like the I'm not saying this is universally applicable to every situation but the main side we are in is that um for example you have um um how do I say okay you have a lot of like a um like medical images right so like they are for example there is a million patients with the cancer diagnosis kind of thing and but not all of the data are are labeled right so like only problems at the beginning only for four four images that are labeled as cancer or not you know so and so forth and now but but these four Images are sampled from this big population and now I'm asking you know I found out my variance is very big so how do I mitigate that so I probably one thing is that I can just sample more data from the same I started I have like one million unlabeled examples right I I had four labeled ones and now I'll say I'm going to collect more labels so I sample like another like 100 examples from the same distribution and then I label them and then I run algorithm when the variance will be smaller from troops it is a leadership so the question is that how do like if you don't know the ground truth right so how do you know that you are uh you're having a large bias so so that's uh like like a you you cannot really exactly know when you don't look around truth so all of these are so far are fall analysis purpose right so when you don't know the ground shoes um you cannot really exactly like um let me think so um yeah we don't know the ground shoes I I think you cannot um exactly computer bias um because you know the definition of device actually requires you to sample a lot of uh like data right so you also don't have infinite data so there's no way you can evaluate the bias exactly so so typically what you do is you say um you fit the data on a training set and you see you are under 13 and that's that's when you say innovating means like you have um you have a large chaining error and that's when you start to believe that you have a large bias this is a test error yeah I'll discuss that in a moment like because I didn't I'm gonna draw this I didn't even tell you what is I'll go back to come back to this degrees for highly imbalanced data set so maybe let's discuss this offline I'm not sure whether this I think it probably requires more you know the investors that is is pretty often you know like we have research on that but maybe it's not exactly related to the context here maybe you can discuss offline okay I think I still have something to say about the virus um and then I'll come back to the trade-off um all right so so basically okay so now let's see so let's Briefly summarize um so basically if you have the buyers this is really just about the lack of models expressively it's something of intrinsic nothing to do with data right this is just the uh lack of if you have large bars that means you have a lack of um like expressivity the model is not expressive enough it doesn't depend doesn't depend much on the data um I guess you know for linear models you can just say it doesn't depend on number of data for not only the models there is some technicality you know which you know you don't have to like the only reason why I add much is just because you know there's some technicality for that provide me to say this is exactly irrelevant to the number of data but but you should basically just believe that it doesn't it's it's intuitive it's not a notion about how many data you have it's really about how expressive your model is so um environments you know if you have a large variance then it could be two things but one is lack of data and another thing is you have a too complex of a model okay I guess I'm just repeating summarizing and then I guess we can see there's a trade-off um so I guess I'll go to here so and also there's a way to for you to prove that test is equal to device plus plus virus uh I don't think I have uh I will see what I have time to discuss that um but um but you can also prove the test error is equal to by square plus variance um so but maybe let's just uh draw this from scratch so this side is the model complexity right so let's first think about how do you draw the bias on the right this is the test this is the um how do you draw the the bias on this curve a smaller complexity change so we say that the bias is large it's because your model is not com mod is not expressive enough so that means that if your model is more expensive than your battery decrease so that's why the bias is a decreasing function as the model complexity right so this is the bias and now let's let's think about how do you draw the virus on this thing so we said the variance is caused because you have too complex of a model that means if the model is more and more complex then you should have bigger and bigger virus that's why the virus is like this and the test arrow is the sum of them so so the test error is like a u curve thing so the test error wait what is my oh here so the test hour is the sum of these two and the environment and and so the question you want to answer is that if you change the model complexity what is the best tester right so so it means that somewhere in the middle so um so so so like a um actually there I'm going to tell you something different from this you know in a moment just though but it's supposed to believe in this then what the conclusion the implication of this is that um you should um somehow kind of find a sweet spot when you choose the model complexity right so for example maybe at the beginning you found that your training error is very low so our training value is very high which means your battery is very very high right so suppose your model complex is here then suppose the model complex is very small and then then what happens is the bias is high and the bias is high it means you are under 13 and means that your training average is Big so basically when you see the training hours back you kind of see your biases you kind of believe that your bias is too high so that's why you should increase the molar complexity and at some point you found that you are you know in the other regime where the virus is too high then you should you should stop so basically you increase the molar complexity to some extent until you violence um uh is uh your bias environment has the right trade-off s yeah so I think this this figure so this is the okay you ask a good question right so here this is the molar complexity of the the model you use to learn the the your your your parametress model right so when you're asking about what what happens if the ground shoes is different right so um the when the ground I think this is now very sensitive to what the ground truth is right there's always a trade-off but but where the trade-off comes from you know where The Sweet Spot is would depend on the ground truth so uh for example actually that's a very good question for example suppose you uh for for this data set right so probably the best thing is to use quadratic quadratic would uh has small enough bias because you know quadratic is in principle expressive enough to express our data so that's why quadratic has small bags and also quadratic is probably the among all the models with small bikes among all the models that can express your function quadratic is the least complex right so that's why you use quadratic that's the probably the best solution and if you really run random algorithm the quadratic you would probably recover something very close yeah so um but if you're going to choose this cubic then maybe the three spot is like the the best trade-off is achieved at Quebec Maybe you know it's not they don't necessarily have to match each other because it also depends on the data uh how many datas for example suppose you are maybe let's let's give you an example supposed to say um your ground shoes is a degree 10 polynomial but they somewhat look like a linear function so so suppose your ground choose is a ground shoes is like a um almost linear but with a little bit kind of like small fluctuation and but you don't have a lot of data you just have like a five data points right so you just have five Training data points and now if you want the virus to be literally zero then of course you should use degree 10 polynomial because that's that's the only case you are expressive enough but maybe but then your virus is too big so the so the right trade-off here probably is closer to be a linear because if you use a linear you bias is not zero but still you know small enough right uh and and in that case the variance is small so so so the right the best trade-off depends on for example how many data you have as well right that's a good question and and the answer to that is that no you cannot come to the biotics and virus so all of this um all of what we discussed today is is more about um some internal understanding so this bias environment is not something you can um at least in some cases you can estimate them a little bit but typically you don't you probably shouldn't really actively estimate the the bias environments in your yours these are mostly just for it's an internal understanding for for for for our research for ourselves but not necessarily something you empirically evaluate um so okay so so I guess so one question you know I guess many of you probably are wondering you know if all of these quantities cannot be even evaluated you know how do you choose the right trade-off what's the optimal model complexity so what you do is actually that's that's gonna be I think what we discussed mostly next week uh next lecture so this is this is Wednesday right next week yeah so um so you think the virus or virus the virus and buyers are just for understanding empirically what you really do is that you try to you should try to uh try a lot of different models and you select based on a validation set right but but this picture would let you know would help you a little bit in some sense because for example suppose you have tried this and this and this and this suppose you have tried four model complexity right so and suppose you believe that this is a u curve the test error is a u curve then should you try even bigger models be your family with models probably you shouldn't right because you believe that you kind of believe that it will be even worse so so you should just try even more in in the middle right so that's the what's what this understanding will help you [Applause] [Applause] Okay so [Applause] um there's some uh more formal definition of the of the bias and virus and that's in the lecture notes in section 8.1 I think I don't have time to discuss the formal uh definition even if the definition I probably wouldn't be able to give you the proof the proof is actually relatively simple so if you are interested you can you can read that section yourself um I don't think it's required for the for the for the exam or anything um but it's a relatively simple rate if you're interested and and also just this kind of bias virus trade-off is not that always easy to to achieve mathematically so uh for square loss there is a classic you know well established kind of decomposition but if you don't have square laws you don't have MSC like mean squared error if you have cross entropy loss actually it was open question how do you formally decompose this so um so in all the intuitions still apply right so but like how do you do the mathematical decomposition is actually pretty challenging um so that's why in the in the election notes we only talk about Square loss and any anywhere you if you read any textbook or any literature probably they were top top bars Square loss um but the intuition is still kind of fine right so if you don't care about what exact definition of classes so I will spend the next 20 minutes to talk about um a new um something that is actually challenging this picture so something that um so this is the maybe just follow the more context so this kind of like a u-curve test everyone best friends trade-off this has been um um um like a this discovered or kind of like analyzed for I don't know how many years maybe like 40 years or something like that but maybe like I I'm not a historian so I don't know exactly which is the the first time this is discovered but this is like a very classic like a like um however people realize that there's some uh issues with this understanding um especially we realized that in deep learning like you like actually people start to realize this in deep learning but actually it turns out that even this understanding has an issue for linear models um so so this understanding has a it's not complete uh it has uh it's Miss uh it misses some uh some other things so um so that's what I'm gonna talk about um and this is a this is the area of um uh uh research productive in the last probably three or four years so let me try to find out where should I erase um foreign [Applause] at the beginning and then analyze theoretically this phenomenon is called double design if you are a historian then I think actually this the the the this phenomenal actually dates back to something like 1990 um um some papers actually at that time also pointed out this issue but I think it just becomes popularized uh and and more relevant these days and uh and what does this mean is that so basically I've told you that this is test error this is model complexity I guess technically here I'm writing a number of parameters because I want to be precise like imagine the model complexity by how many parameters you have and the classical belief as we discussed is that these test error should have this u-curve something like this but then people realize that this is a striking thing so people realize that if you increase your model number of parameters even more at some point you will see that it will be like this so this is the basically this is the new regime that people cause this is the second Descent of the test error that's why it's called double designed because there is a design here there's a design here and and this is the um everything the blue part is what people didn't realize um as as much as as in the last four years last four or five years and and these are the so-called over parametrized regime foreign so which means that in this regime typically the number of parameters is larger than the number of data points in some sense this is the regime that if you ask her you know someone 20 years ago the news said this regime is just a dead no-go Zone because you should see very very bad test error but it turns out that if you have more part you make it even more extreme you make the number of parameters bigger than the number of data points uh you you may actually um is in not in all cases but in some cases you may see uh the um actually I wouldn't say I shouldn't say more some cases like in many cases like I could I'm not sure how to qualify this but at least you know a lot of cases um you will see your second descent so um so that's the Striking thing the one with much more data so is this um not directly let's say because this is you know at least on the surface if you look at this right so you you your this regime is the regime where the parameters is bigger than number of data points so so so if you want to find the right course you know I'm not saying um like you probably would say at least you need to to be in this regime probably you need to compute you need a lot of compute because probably you know like 10 years ago or 20 years ago you cannot even afford to run experiments in this regime because you don't you don't want to use that many parameters because you don't have enough compute right so but of course you know in in nowadays we also have more data points so and because we have normal data points because we are using Lightworks you know we we run larger and larger experiments you know so so indeed we it's correlates with more data points like we do see more data points in on these days right so um and this is the the so-called double design phenomenon and it's kind of mysterious um uh it's a bit less mysterious these days like after people have started um this you know in the last five years um very carefully um I'll talk about some of the explanations intuitions um but before that let me also give another um a related phenomenon which is um also called double design but it's called Model it's called Data wise double desert so here I'm doing a um a similar I'm just showing a similar graph but on the x-axis I'm going to change the number of data points so so here the the y-axis is still a test error and the x-axis is the number of data points so and now okay maybe you have a guess first you know what what this curve should look like well as you have more monitor points how does the test error change right the guess would be the test error would be decreasing right because I guess here you know at least if you believe in this biased environments kind of intuition then the bias doesn't seems to depend much on the data right the virus will be smaller and smaller as you have more and more data right so so then what you if you believe in that then you should say that okay the test error should look like this and it should continue to decrease as you have more and more data and it turns out that actually um in many cases what happens is that the test error will look like this I'll increase at some point and it will decrease again and on this peak here it's kind of like similar to the peak here so this peak it's often happening when um is roughly equal to D I guess by the way here like you know there is a and this is active research area so I'm not being very precise in every places so um so only is the number of examples this is number of examples this number of parameters so what I said here um I think is basically mostly a kind of 100 correct for linear models but for non-linear models you know whether this is exactly n is equal to D or not it's a 2d or the relationship is a little bit less clear but let's say suppose we think about relatively simple models and um when the number of data points is closer to the number of parameters then in this case you you're going to see a peak and then after that you have more data it actually helps I saw some questions or does it eventually increase again um so in the in the first year so in the first figure this this is a good question so I think I've seen empirically um um both cases so sometimes it does increase again a little bit but often not much and sometimes I just keep decreasing and sometimes it plateaus so so I think that's why people probably don't study that part that much yeah this one I think is also new like I think this actually the paper that first systematically discussed this is like 2020. um this discovering the same paper right at the peak there there is it's not molotone the fact that yeah I think at least uh you know they might you know like immersion learnings it happens so often that you know someone did something and then people in the community forgot about it um that that's possible but at least I would say like at least it's only until 2020 that people start to most people start to realize this and because of the that paper uh I think the paper is just called Model wise because this is data wise because you are changing the name of data points right so um okay so now okay this sounds like mysterious enough right so like a very very interesting um and what's the what's the explanations right in the last few years um people try to explain why this what happens right and I'll try to risk ourselves with our old um understanding about this um and also this is an important question because this regime this blue regime is actually um actually you cannot you can it's a it's not clear whether when you run like a classical linear models I don't think necessarily you're in this regime but at least it's pretty clear that um it's at least more it's more um true that uh for for deep learning you are basically always in this regime at least it falls I guess this is still you know it's not it's never nothing is never universally true but I think for most of evasion experiments you are in this regime where you have more parameters than a data points so so this is something that is really like a empirically relevant so that's why people really care about it so um and maybe another thing I I need to clarify is that I kept I think I I was I think I probably mentioned that you know this the study about the linear models the phenomenon on linear models is um uh is uh is more kind of clear like there are a lot of studies and we have pretty good conclusion and what I mean by that is that even within linear models you can try to change the model complexity so what that what that means is that you just insist that you always use linear model but what you change is that you you try to decide how many features you use so you can start with only using one feature or two features like for example in the house price where you can use the square foot as the single feature or you can collect a batch of other features right so keep adding more and more features that means you have more and more parameters right so so even within linear models you can still change the complexity just to clarify that so I know and most of this theoretical study I think are for linear models and they are pretty precise these days um and I'm trying I'm going to try to kind of roughly summarize the intuition from the study of this double design so the intuition I think um I'm gonna list a few I've done so foreign so some intuition and explanations and these explanations are mostly for linear models um so I think the first thing to realize is that this peak so you can I can argue what is the most exciting or surprising thing about this graph right so but let's let's first talk about the peak right this is a peak in the middle so I think the the first thing is that in some sense people realize that the existing algorithms um especially if you just talk about for example Simple Green descent or sarcastic with instance for linear models so the existing algorithms so underperform atically um when n is close to the so so it both these two peak are basically like this right so here you are changing n the number of data points and you found that when n is close to D you have to pick and here you are changing the number of parameters you are changing D the number of features you use and we've realized when D is kind of about n about the number of data points you you have to pick so both of these two picks two peaks are are showing up here it's just that you are changing the axis in something so this is also when n is close to D when the number of number of data points is closed number of features so so and and the the explanation is such as the algorithms the existing algorithms or the algorithms you are visualizing here right so when you visualize this right you do you do ranks part some you do use some algorithms to learn the parameters so that particular algorithm that you use to produce this graph is it really underperforms very automatically it doesn't it's not really saying that when n is close to D the the real test I will shoot with this it's just saying that this algorithm is bad if you change your algorithm you probably wouldn't see this peak so so that's why the peak shows up so um and okay so so and what's what's wrong with the the by existing algorithm I really just mean that for example some just basic reading decency so um so for linear models maybe this is let's say this is for linear models so so what goes wrong with the so-called existing algorithm right so this um basically green descent algorithms so um the the what goes wrong is that the norm of the the the the Theta the the linear models you learned uh it's very big it's very big uh when is roughly equality so and and I we kind of believe that this is at least a partial reason for why this leads to a peak so this gives the peak so even though um so okay so I guess let me draw something here we have some real experiment real data in the lecture notes but if you draw that Norm foreign so suppose you you change the number of parameters which means you add more and more features in your you know in your data set uh and so that you have more and more parameters and you if you visualize the norm in the y-axis you're going to see something like this and this peak here is roughly corresponds to and it's closely and which is kind of similar to these Peaks so so basically even though suppose if you compare this this experiment in this experiment right so so here you have more parameters than this than here but when you have more parameters maybe sometimes you have lower smaller Norm so the norm is close to D for some reason it's just very very big and there they are actually we know the reasons the reason is that some random Matrix you know is not well behaved and it's close to D but I guess we are not going to go into that but at least the immediate reason is that when n is close to D somehow this algorithm is producing a very large Norm uh uh classifier Theta which is you know you can argue that in the norm is if the norm is too big then your model is too complex so so in some sense this is saying that if your model is actually very complex so very complex uh according to the norm so this model it simulator doesn't have a lot of parameters compared to for example this model so if you compare this model and this model so this model seems to have less parameter than this by definition the norm is actually very big so in some sense if you use the normalizer complexity actually these Peaks have large complexity exactly 0.1 to give us that's a great great question so so so you got that you know I'm implying that the norm seems to be a better metric for the complexity right so what is the right measure for complexity so this is a very difficult question like for different situations you have different answers um um so so but you know there is no Universal answer but Norm could be one uh complex match in some sense the norm is also a way to describe how many like suppose you have a small normal right so you have fewer choices to to fit your data in some sense so you have a few degree of Freedom if you have you know in like you have a few options in some sense to fill your data so that's restrict the complexity and which Norm that's that's actually um for different situations you can argue which Norm is the right complexity actually there's probably no Universal answer but but well I guess what I'm trying to say here is that the number of parameters is also not necessarily the right complex measure because if you have more parameters suppose all the parameters are very very close to zero that's probably also a very simple model because those parameters are not really working right so so but if you have just a few parameters but the norm is really really big maybe you can use the the right maybe it's used to also called quality very complex so you know my short answer is that there's no Universal uh answer to this um um the the point is that you know probably the number of parameters is not the only complex dimension so and for linear model it just happens that for mathematical reasons I think L2 normally behaves really nice like it seems to relate to a lot of like fundamental properties like maybe you can argue L2 Norm is useful because you are measuring a square error in many cases and it's it's nice with the linear algebra so and so forth okay so I guess let me I'm running a little bit late but I think I'm almost done here so [Applause] um right so so here it's just saying that at least for this case it sounds like Norm seems to be a slightly better complex dimension and and actually if you um and you can test this hypothesis in some sense so you can say that okay I'm saying here the existing algorithm underperforms but if you have a new algorithm that's uh regularized suppose you regularize the um the norm I guess I haven't told you exactly what regularization means but here just what I mean is that you you try to find a model such that the norm is small so so you add an additional term that tries to make the norm small so you don't only train on the tuning loss but also you try to make the norm smaller then you're going to see something like this so so regularization would mitigate this to some extent I'll discuss more about regularization in the next lecture but here just it really just means that you you don't you don't only care about tuning loss but also you try to find uh uh a model with small knobs so you and you have some kind of like balance between them right so you can sacrifice a little bit of training hour but you insist that you're normally small then you can see this right so so that in some sense explains partially why you had a peak because the peak is caused because your algorithm was sub-optimal right your algorithm didn't use the right complex measure and you can fix that Peak by adding more but there's one more question which is you know there's no peak but why there's no ascent right so so suppose you just see this right well actually here you also you will also see this something like this so this figure is actually pretty reasonable because if your data point is increasing you probably should just have one decrease like you just keep decreasing your um or you you just keep decreasing the the tester right so this one let's say we are okay with that we're happy if you see just a single um single decrease but here you're supposed to see a single descent right I I feel like it's still you know it's kind of arguable whether you're you should be happy with this answer because um why when the number of primary is so huge you can still generalize right so why when you use for example a million parameters and you just have like five examples why you can still generalize why you don't have a Ascent eventually in many cases you don't have a sign and in many cases the best one is just you have more and more parameters so um and and actually for example another question is that when number of parameters is bigger than the number of data points you know sometimes you are thinking this is still you have too many degree of freedom to fit all the um specifics of the data set you shouldn't generalize but actually in particularly you do work pretty well so that's the that's the last uh in sometimes the another missing point a missing part and this part um we also have some explanation for that and the explanation is that so when any is very much much bigger than d [Applause] sorry D is much better and the number of parameters is much bigger than the the D that the uh sorry the much bigger and the name of data points so the thing is that even though it sounds like you are supposed to overfit but actually the norm is small but why the norm is small right why when you have so many parameters you still learn very simple model the reason is that somehow there's a um there is a there is a uh some implicit regularization effect which makes the norm small so so so when you when we applaud this right so all of these experiments didn't have any regularization I didn't have any explicit encouragement to make the norm small so that's why the norm here is very big but why the norm here is small the reason is that your your optimization algorithm has some implicit encouragement to make the norm small which which is not used which is not explicitly written in the in the loss function so um and that's something I'm going to discuss uh I think more next time um um right so so so for this lecture I think I'm just so so we're going to discuss this more uh next time so the um yeah the high level thing is just that something else is is driving the norm to respond next"
"Stanford CS229 Machine Learning I Feature / Model selection, ML Advice I 2022 I Lecture 11","Stanford CS229 Machine Learning I Feature / Model selection, ML Advice I 2022 I Lecture 11

advice happened it happens that I write too small font please feel free to stop me and let me know it's just uh as I said every after a few lectures I stopped uh I I start to forget about it so please remind me um so okay so I guess um first let me briefly reviewed the last lecture just very quick so last lecture we talked about these two important Concepts um under fitting and overfitting so I got some so so here you know our goal is to make the organization work right so we want to generalize two unseen examples so um and last time we talked about two possible reasons for why your test error is not good enough right so one possible reason is overfitting so overfitting means that your tuning hour is actually pretty good your children loss is pretty small but their test loss is pretty high so and we have discussed the possible uh reasons for why um you can have overfitting and two possible reasons are if maybe you have two complex of model for example last time we discussed that you know if you use a 50 degree polynomial for this very very small data set where you only have like four examples then you may overfit or maybe you don't have enough data right if you have more and more data um if you have like a million data then a 50 degree polynomial wouldn't be a problem and also we discussed that um another reason underfading so under fitting is much easier understating in some sense basically just means that you don't have small enough tuning loss or tuning error right so your model is just not powerful enough so that you cannot even fit to the training data you have so so and in some sense these are kind of like two complementary situations right so in this case you probably want to make your model more expressive and in this case you maybe you want to make your model less expressive or less complex so we use this word in your complex expressive you know a lot you know without the formal definition right so we say some models are more complex you know some models are less complex typically you know you can somehow feel it right so a fifth degree polynomial probably is more complex than linear model but actually you know if you really want to have a you know concrete definition it becomes a little bit tricky right so what is the right complexity measure of the model someone asks about that as well in the last lecture you know and the answer is that there's no um Universal measure for what's the right complex model complexity measure and and there are a few you know um um kind of like a complex financials people often use you know they all have their kind of like particular strands and they are like and and also like there's no um you know real kind of like a formal Theory to say which one is better so these are kind of complex measures that can be you know um theoretically kind of Justified in certain cases but they are not kind of like a um Universal so so what are the complex measures so I'm just listing a few just to follow kind of like for your knowledge in some sense so so I guess the most obvious one is how many parameters they are right so if you have more parameters than your model might be more complex and this is you know very intuitive however the limitation here is that maybe you have a lot of parameters but you actually the the effective complexity of the model is very very low maybe all the parameters are very very small then maybe you can say in this case maybe the complexity is actually not as big as your thoughts so so to kind of like a deal with this kind of like scaling thing right so what do you feel complex your all your parameters are basically zero even though you have a million parameters then people consider kind of like Norms of the parameters right okay but this may not be no this is actually typical the Norms of parameters is actually very good uh they are very good complex measures for linear models um and and this there's no basically before deep learning kind of like uh arised um um like uh you know before I think we are using Norms as complex measures a lot um and still we use them in some cases but these also have some you know limitations for example sometimes you may for example would be that you have a very low Norm a low Norm solution and you add some random kind of like a noise to the to the model and when you add the noise you make the norm bigger but actually the noise doesn't really change the complexity right because you add some noise and when you take the Matrix modification you average all the noise to some extent so so these are you know there are also this kind of like issues so some of the other kind of more more than complex measures people have considered uh are for example something like lipsteousness right whether your model is ellipselessness uh it's Ellipsis or maybe your model is smooth enough and here I'm using the word smooth uh you know relatively kind of like um informal way you know you could mean this could mean the bound and the secondary derivative it could mean the bonus third of derivative something like that if your model is kind of like no it's kind of like oscillating or kind of fluctuating a lot maybe that means it's not very complex um and there are other kind of like a complex measures for example how environment your model is with respect to for example certain translations certain environments um that you should have in a data set for example whether your model is environment to data augmentation um but in general there's no kind of like a very kind of like established theory on what is exactly the right complex measure and sometimes it also depends on the data as I would see as you will see today so so if you if your data you know sometimes for you know for example suppose your data um um for example let's talk about Norms right so different Norms you know what what type of norms are you talking about L1 Norm L2 Norm sometimes L2 Norm is the right complex measure for certain type of data and sometimes L1 Norm is the right complex measure for a certain type of data so basically I don't think this is kind of like there's anything super concrete we can um like it's not like I have a kind of like just a fixed like a suggestion for you to consider so so in some sense you should just keep this in mind and kind of like a kind of consider them you know when you um when you do your own data set so um so so this is the um okay so now we have to discuss the complexity measures and now that in the the the the the rest of the lecture I think I'm going to cover two things so one thing is that so once you have some kind of like guess on what's the right complex measure you are looking for how do you make the complex measure small right so so um how do you encourage the model to have small complexity right so it's easier to do this because you just change how how many kind of neurons or how many kind of like hidden variables in deep networks where you can change the number of parameters but if you want to change the norm what you do so that's called regularization I'm going to discuss that in the first half of the lecture and then in the second half of lecture I'm going to talk about you know how do you um I'm going to talk about you know some kind of more General ml device for example how do you tune your hyper parameters like when you do regularization or when you kind of like choose your model complexity right you can use a lot of hyper parameters meaning you're going to choose how many parameters you have you're going to choose you know how strong your organization is so so how do you tune your hyper parameters um and on what data set you should tune your hybrid parameters and at the end I'm going to probably spend 30 minutes on even more applied angle about some email or device right so like for example how do you design the ml system you know from scratch you know there are a lot more things in reality more than what you're doing research um so so that part will I'll use some slides to talk about some general no ideas on how to design ml system uh in in the in the in reality so so that's the general kind of introduction of this course of this lecture um I'm going to start with regularization I need questions so far foreign I think we have probably mentioned this um I think in the sometimes like in the previous kind of lectures because just because you know um just because you know uh we have mentioned this you know informally so by regularization mostly we just mean that you add some additional term in your training loss to encourage low complexity models so um so for example you so we use J of theta as our training loss and then you consider this so-called regularized loss where you add a term Lambda times R Theta so here this R Theta is often called the regularizer and Lambda is you know I think there are different names for this but you could call it regularization strengths or regularization coefficient regularization parameter regularization strengths whatever you call it if let's call it regularization strength so this Lambda is a scalar and R of theta is a function of data um which um which will change as data changes so um and this and the goal of this Rotator is to you know either additional kind of like encouragement to find model Theta such that are obviously small so for example typical R of theta could be um something like you know L2 regularization so you say R Theta one possible choice is that you take this is probably the most common choice um you take the L2 North square and you multiply about half the half doesn't really matter this is just some kind of conversion um you know that because anyway you can multiply a Lambda in front of it so so whether you have half or not here you know we just change your choice of Lambda but just uh this is you know um just a convention um and so so this is called you know L2 regularization also in deep learning people called Ray Decay there is a reason why people call it way Decay I guess probably I wouldn't have time to discuss it today so you know the lecturer knows there's a very short paragraph you can see actually if you use this regularization and update rule will look like a very Decay so there's one step in the update rule where you Decay your parameter will shrink your parameter by scalar so but anyway so you know it's just a name like either it's called vertical you call it Auto regularization so let's play one of the pretty common one so and and you can see that if you add this thing to your loss function and you're minimizing your loss function so then you are trying to make both the last month and also make the the L2 Norm of your parameter small and the Lambda in some senses kind of kind of like controlling the trade-off between these two terms right if you take Lambda 2 back then you only focus on the regularization you just only focus on low low Norm solution but maybe you don't fit your data very well if you take Lambda to be you know for example zero literally zero then you are not using your regularization you are just only fitting your data and actually when you make the Lambda very very small this can still uh do something right so even say let's say Lambda is .0001 Right very small still this might do something because maybe they are multiple Theta such that J of theta is really really close to zero or maybe even literally close zero so so if you don't have this then you are not doing any type working right if you don't have if you literally make lambda zero then you are just picking one of the solution where J Theta is zero but you don't know which one you pick but even as long as you add a little bit of organization then you are using this as a tie breaker in some sense so so you are finding some solutions such as J Theta is very very small but you use the the r the norm as a tiebreaker uh among all of the solutions that have very small tuning laws so so this is probably the most typical regularization people use and another one is the following so you can take R Theta to be uh the so-called zero zero Norm of the parameter but actually this is not really a norm this is just a notation so this is really just defined to be the number of non-zeros in the model in in cell so you count how many non-zero entries in Theta and that's the uh that's what this notation is for some sometimes people call it zero Norm but it's actually not a norm uh literally it's just the number of non-zeros in a parameter and sometimes people call it sparsity because you know if you have very few non-zero entries then it sparse otherwise it's it's dense so if you add this to the the thing then you're gonna have a different effect you are trying to say that I'm going to find a model such that the number of non-zeros in it is small and this is particularly meaningful for linear models in the following sense because if you think of the state as a linear model then say you have Theta transpose X right suppose you have a linear model and then what is this this is really just some of say the i x i from one to D and then you can see that the number of non-zeros is really how many if suppose you have like s non-zero answers in Theta that means that you are using only s of the coordinates of X size so basically the number of non-zeros is in Theta is the number of coordinates or number of features you are using uh from x i right so you can imagine that maybe for example for some applications you have a lot of like a coordinates in our features in your input features right so you have so many different kind of like informations but you don't know which one you should use to predict right for example suppose you want to predict house the price of a house then you have so many different features but some features may not be that useful so then you can imagine that that could be a situation where you should use this as a regularizer because you want to say I want to use as few features as possible but also I want to make sure my training loss is good right so I want to find the simplest explanations of the of the existing data or simplest meaning that you want to use as few features as possible so and and once you find this Theta such that Theta sparse right so suppose you have a Theta such that you only have a few non-zeros uh in Theta then you are selecting the right feature so in some sense you know you have a sparse model means you are selecting the features right because those non-zeros corresponds to its features that are selected by the model so so people often call this kind of like a feature Selections in some in certain kind of contexts so um however I mean you may have realized that this regularizer as a function of theta this sparsity one is not uh differentiable right so you're just counting how many non-zeros they are right so suppose you have one suppose you have like you have one entry that is um maybe it's just zero zero zero right you do an if the testimo change in this uh in any of the coordinates you're going to change the this function the value of this function by a lot so that's why it's not differentiable right a differentiable function should satisfy that if you change Theta if the decimal is small then you should change the function output by a small amount but actually here if you currently the sparse is zero but if you change the state a little bit your sparsity becomes one so you can have infinite testimony small changes to make the regularizer value change by a large amount so that's that's why it's not differentiable so so it's not because because it's not differentiable then um you don't have gradients you don't have like a derivatives so that's that cause the problem in in using this so so basically even though I told you this is a regularizer but literally in reality nobody use this exactly in their algorithm because if you use it you get this you know you put it here but this term has no gradient how do you optimize it so so if because it's non-diffensible so then what we will do is that um you like you change um you have a surrogate so this is a typical surrogate um the reason why this is surrogate is a little bit kind of tricky um but um um but you know but this is you know has been your surrogate for the sparsity so this is a differentiable surrogate um for the for the sparsity of the model so you use the one Norm so here one Norm just means the the sum of the apps value is the sum of the absolute value of each chord so and you can see you know I wouldn't attempt to give you a very formal justification for why this is so good um why this is a good Target for the zero um you know one reason could be that you know you can see at least one Norm is closer to than two Norm to zero Norm right Y is close to closer to zero than two and another reason probably you know this is not really very solid mathematical reason but it's just to give you some you know intuition so suppose you think of theta as a as a vector in zero one suppose you really just have a binary Vector then indeed this is equals to this right so that's that's probably another kind of intuitive reason why they are somewhat kind of like related but you know you can see a lot of problems with this argument right so why I'm assuming Theta is between is only taking value from zero and one right so if they are from zero and two then these two are no longer like related and more so um so so I'm not saying that this is really a good argument for why they are related so so if you really want to I'll say they are related um or this is a good surrogate I think you have to go through more much more mass um any questions so far regularizer with the second nominal regularly you mean yes okay yes that's what I'm gonna do next so um all right so right so that's a great question so why um you want to encourage sparsity um sometimes and sometimes you want to encourage the two Norms to be small right so let's answer that question you know let's think of this as a as a surrogate for the sparsity so the question I'm trying to answer is why sometimes this is better sometimes this is better so in some sense the the fundamental reason is just that you know in some sense the regularization or another way to think about regularization instead of just encouraging low complexity is that regularization can also improve can also impose structures kind of like prior beliefs about the Theta so so this is probably another at least you know one of the other ways to think of requisition this also imposes structures and this what are the structures the structures you know probably sometimes units you have a prior belief so for example suppose you believe you you have a prior belief because of domain knowledge right so such that um you somehow believe that uh Theta sparse right so in in smart SATA then in this case you probably should just use R Theta is the the one Norm or the zero Norm just because you believe that your model is sparse why not just encourage that right so so this you know when you have this belief then you should say okay so if you if you encourage the one normally you're you're in some sense you you limit your search space right so yeah like you limit your search space before you are searching over all possible parameters and now you're only searching over low Norm low one non-parameters and because you believe that your true model is in is is having low Norm then even narrowing the search space is always new never narrowing your search space is always helping you because uh you didn't lose anything right because you know you know that all every model you excluded are not going to be uh the right solution so so you never search space the search the new search space still has the right model then why not do it so so that's another interpretation of um of the uh of the regularizer so it's trying to it can impose additional prior belief um uh in the in the in the structure of the model right so if you believe in one Norm then you should you encourage well not or whatnot if you believe that your true model has a small L2 Norm then you should encourage um small L2 naught and and you know if if you go into more mathematical uh Theory I think L2 Norm typically corresponds to situations where you believe that all the features are useful but you have to use them you know uh in a combination right so you have to use each of the features you know a little bit an L1 Norm or l0 Norm typically corresponds to situations where you believe only a subset of the features are meaningful and you should discard other ones because other ones are just kind of like are there to confuse you in substance so so if you believe your model is his sparse you should use L1 norm and if you believe your model shouldn't be sparse then typically people use l tuna so and and if you have a linear model so suppose you have a suppose you have a linear model and and then this this law this loss if you use this L1 normalizer this is called lasso I guess I'm here I'm just the final name because I think it's probably useful for to at least a third of this name this this uh acronym uh was actually I don't know what it originally stands for but you know this is uh so like this has been like there for like 20 or 30 years which is a very very important algorithm for linear model you apply L1 uh norm regularization and it's called lasso in everyone in machine learning should know the equity so um right and taking a little more broader perspective so um so if you think about like a non-linear models like deep learning models so what are the most popular regularizers these days I think L1 Norm is not used very often and actually pretty much it's never used um you know I don't know exactly you know how frequent it is but you know I think probably less than 10 percent of model use L1 recognizer maybe even less than that 10 probably is much overestimate maybe one percent so um and but the L2 regularization is almost always used like even though sometimes you only use a very weak Auto regularization indeed I'm talking about deep learning model so sorry maybe let me just clarify for for linear models people tried uh you can try almost anything anything would be reasonable and you probably should try all of them you could try one Norm two norm and sometimes you can try different Norms which I didn't write down but you can try 1.5 Norms something like that so for nonlinear model for deep learning models I think basically L2 Norm is something that you almost always use but you only use of it with relatively small Lambda people generally don't use very large Lambda I don't know exactly what's the reason you know um researchers don't really know that much either but a small Auto regularization is typically useful for deep learning and in deep learning I think some of the other regularizations you know could be useful for example you can try to regularize the lipsense of the model um and you can try to use data augmentation which we probably haven't discussed I'm going to discuss that um in a later lecture but you can use data augmentation which tries to encourage your model to be environment with respect to a kind of translation cropping this kind of things for for images um I think those are pretty much the the only regularization techniques in deep learning um yeah um [Applause] this kind of pertains to my employer yeah what you suggest initially using the and one day to kind of eliminate the features that's a very good question so I think this kind of algorithm was pretty popular in uh um for before deep learning era so when you use linear models I think using L1 to do a selection and then you use L2 I think you know I don't know how exactly how popular they are but this is definitely one algorithm people try to could you could try to use um in deep learning I think it's probably less likely to be useful you know but also depends on the situation for example if you don't have enough data maybe you are more or less in a linear model case but you just need the nonlinearity to help you a little bit maybe then in that case you should still mostly use some kind of like more like linear model type of approach if you are in the the typical deep learning setting for example you do for a vision project right you have like images right as your inputs I think in those cases you probably don't want to select your features first I think all the inputs are useful like I'm able to use them as much as possible and you just want to let the new light works to figure out what's the best way to use those inputs any other question by the way this lecture will be pretty kind of we don't have a lot of math right most of the things are about um just uh I I don't think there's even a theory here sometimes they're just experiences because especially if you talk about um like the more the Machinery in the last five years right everything seems to change a little bit right so I like I cannot say anything with about 100 guarantee I can only say okay it sounds like people are doing this a lot and that's that's the best thing I can I can tell you in some sense um so so feel free to ask any questions any other questions right so and the next thing I'm going to discuss is the so-called implicit regulation effect and um this does uh this released more to the the Deep learning and so one reason uh that people started to think about this is that you know I haven't told you what exactly means so one motivation that people started to stand up for research is that people realize that in deep learning you don't use a lot of regularization technique right so you use L2 as I said you only use a weak out to regularization and and often some of these latest ones but they only have a little bit right they can be useful but people don't necessarily use them very often so why in deep learning you don't have to use strong regularization at least like you can feel that the regularization stop method stop matter from that much it still matters when you really care about the final performance you care about 95 versus 97 but but you don't have to even you don't use regularization sometimes you get reasonably good performance so so that's why people are especially with theoretical researchers people are wondering why you don't need to use a strong regularizations in deep learning and this is particularly mysterious because in deep learning people are using over parametrization like we are we are in this regime where you have more parameters than the number of samples we call that in the last lecture we have drawn this uh double descent thing right where you have this kind of things right here is the number of parameters and this is the test error and you know we have kind of discussed that this peak might be just something about the sub-optimality of the algorithm which let's say you don't care for the moment but at least you have to care about why here um why it's go going down you know still here right so why when you have so many parameters a lot more parameters you can still make your model generalized and and it seems that more and more Paramus makes it looks better so so so this the overall primary Choice regime is kind of mysterious because you don't use strong regularization but you can still generalize so that was the kind of the motivation for people to study this and people realized that there's you know even though in this regime suppose you don't use any expensive regularizer you don't you make that lambda zero literally zero right in this regime still it can generalize and the reason it can generalize in many cases is because um you can still have some implicit regularization in fact even without explicit regularizer and where that effect comes from we know what kind of make that happen the reason is that um the optimization process the optimization algorithm the optimizers can have a con implicitly regularize you so so why this can happen I think the reason is that let me draw kind of like illustrative kind of figure which I kind of like use pretty often so suppose this is the let's say this is the the pyramid like suppose you have a this is the Lost landscape the Lost surface so meaning that here is Theta let's say is one dimensional and because we are in this deep learning setting where we have like a non-linear models and non-convex loss function so maybe a loss function it looks like this maybe so this is the loss function and you have two maybe you have multiple Global Minima of your loss function right so so this is a global minimum this is a global minimum and but this but you have multiple Global minimum in your loss function however I'm here I'm talking about trending logs right if you really look at the test loss you're going to look they will look a little bit different the test loss would be different from the training loss so test loss maybe look like something like this maybe okay let me draw something I'll come to my figure so that um so this is the training loss now test loss probably look like this so that means that even though both of these two Global Minima are um are good solutions from the training loss perspective one of them is better from the test test performance perspective Right This Global minimum is good and Better Than This Global minimum because the test performance is better so and in some sense like the regularization effect is trying to choose the the right Global minimum right you want the regularization in fact to choose the right Global minimum so so that you can do some type working or you can encourage certain kind of models maybe this model is more kind of like ellipses so this model has more Norm than this model so that's why you prefer this one so if you use explicit regularization what you do is that you you're going to say I'm going to change the tuning loss I'm going to add something to prefer this one than this one I'm going to reshape the tuning logs right that's what the explicit regularization would do but the implicit regularization we'll do is the following so if you consider an algorithm that optimizes for example suppose you have you run algorithm this algorithm which is always initialized this is initialization and you do green in this set so you're gonna do something like this and it converts to this one so so this algorithm will only Converse to this one but not this one just because you initialize at this far right right so that is kind of in some sense a preference to convert to This Global minimum over this Global minimum because your algorithm somehow prefer one Global minimum than the other just because your algorithm has some certain specifics right so the initialization make it to prefer to converge this one and and there could be other kind of effects for example if you um use you know bigger step sets maybe you are more likely to converts to this one maybe or maybe vice versa you know depending on on kind of like the situations right so this is a very illustrative thing with one dimension then you don't really have a lot of like flexibility here but if you have a very very complex thing then if you run different algorithms different algorithm will converge to different Global minimum and that preference to certain type of global minimum is in some sense is a regularization effect um so so that you don't converge to arbitrary Global minimum um does it make some sense you said so how does like having a large number of parameters ensure that it initializes at that point yeah I I was I was silent on that in some sense like I didn't really say why the initialization has to be here like this is a active area of research so what we are sure about is that the algorithm could have this effect the algorithm could possibly um prefer certain kinds of global minimum than the others but why it would prefer which kind of global minimum we don't exactly know for certain kind of like toy cases we know but for uh for the general cases we don't I'm going to show you one cases where we actually can say what does the algorithm prefer to do but that's very very simple case for General case I think the the research this is still very open research on question I saw two other questions here this seems like of the authorizer is keeping a number of problems it doesn't quite necessity no no here that no no the what do you mean by the name optimizers the access is the value of the parameter it's just they only have one parameter I'm just I'm joining the the landscape of the pyramid and I can only Draw Something in one dimension so so so this is the value of the parameter you are just tuning in this parameter you are doing good instant and this is the loss surface so so it does depend on where you initialize right so if you initialize at different places you're going to converge different Global minimum and and they may have different generalization effects different algorithms and then just choose the one that has the best performance that's pretty much the right to do um of course there are some I'm going to discuss this you know but uh more detail later but but you know basically you know like you can have some intuition where you have you know the theoreticians have tried to understand you know what kind of like algorithms can help generalization but I think the conclusion at least so far is not no no like it's very far from conclusive they can give you some intuition but they are not going to be like predictive you know I don't just tell you like what to do so you still have to try a lot yeah so yeah going back to this you know this is just one dimension you know another way to think about is that if you can think of like a two-dimensional question for example you are skiing in the in the in a ski resort right so your objective is basically minimizing your like uh you're trying to go downhill right that's your objective so and and this key result probably have a lot of villages right like that you can eventually go home there are multiple parking lots right so so you you like in some sense you are saying that you know one of these parking lot is great right so this one of this parking lot is really where they are so you want to go to that one um so so diff but but uh but different algorithm would lead you to do diff to convert to different uh different parking lots right so for example someone is doing very fast skin then when you do it that you cannot go to those kind of small Trails so then you lead you go to one of the parking lot and some other one prefers like um a wider kind of like trails and then you go to the other parking lot so so different algorithm will lead to lead you to different parking lot and different parking lots have different generalization performance eventually so um so this is the high level um intuition so I'm going to um let's see I'm going to discuss a concrete case which um which will also be part of a homework question so this concrete case uh just to give you a concrete sense of how this could even be possible so I'm going to show you the high level thing and there are some mathematical part which will be in the homework so this is the linear this is a in a linear model so interestingly even though this implicit requisition effect was mostly discovered after deep learning uh start to be kind of like powerful but actually you can still see it in linear models and that's how researchers start to do research so um so so let's say suppose we are just in the most vanilla linear model setting where you have some under data points this is just the the trivial linear regression and your loss function is something like just the L2 loss the screen squared error something like this you have a linear model but let's say let's make the the the one different thing is that we assume n is much smaller than d so you have very few examples and a very high dimension so what is d d is the dimension of the data and the N is the number of examples I'm going to assume only is much smaller than b so this is over parameterized you have multiple Global minimum while you have much so first of all you have multiple Global minimum why because they are I'm claiming that they are minus Theta such that minus Theta satisfies y i is equals to Theta transpose x i for all I why because you know how many equations here you have right so so if you want to so this is the equation to make training loss zero which is means Global minimum right so if you have all of this equality then it means you are either Global minimum of this training loss and why there are multiple status such that you can satisfy this that's because you can count how many equations they are right so there are any equations right and D variables and these are linear equations right so so I guess the linear algebra tells us that if you have any equations T variables and if n is less than I think if n is less than D or D minus one I think only is less than D then uh you're gonna have at least one solution and if any is much much smaller than D then you have a Subspace of solutions and that's called the uh the what's the the kernels of the anyway you have a Subspace of solutions um uh for for this kind of linear system equations right so um and that's why you're gonna have much for Global minimum of the chaining blocks because the entire Subspace of solutions are Global minimum of the tuning loss so the question is which one you're going to converts to right so which one your Optimizer will will choose so it turns out that you know if you use a good InDesign with zero initialization then you are going to choose the one with the minimum L2 naught so here is the claim so the claim is that if you do good in distance with initialization Theta is zero uh this will converge to uh the minimum Norm solution so what does the minimum solution mean formula it means that you converts to a solution with the smallest L2 Norm among those Solutions such that those global minimum of the loss function so so we use green designs you are not only just finding a Theta such as the loss function zero right so typically when you think about optimization the optimization is trying to find a solution such that the loss function is minimized right that's true you still find you definitely find a solution such that the loss function is minimized but you also have you actually have a tight breaking effect among the solutions such that the Optimus the loss function is minimized you actually choose the one with the smallest L2 naught so guys you know in some sense you know the kind of intuition is the following so I'm going to try to draw this this is a little bit um I need to try to draw this well um so suppose you have a suppose let's say the intuition is that supposed to let's say you have n is one um and say these three so you just have one equation one linear equation and um so and you have like three variables so that means that the family of solutions is a two dimensional Subspace so make sure to draw this okay okay so so here the Subspace I'm drawing here is this is the family of theta such that you satisfy that the loss is zero right this is the Subspace right so you have a substrate Solutions and but which solution you converge to that's the question it turns out that if you if you start with let me see what maybe I will write here um it turns out that you're going to find a solution such that this is the solution of the fund this is the solution drawing this is a little bit challenging I guess how did I do this I think I did this so so you consider that you project zero to this Subspace right so that you find this point this point is the the solution with the minimum knob that is closest to zero among on the Subspace and this is the solution that you you will find you're not going to find other Solutions with good in descent on um with initialization zero so basically that's the claim the claim is that you're going to find this particular solution but not the other Solutions and the reason is actually fundamentally reason it's pretty simple like especially if I draw it in this way of course if you want to prove it it's a little bit more complicated um so the reason is really just that you start with zero this is uh where you start with right we need that and you have a property such that when you how do I um you need a series this so you have a property such that if you start with if initial is zero right and then at any time so uh your Theta is always in the span of all the data points here I'm gonna have actually one data point so I'm only so um so so basically your Theta cannot move arbitrarily in any places you only have you have a restriction on where the setup can go so actually for this particular case what happens is really just that you are just moving it along this direction and here you'll find this point that has the Subspace and that's what the gradient design is doing so green in design will not do something like this will not converge to here it will not converge to here it will just go directly go to this this mean this closest point where the point that is closest to zero on the Subspace so so this is this is probably a property of the optimizers right you can imagine you may have optimized other Optimizer suppose you design some crazy Optimizer which does this or does this then you will convert to a different points but if you use Golden design you're going to do this you show that green inside is doing this is just by saying that the good instant is always in a soft the span of the data I think this is uh this is actually something we have we have approved for in the kernel kernel uh lecture I'm not sure for a different purpose you know it's not for this purpose remember that in a kernel lecture we try to show that your parameter is always in a linear combination of the data and and then there the purpose was that you want to represent it by the betas in that lecture so it's a different reason but it's different on goal but it's the same fact right your status is always in a span of the of the data [Music] is defined to be the all the solutions that have zero loss so these are all the spelling is that's my definition of the Subspace this is the family of solutions that have zero training loss so and the question is which one I'm gonna convert to in kind of like I was arguing that you know they're much for Global minimum right so this whole spine is our Global minimum all of them are Global minimum and which my own converts to so so different algorithm probably would convert to different points so if you run cooling design you're going to converts to one particular one in this in this bag right but but this um um this this phenomenon also shows up in other cases but it's going to be you know much more kind of complicated like like I think they are only a very limited number of situations where we can theoretical proof where you converge to um but but it's almost always the case that the optimizer has some preferences the optimizer will not converge to arbitrary zero training loss solution it will converge to one particular zero General solution and sometimes that solution just to General is much better than the other ones [Music] so um so it's not only one or linear models or right so so this so only for linear models the final of zero zero loss uh solution is a spot right so if you have non-linear models then the family of solutions satisfying this wouldn't be a spot maybe it's a manifold something some other weird structure uh right so so in that sense this is very special Solution that's gonna be in that's bad it's going to be the constraint optimization problem that we just saw that like it's going to constraint itself to this like minimizing a second right right so so I didn't show you the full proof so this point turns out to be the point that you converts to turns out to be the the minimum solution and it turns out that you actually just going straight at least for this at least one case so you know not it's actually it's not even always true that you are going in a straight line like some but uh but you always go in this Subspace um so I'm answering the question maybe I didn't um can you prove that yeah you you can prove that prove it yeah I think the homework question I actually asked that you're gonna converts this this Pawn is exactly the minimum normal solution and also you're going to convert to that oh okay actually you can have a actually you can have a pretty concrete representation of this point right it's really just some inverse some of the Matrix times something you can you can compute what exactly is and and you can show you converge to the point um I'm not sure whether the homework asks you to so I think the homework has to show both but we will have a lot of hints you know along the way it's not going to be interested show this that's it and maybe for example another just to give you a sense on you know what these kind of things can change right supposed to initialize here then you wouldn't converge to here so you probably would convert to somewhere here and you know and so uh and if you use stochastic reading is that you probably wouldn't commercial exactly here either your power will convert to some somewhere definitely so so so where do you exactly converts to it it's it's a very hard question we don't really know uh we only know that this this the only thing we know right now I think formula is that this matters if you use different algorithmic cucumbers different solutions and different solutions generalize differently so you have to consider the effect of the optimizers and going back to this the reason here is really so I guess like a this in some sense this kind of is trying to explain why you can you can challenge here that's because because of this implicit requisition effect even though like you don't have regularizers you still implicit regularized L2 Norm so and that's why in this regime even though you have a lot of parameters but actually you are still implicitly regulating out norm and if you look at the norm the norm would look like this so this is the norm as you change the parameter so so basically this is saying that when you have a lot of parameters actually your real Norm is actually relatively small and that's why you can generalize so um so so the the the reason why you don't generalize in the middle is because this minimum normal solution is not actually doing well in the middle for some other reason so um so the norm actually turns out to be big but actually the norm is very small in the over parametric 3G even though you use a lot of parameters thank you Okay so so now let's talk about you know um how do you really do um how do you really find out what's that you know I've told you that we don't know too much about you know what how does the optimizer uh change things right so we also don't know exactly how does the model complexity uh uh change things right so you only know some intuitions right so you know that if you have more complexity it turns out to be more likely to over fit but you don't know exactly what is the right complexity right so how do you find out the right model the right optimization algorithm the right you know regularizer all of this where you have so many decisions where you probably have like 10 decisions you have to make in this machine learning algorithm so how do you find out what's the what's the best thing so so I think the the typical way is just that you are user uh validation set to um to figure out what's the best decision so maybe just to motivate that just briefly so the the easiest way to do is that you just use a test set right so you have some test set and you just you try all kind of algorithms all kind of models all kind of like a regularization strength and you see which one has the best performance uh tested so that's okay as long as you only use one you only use the test set at the end right so to try all of this algorithm in advance you know you you and then you collect some test the site or maybe you collect the test set before but you never touch it right so that's okay so you you so you if you only use the test test once then you can use the tester to evaluate the performance of all possible algorithms all possible kind of like models uh you you want to use so so that's that's that's that's that's that's a good thing so however the problem is that sometimes um you like you um you want to do this iteratively you want to look at a test set and see what the performance is and then you go back to say okay maybe I'll change my model size right or maybe I'll change my uh uh like my Optimizer right so maybe I'll change from Green instant to stochastic in science maybe I want to add some regularization effect I add some regularization on like a function so um um so if you want to do with iteratively then the what I said before was not going to work that's because you know so typically if you have a test set this you can only use it once so because if you use it multiple times what happens is that you are you could kind of like overfit to the test set so basically the your later decision becomes kind of like a over 3 are our decisions over fitting to the test set you have seen before so so the only the validity of the test type is only insured when you only suggested after you you do the tuning so and if you see the test set and then you like uh you you do the training and then you test it again then the second time you test on test that it will be not guaranteed to be valid so you may all over a fit to the test set so does it make sense I'm trying to be not over complicated this right so that's why I'm trying to use informal words for it but if there's any questions right so so how do we all deal with this right so the test set we can only use it once or at least we can only use it we cannot use it interactive interactively you can also see the test stats tune and then see your tester get set again so so one way to deal with this is that you um you um you you have a whole doubt or you have a validation set so so basically you split the data into a three part so one part is called training site and one part is called validation set and also test that and for test start this is your kind of like uh this is a very you have to be like a very careful about it you shouldn't touch touch it this test that is only only about the very very and you are using a test set to evaluate your performance so and but the validation side this you use this to tune hyper parameters and by hyper parameters here I mean all the kind of like the type of parameters that you are you are choosing for example the the batch Stars you know the the the Lambda in the regularization uh maybe the the choice of the optimizer the number of neurons you're going to use in the in deep learning in your deep learning model um how long you have you're going to train in all of these you know decisions um that you are going to decide in the in this process they are called hyper parameters and so you're using mobile edition set to tune the hyper parameters and you are using the chain inside to tune the real parameters right by the to optimize the parameters right so I guess typically we don't know so to tune the parameters right these parameters are just a numerical numbers right in the model which anyway you don't know what where the meanings are but the hyper parameters are those kind of things that you uh you know their meanings right batch size learning rate step size right so they all have some meanings uh and you want to use this validation set to tune the hyper parameters so so basically the kind of process is that you start with the tuning and then you you valid you start with training with some hyper parameters and then you validate on your performance uh and then you go back to to tune again maybe using some other hyper parameters and then you do this iteration for many times so and after you do all you are done with everything and you find out a model that you are happy with which you know by your happy with you I mean that you found another model that is very good on a validation set then you finally test your model on test set and that can be only done worse so so in some sense I'm not sure how many of you have for accounts in this kygo computation right it's kind of structured exactly like this so there is this online platform where people release their data sets for and set up some kind of kind of like challenge for people to submit their machine in the model to solve their tasks so basically you know kygo competition so they have a like the organizer I have a test set which nobody can touch at all like this test side is only the it's only used once at the very end when you decide who is the winner so but and then the the the the organizer released these two actually I'm not sure sometimes they give you a division right so they say this is advantage that this is the training side sometimes they just released the total um all of them to you and then you can you can divide yourself you know even the release in this format you can re-divide them you know whatever you want to do so let's say suppose you have divide your you know all the training example into these two sets you can do whatever kind of like um whatever kind of like uh optimization uh you want so and I think typically they do have like a desk in the validation side which is used to for the for computing the scores on the leaderboard whether there's a leaderboard which kind of like tells you how well you are doing against others at least temporarily right so so that's the validation side that's that's evaluate on the validation side but this leaderboard may not be exactly uh the same as the final rank it's possible that you know finally you found out that somebody is succeeding in the leaderboard but eventually in a very final test um the the performance is not like as the validations I suggest so so but but this is the general setup that people are doing um does it make sense so one common question you know that people kind of like generally ask you know which I ask myself as well is that you know how how reliable this validation site is right so like a like if you have very high performance on the valuation side should you trust yourself so on one side you shouldn't trust yourself you know 100 because if if you can trust the organization side performance why you need to test that well the test stat is supposed to give you the final verdict in success right it's it's a very uh like a it's a guarantee it's something that guarantees to give you the right answer so so the value set is never 100 you know you probably shouldn't 100 trust it so on the other hand in Prior college so people realized in the last five years that I think there is a sequence of paper on this people realize that actually the validation side performance you know is actually well card with the test set like so this is a reasonable indicator about how good your performance is on Tesla it's just not there's no theoretical guarantee that these two are exactly the same so but but in most of the cases if you don't do anything crazy you don't kind of like a somehow just memorize the entire validation Side by creating some kind of like um some kind of like uh like lookup table kind of things then typically a validation size performance the performance and radiation set is kind of very close to test that and this has you know there is a there's a very um important paper in the probably three or four years ago um by Berkeley people so they actually look at them like maybe 300K gold competitions and and they are like they look at the the best performance you know the the the rank of the performance on the validation side on the leaderboard and they look at how they correlated with the the final winner the final performance and they found that they are very correlated so which suggests that the validation side is actually a pretty good indicator for test set even though it's not guaranteed um and any and in this case the typical machine the uh the typical machine learning kind of practice is that you know if you look at the the um the um um like the the when people publish papers right so in some sense people publish results based on validation search um so for example if you look at image United performance in some sense people are uh like the the so-called test performance that people report is actually it's actually a performance on a validation side because that that so-called test set of has been seen so many so many times is I think actually it's lit uh I don't know exactly whether there's a label like name for it like in the image not uh the official data set but at least that's that you know that you report your performance um with that side shouldn't be considered as a test set because test that you should only use it once but actually people have used it so many times maybe a million times so so so basically abstract speaking I think these days when you publish paper you use the validation set only when you have the Kegel computation you use the tester to really decide the winner but but empirical it sounds like they are very close so so actually that's why we are not worried too much about it any questions oh I think it's this is a [Music] I think it's called hey go or kaigo I don't know how to be an artist uh this is a platform so the platform hosts a lot of computations maybe like um a hundred every year or something like that you can submit your model and sometimes there is a kind of like there is a price uh for winning the the competition so um right and by the way I think this validation is that sometimes now people call it development site as well um I don't know how popular this name is uh but at least if you say but addition said I think everyone would know what you're talking about development side I think most people would know as well but it's a relatively new term in the last five years trading certain validation sets as part of like a bigger so like once it's actually decided on what type of parameters you want to use right so how do you how do you do the split right so so here um so the most typical way is that you just split randomly you reserve probably a tenth of the data set as validation size maybe 20 you know depending on how many data you have so um and I think what you are probably thinking is the so-called cross validation which does something much more complicated you can kind of split your data set into uh you can do multiple splits and try multiple experiments on different splits so um I think uh um I I think I'm not going to cover it uh for this lecture mostly because I think these days if you have a large enough data set typically you just do this static spelled just because it's much easier you don't have to run your algorithm multiple times and this is just almost like the like like in most of the larger scale machine learning situations you just use this so but if you just have like 100 examples then indeed as you said you know if you fix like 20 examples that's validation side it's a little kind of like wasteful so then you have to do some cross validation so we have a we have a kind of like a section in lecture notes about cross validation um there's a there's a description of the the of the Practical um I think you know if you're interesting you can you can read it it's nothing very complicated either thank you okay so I got some um yeah so I'm gonna use the last tournaments to talk about some more um applied perspective um I'm gonna use the slides so I guess I'll see that this can work okay great so uh it's not censored is it wait okay sounds good um okay good so um so so I think this lecture um um so we're talking about some ml device so um I think I um so here on these slides are made by our other instructor crispy uh with the help of Alex Radner um I'm pretty much just repeating uh whatever um he's saying in the slides so um I think the slides you know used to be a little bit longer than this I'm going to release the longer version as well so I shorten it you know to only 20 minutes or 30 minutes um and part of the reason is that I think slides also contains something that has been covered on the Blackboard on the Whiteboard and also part of the reason is that there are some applied Parts on which uh I think we don't have a lot of time to discuss in um in this quarter so but I'm going to release the the longer slides as well for your reference so so so so this set of slides are mostly for kind of like a little bit more applied kind of like situations for example you are thinking that you are for example your startup and you are doing machine learning to solve some concrete problems right so um so it's a little bit less like a research because you know you're gonna see that you're gonna have more much more issues than a concrete research setting right in research actually you sometimes also have this right in research the most typical setting is that you probably have a con very concrete data set right you know that the input output you know everything there's no any room in flexible you cannot redefine the problem and you just want to get the best number that's one type of research I don't think this is the most typical one either right like but um but this is one type of research and then from there you can have more and more flexibility you can change your data you can refreeze your problem you can find out what's the right problem and and once you really do it in in Industry then it's going to be you know much more complicated so some disclaimers to start with I think this is Chris disclaimer which is also mine so these are you know like there's no Universal kind of like what is ground shoes here right so there's no ground shoes it's really just some experiences and some um uh some experiences from people doing right you know in real life so um and and things change you know over time sometimes people thought that was the right thing to do in five years ago and now things changed so I'm going to go through this a little quickly so um you know um so um but you know some I'm gonna omit some of details as well but feel free to stop me um right so so there are many um so in some sense there are many phases of IML project if you really do it in Industry right so so for example one thing you want to discuss is that you really need ml system even to start with right some of the questions are not really necessarily suitable for ML I think at least I knew I'm not I don't do as much industry uh work you know as um you know Chris is also entrepreneur besides you know um a professor um so uh so he knows a lot about this but even I know that sometimes you know people actually when they really uh sell their product as a ml system but actually the underlying system is not really using much ml so sometimes you don't really need ammo and and and when you need when you use the ml if it doesn't work you know what you do so and uh and and also like you know how do you deal with all the kind of ecosystem so and when I use the runner example you know we're going to have a Spam detector and the question is how do you know detect spams um um you know we'll use this example a lot in this like in this course so this is the seven steps for ML system so here again it's more a little broader than just the ml research right so you're thinking about designing a system that can actually work in practice so um so acquire data and you want to look at the data you know and maybe you want to create some kind of like twin development test set as we discussed you want to um Define our refund specification which I'm going to discussing in some sense this is saying that you have to have a evaluation metric for for your model in what sense you want to you want your model to succeed and then you want to build your model and try a bunch of models maybe you are going to spend a lot of time in step five and then you're eventually you're going to measure uh models performance you know not necessarily only according to the specification you have defined in step four but maybe you'll have like other match measurements for example speed training time so and so forth and then eventually you have to repeat and maybe you have to repeat a lot of times so um I'm going to go through these steps you know relatively quickly I only have like 15 minutes so but you can kind of like um if you're interested you can look at a longer slides as well so um supposed to say um you want to decide what is Spam or not so so ideally you want a data sample from the data that your stump product will be around right so you want to have your data to be somewhat kind of like closer to the the final test data right so you don't want to just collect some spam data from 30 years ago and then and use this data to choose something that's that can work not these days but sometimes this is not always available because you never know what what the spam emails will be 10 years after so you have to make some sacrifice um you know sometimes you don't even have the features you know it may be like your existing record didn't save everything maybe it just saves the title of the email didn't save the the entire content then that would limit your capability of detecting spec and there are many legal issues to look at the data um and this is according to crazy you know I think this is true as well so you're getting wrong on the first try right so like sometimes you like you'll find out that the data you collect are not the right one you have to repeat and then you know after collect some data you have to look at them right so and this is something that we actually don't really teach a lot you know in in this machine learning course looking at your data right because you know we are mostly assuming that you already have your data you you make the right assumption already you already know your data is gaussian and then you uh you you are engaging discriminated analysis right but we never say how do you how do you decide whether you you really make the Assumption of the about the gaussian Assumption so but in practice you have to do that because you have to um uh see whether data makes sense you know there are many nuances there for example sometimes your data are not as good as you think maybe there are some kind of like um maybe the format is not right maybe there's some kind of like outliers um so and so forth and and only if you look at the data you cannot see uh what's what's going on there so I actually you know even research sometimes I experience this right so like I think in one of my projects um like uh I think we use we just use the wrong data from day one in some sense I think the data like some of the data will just crafted you know just by accident and we are tuning on them and until only until like um like one month I think we realized that so of course in research it's probably easier to tack that you know one month is a long time for us to detect it I think but but actually you know you can easily detect them but for for real life cases you know sometimes it's even harder for example you don't even necessarily have the tools to look at look at your data you maybe have to build some tools to look at your data and and you need to kind of like think about different subpopulations maybe spams from edu emails or spams from.com emails and and see uh what are the differences so this will give you a lot of intuitions on you know what data you should use and our Workshop models uh you should use and and do this at every stage because you know like um in for example we will really do them and this is also while the reason why you want to build some tools to look at data conveniently right so sometimes like if you just look at it once then sure then you can just uh maybe print out something right so but if you want to look at many times then you should have some convenient tools which actually eventually will reinforce and let you to more likely to to look at data right so I think at least in research I I also realized this so if the data is very hard to visualize then people are less likely to to to visualize the data so so sometimes it requires an investment so that you can um you can have this tool so that in the future you are you have less of um uh on um kind of like cost to look at your data and you should do this at every stage uh in many cases um so and and this is um I guess you know this is about domain knowledge where you sometimes you know some of the data requires expertise right like um to know so so some I think there are there are some examples you know in the slides which I uh removed just to save some time but like in short and sometimes you know for example if your data is crowded like experts only experts can know like a like for example you have medical data only experts can know that your data are crafted but like from a machine learner um perspective the data looks fine so I will talk about children depth on tester split so um so this of course is something important for you to um to do um and in practice you know it's other lesson um clear um than uh than in research right because in research you already got sometimes you already got a split even at the first place right so before like you got a data the data already has a split but in real life sometimes you have to avoid certain kind of like leakage uh so for example maybe sometimes for example let me take an extreme case right so suppose your data has reputations so if you have like a million data but actually it's just a two like every data points is repeated twice so essentially you just have 500k and but repeat it twice if that's the case then you split the data then you're going to see some reputations between like you some examples in the test will also show up in the training exactly the same so that would be disasters so so you have to kind of avoid some situations and this actually happens in in the Kegel context so actually in many in many many like exactly like I actually I I try to do some of this Kegel contacts at some point at least at that time like that's probably three or four years ago um maybe maybe more than four years maybe six years ago at that time many of the contest so if you look at them there's some always some kind of like forum for discussions discussing like uh uh okay [Music] like I think I'm sure this happens more in the industry which I'm not less familiar with but in even in the Kegel contest so um so in many of the Kegel context if you look at the Forum always after like a half like a like a half a month I mean after a few weeks someone will figure out some leakage but just because you know something examples are very very close to test example so that they just use this leakage uh to to hide the number so it's kind of like some kind of like weird rule so that you can make the validation performance much better than you thought and and everyone has to use that and it's kind of interesting I don't know why it's like everyone who found this kind of late cage they always post it in the Forum somehow and and then ever I don't know whether this is always true but like for the few cases I've seen they do this and then everyone else will have to use this small Gadget to improve their model performance because if you don't use it your model performance is just not as good as others um so yeah I don't know whether they now they have maybe they have some better ways to to detect this leakage to design the computation much better um I don't know so but this is something you have to pay attention you know um in practice and also another kind of tricky thing is that what is a good spirit so we have discussed whether you should do random splits right so in research as I said you know random side is pretty much the best way you can do because you really literally care about uh the validation performance but um but the problem is that um sometimes you know uh in the real world like your test you like your the the the the test data set is actually not really what you care about so um so so that's why when you split it you also want to split the the children validation set in a way such that the validation set is some of the closer to the real test set let's so I guess this is the situation so suppose you say you are thinking about stock price prediction right so so your final goal is to predict the price in the future right so that's something you just don't have at all right so and and so now suppose you have data between um 2020 or 200 2020 right so you have these 20 years of data so how do you do the tune uh uh validation split should you just just do a random split or you should do some other things so one possible option is that you should probably split into for example 2000 to 2015. that's the tune and to 2015 to 2020 that's the validation why you argue that why that's a possible option um possibly just because the last five years is more predictive of the future than the the earlier years so you know I don't I don't necessarily say I'm not necessarily saying that this is the the only option or the best option but this is at least something to consider right so this was kind of like a kind of like a um this is not what we do in research just and the reason is just because you know like you care about the performance in the future which is something you don't have access to right so I guess this is a right so the better split is they use the first 50 days to predict the last 50 days so um okay and create a specification right so so I think this is mostly related to like how do you kind of like Define what you want to predict and and what's the what's the kind of the goal right so are you so in many cases you can use Machine model in many different ways uh uh from uh from from what you wanted to use it right so and also you care about kind of different um kind of like uh perspectives for example you know what is a spam right so like the definition of spam sometimes you know are different um between different people right so like um maybe do you think like an ad right from some from from say Google do you think that's that as a spam you know maybe I always think of like that but some something else probably prefer to receive some emails some add emails with very low rate right so so you have to specify exactly what you want to predict right so what is really the definition of span and you don't want to have ambiguities at least you know from this day's perspective right so like a machine learning models don't like ambiguities you really want to have a clear cut what is the spam what is not and and also like a what level of expertise is hard to understand it because you know if you specify the spam you know you can have a definition but if your labeler don't are not able to uh label the spam according to your definition is not going to be useful right suppose you have a very very complex definition of spam and then you say I have this data and I'll ask laborers to label them but the laborers cannot execute my definition of spam easily that's going to be another issue and and also the specifications like uh um so you use the specification you want because you want to kind of like a uh uh on you use the specification to define a set of examples right so because eventually if you just have a some kind of description text description about what is a spam that's probably not useful you have to really have a set of test examples and the test examples have labels and that's is really your definition of span so and for example one of the quick and dirty tests here is that um what are your definition of the span um can pass this so-called you know inner annotated agreement so basically what you say is that you write down some definition and then you select a some randomly selected or you get some examples of emails and then you ask um three annotators to see whether they can agree on which email is a spam or not according to the definition you give them and often you don't really get that high of agreement you know you know you don't get 100 agreement you know in many cases you know people's interpretation of the same definition would be different um and let's say you know you have 95 agreement I think that's already considered to be great and then you have 95 of agreement then the question becomes you know whether uh uh it's meaningful to shoot for some accuracy more than 95 right if the annotators don't agree with each other uh um only agree with each other on 95 percent of 95 of time actually sometimes you can do better than that just because humans are sometimes you know lesson um have less in accurate like interpretation or sometimes machine models can do better but you know typically you probably shouldn't shoot for much higher than 95 in many cases so um and and then you know you're gonna you know do this iteratively for example you have to kind of examine the specifications you gotta look at you know what's the discouragement you know why you have discouragement maybe that means you have changed your specification and um and and and you know and the last question is kind of interesting what do you tune the people on the machine eventually at some time if you kind of have a lot of like a different um sometimes you have to tune the people to label them correctly right so for example even like the image classification problem we have this clearly defined labels right so dog cats you know but once you go to the the more the the the breeze of godox right so some some of the laborers don't really recognize different breeds of dogs so you have to tune the labelers uh in some way so I have a friend who did this like a in PhD and and basically he has a lot of kind of training documents for the labelers you know Amazon account workers and and I I they have to actually ask the Turkish to pass some exams to be able to to be a labeler for them so so so it's actually kind of complicated okay so I guess I'll I'll be quick given that we are almost running all the time so and then when you do the model this is the more machine learning part so you want to implement the simplest possible model so um and you keep it simple and then um um and sometimes I think this is the this thing so don't get embarked on new models uh use them to understand data right so sometimes like a um the problem is that um the models are not only the angle sometimes you know you want to use the model to understand uh what are the problems with the data right so and and sometimes you can you can fix the data and then the model becomes the performance becomes much better so um so this this is a whole Loop right your bottleneck may not just be only about the model sometimes it could come from some other places maybe a data maybe a specification maybe the chin test split so and so forth and and you have some bass lines so that you know what you are what you are doing and you need to do some mobilization studies on so and so forth and and then step six you know you need to match the output so and uh you have to measure the output so that you don't you know make mistakes twice and you want to catch up catch new mistakes as soon as possible um and you want to measure kind of different things and simple things for example um like like there are a bunch of kind of like qualities you care about here um and and so and so forth and this is probably one thing that um um that we it's one challenge we are really facing these days about machine learning model maybe I would say probably one of the most important challenges so the reason is that you have a description shift right so you're training an validation description are very different from a distribution that you will test eventually um or maybe they are similar but there is some kind of like special subpopulations that make them different right so for example you're tuning on San Francisco street views a new test on Arizona street views for example when you build a automatic autonomous driving car then you know you tune on some street views and you have to test on some other places and that creates a distribution shift and then uh you can you can have surprises and you know there are not so many kind of like good uh ways to do this you know except that you have to kind of be careful about it and these are new algorithms um and um and this is incredibly hard right there's no real solutions in Industry I don't think there's real solutions in um in in research either so I think we're gonna have for one guest lecture by James o about the robustness of machine learning models there are a lot of recent work on it um but so far we don't have you know I think we have definitely better algorithms to to be more robust but I think the performance is probably the robustness is still not um ideal enough Okay so okay I think I'll just jump to step seven so yeah you have to repeat and look how data I release the longer version of the slides you know if you are interested in some of the details thanks"
"Stanford CS229 I K-Means, GMM (non EM), Expectation Maximization I 2022 I Lecture 12","Stanford CS229 I K-Means, GMM (non EM), Expectation Maximization I 2022 I Lecture 12

hello so uh welcome to the the next section of cs229 so what we're going to talk about in the next series of about four lectures is the start of unsupervised and kind of less supervised learning and I'll make that concrete and kind of the plan is is what you're going to see is you're going to see a bunch of things that look like um various different ways of dealing with this fundamental problem of what do we do when we don't have labels right so we're first going to look at an algorithm called k-means and then we're going to look at this algorithm called GMM this gaussian mixture model those will both happen today and we'll try and put those algorithms on kind of solid footing in the maximum likelihood framework okay we'll then see a couple of more algorithms that have to do with this unsupervised way of viewing the world one of them you're going to use in your homework called ICA it's actually the reason I teach it it's kind of a fun algorithm it's a weird bit of setup but it's probably people's favorite homework problems when we look through the kind of feedback it's a fun problem if you remember the cocktail problem from the first day where you have people talk talking in a party you have microphones scattered around the edge and you want to know who said what you want to do what's called sometimes Source separation and understand the sources so we'll we'll do that in the in the ICA section and then we'll talk about a a more advanced topic called weak supervision weak supervision is industrially used to create large training sets for big deep learning models which you've just come out of that but these are labels that are lower quality than the traditional supervised labels and then finally we'll talk about self-supervised learning which is really exciting and is one of the big Revolutions in machine learning where we train on something simple like predicting the next word and if we train on enough data we can use it for all these other sometimes called Downstream tasks okay so that's kind of the program for the next four or five lectures and today we're going to start with with those Basics okay so we're going to start with the basics is unsupervised and work our way up to pretty modern stuff and I will say historically I would say five years ago I kind of didn't like teaching unsupervised because it felt kind of squishy and weird to be honest with you it didn't have the beautiful Theory or maybe even 10 years ago didn't have the beautiful theory of supervised learning where you could say like this is the right model and we go find it in a lot of cases but that's been where almost all the research activity not only in my group but kind of from AI over the last three or four years has been in this unsupervised realm so we'll start with some of the classics and get our way to something really really exciting so the classic and probably an algorithm you already know or probably you can guess kind of how it works is this algorithm called k-means all right and then we'll get to these other ones but this is the one we're going to start with first so what's the difference so in the supervised setting right we get points and the key issue is that the points come with labels you have labeled these plus and these minus as we were doing and we would draw a line or a boundary in between them that was the way we formalized a wide variety of learning tasks okay right and the point is is those pluses and minuses those are labels right and unsupervised started in this setting I give you the points but I don't tell you any labels underneath the covers okay now there are many things on the spectrum between I give you the labels perfectly to I don't show you any information about the labels and I just show you the data points we're going to talk about that Spectrum but now we're going to start in the cleanest setting and unsupervised where we just see those points okay now the the thing that you should calibrate here is like what can we expect as an answer so in the supervised setting as I said it was super clear what we wanted like we could talk about what this line is and we can go deeper and deeper into like why that's the right line right that's the right separating hyperplane and the unsupervised case it's necessarily squishing we'll talk about the ways in which it's squishy and what I mean by that is it's harder in some sense so unsupervised learning is harder and what I really mean by that than supervised what I really mean by that is we have to allow stronger assumptions okay stronger assumptions right so one of the things that kind of passed by you without thinking about it too much in the supervised setting is you're like there's just some data and there exist a separator there exists some class that's pretty weak for what we had to assume about the data for unsupervised we're going to have to assume there's some kind of latent or hidden structure and a lot of the conversation is hey if that structure is there can I provably recover it that's the kind of thing that we're going to do and we're going to trade off kind of assuming more about our data and trying to find it robustly okay and now compared to supervise we're also going to accept weaker guarantees okay so weaker guarantees so in supervised learning in many cases that we cared about I think your guarantees we would say like we got exactly the right answer right this was the right model we could say you know there were some Theta star out there that was just the right thing we were looking at as we'll see and I'll come I'll show some examples in a second in this unsupervised world it's not super clear uh you know what the right answer should be all the time if I just show you this picture and say how many clusters are there some of you say well there's two clusters somebody said well why aren't there six there's six points why isn't each one in its own cluster and if we'll see in some examples it's not really clear what there is we have to make some assumption or do some modeling to understand and kind of kind of get to the bottom of our data okay and so that's what I mean by weaker guarantees and stronger assumptions and we'll see those mathematically but just keep them in the back of your head we're going to be more comfortable saying like what if our data are generated this way that's going to be a stronger assumption and then we're going to say you know we can't recover something that's pretty good some of the time all right this is less jarring to you than it was like historically honestly because historically we would teach the first part of supervised Learning Without things like neural Nets and all the rest and neural Nets you kind of got comfortable with the fact that like hey maybe it's Computing something useful maybe it's not we don't know we just run it for long enough okay this used to be a little bit more disturbing okay so if you look at the literature you'll you'll see that in it so let's start with our with our first version of this all right so what we're going to be doing is we're going to be given some data oops it's not very doesn't look very nice this is our given data okay given and we have some points and this is basically me just introducing notation and copying what we had before all right we're also here as I mentioned that stronger structure we're going to also be given a parameter which says how many clusters we think there are so the game is going to be here I'm going to write down you know K is equal to 2 and our goal is that what we want to do here is find a good clustering right and so our do is we want to use two I'll use colors here apologies hopefully it'll be obvious you don't need to know what the colors are but you know we'll we'll kind of find two colors like this say like in what sense is this kind of intuitively good okay this is the same thing we're doing in supervised learning right we're like this is an intuitively good thing we talked about different loss functions you're kind of an old hat at that so you can kind of imagine what we're going to do like oh the the loss in some way from some object shouldn't be too far away okay now the K means is hinting at that object is going to be the mean or the center of that cluster you kind of want to minimize that Center cluster that's not obvious we'll come back to it okay so this is what we're given given data we're given this K parameter and our goal is to create some clustering that we're going to talk about so we need a little bit more notation okay now this guy will call some point X1 we'll call this point say X3 and that's just the notation that's there I'm going to write the notation formally in a second okay but before I do that is it clear kind of what the setup is you give me some points you don't tell me any labels you'll also tell me how many clusters you're looking for and then it's my job to find them find those clusters and we'll talk about how to make that precise and formal in a second awesome so I'll just write down some notation and if there are any questions please thanks I'll show you that notation in just one second so yeah let me write the notation down and I'll come back to what I'm calling there great question also on the Ed thread sorry that I forgot to do it earlier someone's asking what about the unknown K case we'll come back to how to select K later some of our algorithms as we'll see just to go ahead some of our algorithms will actually have a natural test for K we'll be able to basically try all the different K's and pick one some of them will not and it will be a modeling decision and let's come back to that question for how K is unknown it's not by the way it's not super unrealistic that K isn't known you may know by looking at your data that roughly you think there are about three or four products or something that people are talking about or various different topics and those are the kinds of situations you will need kind of k-means in but let's come back to it great question on the Ed thread all right so let me give you some proper notation so we can actually you know talk about this maybe a little bit more mathematically we won't be super heavy on that today okay so what we're given in general is sum X1 to xn that live in some some Vector space and here we're given some K which is the number of clusters okay [Music] we need one bit of notation to do this so how do we how am I going to make these colors into notation well I'm going to introduce this thing we're going to find and assign oops find an assignment of points to clusters points to clusters okay so how do we do that to the K clusters so what we're going to have is we're going to have a map that's called CI equals J means point I to Cluster J okay so here I is going to run from 1 to n and Jay is going to be you know from 1 to K right these are the same this K and this K are the same okay so back to this I was drawing I got a little ahead of myself I started to draw the notation before giving it this is just saying we can call this point X1 this point X3 I'm just noting down what the what the points themselves are and then what our goal is to find this map that I drew by by kind of this highlighted color and that's going to be this c i j character here so this is you know higher than yellow for reasons I'm not really sure why I picked yellow okay so for example c 3 equals 2 encodes this fact that C3 is in cluster two and I made the green cluster 2. so 0.3 and cluster two okay okay this will sound kind of opaque at the moment but this is a hard assignment we're saying every Point belongs to exactly one cluster the reason I'm signaling that is in the end of the lecture we'll talk about a soft assignment where we we have a different version of this okay but that's what this is any anything that's uh you know obtuse about the setup all right let's see an algorithm all right so let me copy this piece drag it over there all right get rid of these characters because we don't see them and this is our input okay so the question we want to ask is how do we find the Clusters find the Clusters now there are many ways to do this and if your complexity kind of nerd then you would say like you know is there a you know polynomial time you know algorithm for this and all the rest answer is no actually it turns out there's an MP hard problem so we're going to have to use a heuristic based algorithm to do this and let's see the most natural kind of iterative approach here okay so how does it work okay so here's the idea we're going to start by randomly picking cluster centers and then what we're going to try and do is the way we're thinking about key means is we're going to try and find our clusters as two that are described by two cluster centers so intuitively let me just jump back sorry about this jump back we'll kind of try and find a point say like here and a point here that are good cluster centers and they're cluster centers in the sense that all the points in their cluster are closer to the center than than they are to any other Center okay and that's how we're gonna that's how we're going to construct our cij so we'll start by picking these means the means we you know where are we going to pick if we knew where to pick them optimally like we'd be done so we just pick them randomly to start with so I'll pick the cluster Center for two up here and I'll pick the cluster Center for one down here oops I'm going to use a different color for deep-seated reasons now for no reason I just like different colors okay sound good so I just picked those points maybe I want to move this one just to make my life a little bit easier I'm going to delete that and drag it over you say why are you dragging over because it's like a cooking show and I want to make it just look a little nicer when I draw the pictures yeah these were not list of points these are just two random initialized points on the plane I just randomly picked them in Rd we'll talk about a smarter way to initialize them in a second but just for the moment they're just random points they are not points they do not have to be points in that you're given yeah we'll talk about how to initialize them later and you may not exactly want to do that but you want to do something similar okay so okay so let's see how the algorithm runs so the first thing is what would the clustering be if these were the sent if these were the centroids these were the center points oh go ahead means sorry yeah I wrote above it this is called Kings great question please so for the moment K is going to be given to us as input we'll come back as I talked about about how to select K in a minute but let's run through the algorithm once and see how it works then we'll talk about kind of what goes wrong and and how to extend it these are wonderful questions really great questions are there others awesome okay we'll come back to that and please both questions please come back to those as I said all right so the first thing is we have these randomly centered cluster points okay so this is our first step of the algorithm is that we're randomly initializing cluster points randomly initialize uh mu1 and mu2 copy paste oops got more than I bargained for boom uh and this one based okay so we just randomly initialize where those characters live okay two we then assign each point to a cluster okay and the way we do it is which point are you closest to so I think you know if my drawing is okay those are all you know closer to Mu one and maybe this one and and these couple are closer to Mewtwo okay because you're clear enough what I've done this this is the cluster assignment and I can write it kind of mathematically in one second if you'd like I'll actually I'll vamp a little bit and say how I did this more precisely X over J 1 to K oh sorry I'm in all right Max argument uh C or mu let me write this clear going on mu J minus X I okay I'm going to put a square there just because I I like squares because we do it not really too problematic so and they're the same distance so this says CI where I'm mapping the ith Point well I just find the closest mu J to it my current mu J okay that's the way I'm doing the assignment every Point goes to its closest neighbor and I'm like arbitrarily saying this guy's closer than Mewtwo all right now I have to improve my clustering because remember it's iterative so what do I do I'm going to copy it so you can so I don't have to overwrite it what happens well what do you think I should do I should probably take the points that I have and compute a new center my new guess at the center of the Clusters so what I do is I compute new cluster centers what does that mean cluster centers so I'm going to move I look at mu here what's the new cluster Center for that boom boom well it looks like it should be somewhere here right this is where mu1 goes and where should mu2 go well probably somewhere right here right that's kind of the mean of them okay now you can compute these things precisely but I'm just drawing them just so you get an intuition okay and I'll write this notation because there's more notation so now I have new cluster centers now what's the next step I repeat so I go back and I assign every set that's closer to Mu 1 or mu 2. so now I have these characters and I have these characters there okay that's what that's now doing step two again I repeat step three what's gonna happen they're gonna jump to the spot in the middle and the spot in the middle new One's Gonna Be there mu2 and then there's gonna be no more change if I were to continue running the algorithm forever so let me write that out mathematically while you digest that statement that's the entire algorithm okay so what does this mean mu J is going to equal 1 over Sigma J it's going to be some average over all the points in Sigma J x i such that Sigma J is is just the set of points uh I such that CI equals J okay this is just some notation that says these are all the points that are close to closest to the center J and then I average over all of them I just compute their mean hence K means okay I iterate this thing and I repeat until no until no assignments change until nothing changes basically repeat until nothing changes notice that if the labels don't change in step two then the mean is not going to change because it's a function of the labels right it's a function of like what I guessed was in each cluster it's always their mean any questions about that nothing changes please in many many in different ways so let's let's talk about a couple of things that go right and then we'll talk about what goes wrong so the first way it could go wrong is does it terminate right so if you imagine it was in some you know it's not obvious like I'm doing this kind of jumping around it could oscillate wildly right like there's nothing that like prevents it at least at a high level when you're thinking about this of going like oh one switch from wet red to blue then it switched back from Blue to red and then the cluster centers are like jumping around and there's in some unstable kind of equilibrium so that's something that potentially could be bad that happens so that's the first question does it terminate does it terminate all right and it turns out this is great yes does now why does it the reason is that this functional underneath the covers one to n x i minus mu CI that is the distance between a point and its cluster Center is actually monotonically decreasing it's not increasing so that that oscillation can't happen basically and you can you can basically view the algorithm underneath this K means if you really want to be kind of super abstract you can view it as basically doing gradient descent on this object in a particular way see the notes for actually the convergence so it converges to something okay now an unsupervised learning that converges to something is about as is kind of what we can hope for in particular you may also ask okay it converts to something does it converge to a global minimizer does it converge to Global minimizer and if I did my job setting this up your intuition should say no no not necessarily now you may look at this and say well not necessarily maybe there's a better algorithm out there that given this setup of problem you just you know why did you pick this algorithm it's a bad algorithm you should show me a better one there isn't a better one because it's actually MP hard if you don't know what that means don't worry there's no hardness proofs in this class does not really matter it just means we don't know like Humanity does not know a better algorithm for this that runs in polynomial time okay and we have reason to suspect there isn't one okay all right so this is this algorithm as I said like it's going to run but sometimes it can end up doing things that are like you know quite bad in particular gets stuck by like getting a cluster over in a region where it should have never been and if it like had you know more carefully searched potentially could have gotten a lower cost solution all right clear enough clear enough on its properties now this this is going to be a Hallmark of what we do over the next couple of lectures almost everything we do with the exception of one algorithm that we'll talk about has this property that it doesn't necessarily converge to a global optimized solution and as I said this used to be much more disturbing to people but now most of AI is in that mode so people don't seem to care as much okay used to be quite disturbing when I started it was quite disturbing now no one cares you guys you folks are old hats at this go ahead so this is what happens the next situation so you can imagine situations for example imagine there are points that are really far away and then some other cluster of points here and that really there's enough kind of things in the middle that the optimal is actually closer to the to the line then what you'll see is that if you could you could end up with some kind of some kind of local minimum where you pick a center way way over here and a center in the middle of this but you should have there was a kind of a better one closer so that's very hand wavy I can post some examples where you can actually run through and convince yourself all that I really care to get across in this lecture and I couldn't think of how to like get good examples to look up there is that this is a possibility you shouldn't expect it to get you a global minimizer and we have some theoretical reason why we don't believe such an algorithm could exist even in this simple case yeah wonderful okay now a couple of side notes so these questions already came up so these are really great so they're not even side notes they're just responding they're being responsive but I was going to talk about them no matter what the question was asked how did you initialize right how did you pick these points and from my argument as I just talked about even very intuitively and informally the initialization matters quite a bit right if you initialize way way in crazy ways then you force came in to kind of jump back and maybe there's only one point like imagine there's a cluster of point and you throw one of your centers way across the room it's gonna you know potentially pick off nothing and then it's going to have to you know kind of crawl its way back in some sense so the point that I care about you getting is how does the um how does initialization you know matter and I won't go into too much detail but I want to tell you that there's one algorithm that was developed by smart Stanford students a while ago maybe 12 years ago I don't know 12 15 years ago maybe even called k-means plus plus and I'll just you know tell you or what what they did from great Stanford students and basically what they decided to do is that they where they compute kind of a density estimation and then they they Place their centers with respect to the density to kind of spread them out in a in a nice way and k-means plus plus and without going too much into the details what ends up happening is they get an improved approximation ratio so if you don't care about this kind of theory stuff don't worry but they're able to show that if you initialize in this way even though it's MP hard to find the exact solution they can find kind of a low cost approximate algorithm to it provably so this is actually when we were talking about what could go wrong this is something that can go wrong and what people do in k-means is they used to just kind of run it many times but if you run with this initialization it's still a random initialization but if you run it a couple of times you're going to get a pretty good solution and because you're already kind of looking for an ill-defined kind of objective that turns out to be about okay now weirdly enough like I used to as I said I didn't really like this stuff but I wrote a paper about using some of it inside some of making these machine learning models robust maybe a year or two ago so it is something that like you know I've actually used and care about like it does work and and you use it as a way to inspect your data off it like you don't you look at your data set you kind of do your k-means clustering then you can use that to figure out like you know what are the different groups that are inside okay all right this thing here became the default and scikit-learn uh and sklearn so these folks wrote this beautiful paper they proved this nice result and now if you run a psychic learn and run the k-means algorithm it defaults to using the excuse me defaults to using the k-means plus plus initialization okay so pretty fun Sergey and folks did that anyway all right so now there's another question which was okay how do you choose K right this came up both in the Ed thread and it came up uh in the lecture hall itself how do you choose K and here's the problem with choosing k there's no one right answer we'll see other unsupervised algorithms which do give a right answer but let me just illustrate this for you for one second suppose I give you this data set okay there's two copies of it one reasonable clustering is this two clusters equally reasonable excuse me is this one four clusters now if you think about the algorithm as giving you the right answer of what structure is inside your data then you really don't like this answer say well what's the right k well it's not the right way to think about it maybe the right way to think about it is given that you have a k can you use this as a tool to find the Clusters that are in there of different sizes so if you're looking for five clusters and you really only found you know you look at the five cluster and the four cluster map you have to make some judgment about kind of which one is better underneath the covers and that's why unsupervised learning is kind of squishy but it also turns out to be tremendously powerful so we can automate the loop of finding the Clusters but you know which one is good or bad you're going to have to use domain knowledge for that in most situations okay right so this is really another way of saying this is a modeling question so the one part you know even when I didn't really care for I don't know what I'm telling you the history of how I feel about lecture 11 but like you know you're getting it so but like one of the things that I did like about this lecture always or this this part of the course is that it forced machine learning folks to move out of the comfort zone of like there's a right answer and optimization will find it which was very disturbing to me because that's not at all how machine learning works and what you're doing you're constantly in this regime where you're checking if your model works or breaks or does things I don't know how much a technique subjected you to my crazy slides but like that is the way that you live when you're building these kind of ml systems like you build a model you have no idea if it worked you have to check it you have to look at it you have to inspect it you have to measure it um so this is kind of natural at least forces you to do that okay that's all I want to say about key means I want to jump to the next algorithm which is building towards this larger theory of what we did and if you can kind of squint you'll see that it parallels how we taught the supervised learning case but any questions before I move on awesome please is this something you might use if you're doing supervised learning but with labels that you're not sure about oh awesome yeah great question so let's say that you're on super buy here so the way we were actually using it is exactly in that way in this paper what we were looking for I'll just to be really concrete is we were looking for what are called hidden stratification so these are times when a machine learning model let's say that it's classifying uh birds that are on the land versus birds that are in the water I have more elaborate examples than this and it turns out that the model sometimes gets confused by the background and so what you want to do is you want to Cluster some notion of those like feature descriptors underneath the covers of like background descriptors that the neural net has learned to try and find a group which kind of looks like an oddball from the rest so it's like oh here's all the land Birds on land backgrounds I got them all right and there's this other weird cluster there that I labeled as land Birds but actually our water birds that happen to be walking on land right and so that's the kind of thing you look into your your like labels or your Suitor labels or your predictions and try and find those it can either be labels that are given to you by a human or a neural net and that is like one of that was you know the use case that I had is exactly what you what you thought about um yeah so that's one of them you'll typically also run this when you have like you honestly when you have data you don't really understand so like you run an experiment with you know search traffic or ad traffic or something and you're like what the heck are people asking about in this segment and then you run kind of a clustering to figure it out and say like oh this this looks like a topic that people are talking about X Y or Z right so those are the other times that that you use it but noisy labels is a good thing and that will be a theme for next week's lectures great question please yeah so the question is if you pick a larger number of clusters then like if we go to this example here the problem is how descriptive does it get so in four and two you could kind of eyeball the two and the four and and link them up but if you start to really Jack it up to like a thousand then it like is like kind of at the brink of what does it mean to find meaningful Concepts right so if you're looking for 10 things and you know there are 10 things you want to find those 10 best groups if you start to jack it up to a thousand ten thousand it no longer becomes like human digestible so that's where the modeling thing comes in it's kind of like the bandwidth of what you can look at and do this and and traditionally k-means was not used in an automated Loop it was traditionally used as like a way to browse your data if the consumer of that is some neural net in the discussion we were talking about then it kind of doesn't matter you can jack it up a little bit more the advantage of doing so the advantage of doing that is if you put too many centers they may be too close together and sometimes what people do is they'll just do a kind of a follow-on path where they merge some clusters that look like they're kind of not informative a pruning heuristic at the end so your intuition is dead on and like that's the side you want to err on if you air on the side of two small clusters what happens you lump two things together that you could have distinguish and then you could get like a weird cluster in the center so clearly you're right the over provision setting is much nicer than the under provision setting awesome it sounds like folks got it please [Music] [Music] yeah so so I wish there was one algorithm so this is something that like people are writing still writing research papers on we had an iClear paper on there's another one coming out from some other from another group which is great that we're reading right now basically what people try to do and in these things is that data point so this is kind of an impoverished setting right we have data points that are just in Rd that are in a vector space but those data points usually correspond to Something Real like there's an image or text and so the more modern ways that people do with this diagnosis like what what Sabria and folks did in and Maya did and Domino is I said oh we took those points and we embedded the text that's associated with an image and the image itself and we start to like do kind of search queries on top to find the slices that are underneath the covers so this is the purest setting where you're just like I have data from in a vector space but of course like with modern methods like there's an underlying object it's a transaction or an image and a caption and then you start to use some of the more modern techniques to search through it and then of course you visualize it or monitor it and there are a number of tools that are out there to do this I wouldn't say it's solved like oh you just use x and you're done but um you know there are a number of tools but yeah you do have to look at your data like there's nothing that really obviates that awesome questions all right all right so let's talk about a slight what appears at first to be a slight generalization hopefully it feels like we just relax one little thing but it gets us closer to a more fancy model that will allow us to do some really fun stuff okay and this is actually pretty fun so this is a toy example from astronomy and if you know about astronomy please don't humiliate me too badly feel free to correct me I have no idea what I'm talking about but I read the paper and I understood the math so okay it's a toy example okay and it's from a paper from the University of Washington so I can find the paper again for people there okay so here's the general setup of what they're doing they have this light detector this Photon counting thing okay and they're looking in the sky at various different things and what they want to do is they have in this simplified model there's kind of two celestial objects that could be out there could be like a regular star and a quasar okay and one of them is going to take the light and it's going to spread kind of evenly in all directions and another is going to send out relatively concentrated pulses okay and what happens is unfortunately when those photons come to Earth they don't come with labels no one gets to like affix a metadata packet to every Photon that that hits us we have to see the photons and figure out like where the hell do they come from okay so what happens we get some data that looks like this maybe like this okay and these are all our little photons that are there okay so you know quasars and stars now okay and both of these things emit light but all we get to observe are these Photon hits all right so now what we're going to do in our sub oh please go ahead assume no we don't have to assume a linear decision boundary we just have to assume that it's it's close in distance right you can get non-linear right there's no notion of a decision boundary in particular anyway like you can draw like a little vornoid diagram but we didn't talk about that very nice question awesome so we get these Photon hits okay so what we want to do is kind of figure out you know what these different sources look like and so what our given or this is what we're given what we're going to do is try and find a soft assignment of these things into gaussian so remember what gaussians look like right they're basically it's a high probability they look like ovals right they can be circles right that's what a two-dimensional gaussian looks like right and the variance tells you how it's How stretched it is in each Dimension okay so like maybe there's a a cluster here cluster here and this is a tight cluster okay all right so we'll talk about that in more detail in one second but what we have to do is we want to assign each photon to a light source but of course we don't actually in this case we don't have perfect information of the light sources either and so we're going to settle for what we call a soft assignment okay so this is the probability prob that point I goes to Cluster Goes To Source J okay and this is our soft assignment this is called a soft assignment okay why is it soft because we didn't remember in K means we said you must be in this in this setting here we have some probability distribution over it okay all right so just compare that with compare that with k-means all right so what are the challenges here the challenges that we that that we're going to deal with here potentially are there are many sources right if there were just one source life would be easy we would kind of fit the gaussian you know how to do that from GDA you take the mean you find the variance you're done okay there are many sources here but for now we're going to assume we know the source is K we know K which is the number of sources okay let's solve the problem in that setting okay and also the other problem is the sources have uh different intensities and shapes intensities and shapes okay so when I was wildly drawing those ovals the reason was is like oh maybe there's a gaussian that looks like this maybe there's a tight gaussian I don't assume I know that ahead of time I just know that they're somehow well described by gaussians please oh no great question so those are the two types of things I need the number of celestial bodies so like one reasonable thing would be three there's one here one here and one here but K is still going to be picked by a modeling assumption right so this is this this curve is hard to generate with uh with a gaussian in our current setup right so there's probably not there's probably more than one on that side but that's just intuition there could be four there five there I don't know so let me let me talk about the assumptions okay so we're going to assume it's well modeled and here we're in you know most of what I'm going to do is going to be in one dimension so I'm still just going to introduce the notation in one dimension by gaussian right and you remember that a gaussian means mu J Sigma J Square okay so just one dimension to make our lives similar in a second but we do not assume we know how many points that there's an equal number of points number of points is equal okay now the reason is in our setup the physical reason is like you know some point sources are really concentrated and strong and they shoot out tons of photons and in other cases they'll shoot out you know diffuse photons over a region that may be farther closer away different energy levels whatever mathematically what I mean is we don't know when we're going to talk about these gaussians the shape may be a gaussian but it's only sampled infrequently for Source one and it sampled 90 of the time for Source two okay this is formally known as an unknown mixture okay now one thing that's nice about this problem and the physics setting is once I get the values out so what are the what's the do here I have to get out these cluster centers that we talked about before and these probabilities which I haven't given you notation for but they're going to be called Phi in a second I can check that if how physically plausible that clustering is right so in the K the reason I like this example is and the K means you kind of kind of have to eyeball it but here if I have a physics model I can compute How likely is one clustering versus another given the data sources that I'm seeing maybe have auxiliary information about what celestial objects are in the sky and that particular region and so I can I can check this information okay and the physicists in this example could actually check the information please [Music] the range of what J oh J is going to range the same it's going to range over each number of sources so we're going to assume that we have this K number of sources here so J Will range over K just like it did so that's fixed that's a parameter of the problem someone's going to tell me basically what someone's going to tell me is there are K sources I have don't tell you what their means are I don't tell you what their variances are so that's for each one of them and I don't tell you how often that's this mixture they emit light so what percentage of the points go to Source a versus Source B please so so for example let's imagine that we had a situation like uh let me just draw it so let's say that we had you know three points here and then I'm not going to draw them all but like a thousand points here so there are three points here and a thousand points here right so the clustering that I would want out is one cluster here and one cluster here but I don't get to see the three and one thousand I'm assuming that they're of unequal sizes so you could for example assume that all the Clusters are equal sizes I'm trying to and that would be a that would be wrong and this is not the way we'd have the model that's a fair assumption if you knew that right but that would make your life a lot easier it doesn't make you have to solve both of these two things awesome question all right please yeah we make that assumption kind of implicitly because we don't say how often like there's no probabilistic model but we do allow K to have like five neighbors and 500 neighbors right center cluster one could have five neighbors another one could have 500 so it does parallel k mean so in that sense it's the same assumption I'm calling it out here as an extra parameter that we didn't have to think about last time right awesome questions okay let's see some 1D mixture of gaussians mixture of gaussians is like a very fun and famous Problem by the way um people like work on like which ones you can solve um in theory still it's like a fun it's actually fun and interesting problem we won't do that all right so I'm going to be in 1D for Simplicity here because it just makes my notation and drawing easier so I have to draw distributions all right so just make sure it's super clear what we're doing I think folks sound like they have it so we have 1D this is going to be the the place where our points live and then imagine there were two gaussians that were generating data okay now when I say this generating data this is the piece like when I talked about we were going to talk we're going to allow ourselves to talk about models we're going to imagine that there's really a model that's underneath our data that's generating it okay this is a quite a powerful technique okay so how does it work what we're going to do our our idea for the model is we're going to pick you know cluster one with some probability let's call it P or cluster two with probability one minus P we pick cluster one we then sample a point oops let me get a thicker thing all right oops that's probably too thick and we get a series of points okay that's the thought process that we're going through right there if we only knew those parameters we could sample from our model if we knew the probability to pick cluster one versus cluster two and then we would sample from it and there's many different parameters given that model that could have generated our data and we're interested in recovering the ones that are most likely that most likely generated our data okay now what we see unfortunately is just this just the points okay so this is the the model over here this is what we observe oops which back just the points in r okay these are the X I's now one thing and this is going to seem like a trivial piece but it actually makes our life kind of interesting what if we knew these points all came from cluster one and these points claim from cluster two okay this is our first observation well if we knew the cluster labels what would algorithm would we run we just run something like GDA we would just fit gaussians to this we'd be done right so if we knew this we could just solve this instantly solve and fit gaussians so we compute you know mu I mu 1 mu2 how do we do that well we would just average up all the things in here average up all the things in here that was bbr Muse and we could even compute the sigmas and be done the challenge is we don't get to see them we don't get those labels challenge we only see this thing okay this is our first example of a latent model there's something hidden that we don't observe and we're trying to estimate the parameters of that model and that'll be a theme for the next couple lectures please [Music] sure yep oh so this is this is yeah so this was an example to motivate like hey this is like a nice picture to look at and kind of get you thinking empirically about what happens I'm going to walk through most of these things in 1D because it makes my notation a lot easier and so there's really no relationship between uh like these points and the points I showed you before yeah great question awesome all right so why am I doing it in this kind of like piecemeal way is because when I show you the definition then you know it has a bunch of notation in it and I want to show you the simplest version so that then you can generalize it and do all that fun stuff on your own all right but it is it clear what the what goes on here there's this notion I've introduced and kind of sneaked into you which is the model right there's some parameter P here that picks this side or this side right which is going to be called Theta Theta 1 and Theta 2. hinting later I pick something and then I sample from it if I knew where the points came from I could just solve but the problem is I have this hidden how often is everything sampled I don't know how many points should be in each cluster and I don't know which points are in the cluster not only do I not you know I don't know either of those facts so we have to estimate them somehow from data that's what we're going to do okay so let's see some notation and we can come back to this example if if there are more questions about it okay so let me be a little bit more precise so we're given X1 x n element of R okay and a positive integer K and K you know positive integer or a number of sources and our do what we're given what we have what we have to do is we need to find that probability function find P such that for I equals 1 to n j equals 1 to K the Clusters this is the data we have an estimate of p z i equals J this is our notion of soft assignment okay so that's what we're responsible okay and this is although it's a function it's discrete so I can write it down right I can just write down the probabilities in a table we'll later worry about cases that's not the case but for now we do make sense [Music] no so J is a cluster index and K is the number of clusters so J here says this says the ice Point belongs to Cluster J with some probability J ranges over one to K so they are K clusters K was equal we we never actually were precise about what K is we had rough intuition that K should be at least three and I don't think we actually use J or if I did I meant and scribbled it incorrectly and I apologize probably was a wayward eye but I don't I don't I don't see it in my notes but apologies for that I'm sorry for the confusion please that the sample height belongs to clusters each uh belongs or comes from yeah so I I'll copy from here is this this sentence probability of that point I belongs to I think it should be I think is what I meant to write I think that's better probably a point I uh comes from source that makes more sense in English from Source J yeah yeah Source J is the particular source always a particular source so I is always the data J is always the label for the cluster and K is always the number of clusters please uh that in X Oh you mean from the uh no um uh so yeah so z i is the assignment so z i is the is the soft assignment itself x i is the data point yeah sorry for the overloading the notation so the zis will always be the hidden piece that says the so let me write the next sentence the next uh piece and then we'll come right back to your question I can see why you'd be be confused by that so let me write the GMM model and you'll see why these two things are being used according to the GMM model so z i is just that z i is the random variable that this probability that point belongs to it it's not the actual point itself so then for example we can talk about the probability of x i and z i this is the probability that given this point and the zi is that it belongs to a particular cluster um and we'll write down the model in one second so let me write down the model and then it'll be clear what this notation means okay all right this is just Bayes rule nothing fancy here I said nothing there's no content okay but I'll use it now z i is going to be distributed as a multinomial According to some parameter that we'll call Phi and Phi has a multinomial is says that some Phi J goes from 1 to K equals 1 and Phi J is greater than or equal to zero okay so Z J if you like is that sampling probability like it's the the likelihood that this particular Point came from that that place and Zi is going to be picked with this background probability that we're calling P before so in our earlier example there was like P and 1 minus P were the chance that I sampled from cluster 1 or cluster 2 above here okay these are Phi 1 and Phi 2. that's what I was hinting at okay and remember in our model like I picked di that told me where I was sampling and then once I knew Zi x i given that z i equals J that means given that it was in cluster J this is going to be distributed like a gaussian mu J igma J Square okay so these are all gaussians okay now let me highlight for you the parameters we have to estimate highlighted parameters okay okay this thing with this thing okay all right these are the same color so what's going on here this is formalizing the model that we talked about earlier it's our first kind of hidden model we never see Zi we don't observe it in data we don't get to see the labels you know the deity or whoever is generating his data generates a Zi and picks one of the Clusters that's its sample for this point and then it samples from this normal distribution to generate x i and if you like the data we think about as being the remnants of this process we don't get to see this process run it gets run ahead of time and then the data is dropped there and the reason we're thinking about this model is we say Okay assuming that the model were like this could we recover the parameters okay and so that's the sense in which we're assuming there's some structure and it's pretty reasonable what it's saying is look I don't know how many points come from every cluster that's because I have a multinomial and two I know that the points that I the Clusters that I have they're gaussian shaped but I don't know if they're circles or all ovals and I don't know where their centers are and I'm assuming nothing else about my structure can you find it for me that's your job does that make sense you have to come up with we have to come up with these parameters we have to find the cluster centers and the probabilities that each are sampled from observing the data right clearly if I pick one clustering it's going to the data is going to be very unlikely have to have been generated with those centers right like if I go back here and I pick the center over here in the center over here that's less likely than if I put the center here in the center here right it would just explain the data less well we'll be able to formalize that with maximum likelihood but I hope that intuition is clear if it's not please go ahead ask me a question I'm super happy to answer ah so Phi is basically just a bunch of numbers that sum to one it's a multinomial so this is a probability I I like if I have class cluster one like so let's say this is going back here let's say this is cluster one cluster two cluster three or Source one two and three and lots of points are in Source One so then maybe Phi one would be like point seven right because they're like seventy percent of the data is there and ten percent of the data is here and twenty percent of the data is here then the numbers would be point seven point one point two roughly does that make sense awesome right all right all right so let me let's do one example um I just wanna I'm trying let me see call the zi latent because we don't observe it we didn't get to see it right we just got to see the points we didn't get to see which was assigned to which cluster because it's not directly observable this concept which seems at this point like kind of strange I would think about a bunch and we'll see it again and again this is what we mean by structure we're like well there's some wild collection of points but like there exists a small number of clusters that are generating them that's the mathematical embodiment of what this thing is the mathematical embodiment of that intuition that's all it is and we say if that's the case then we should be able to recover it okay we should be able to recover those clusters in those situations and in the physical situation of like light sources that seems pretty plausible right like you know they have different intensities different shapes so on all right so let me give you one more example just to make sure that all these terms are this notation is there and I want you to think in sampling and let me just walk through so hopefully these things make sense so Phi 1 is going to be 0.7 if I2 is going to be 0.3 why are those the numbers because I'm making them up that's why don't have another reason Phi 1 is going to be one mu 1 is going to be 1 mu 2 is going to be 2 and I'm going to set Sigma 1 square equals Sigma 2 square is about a third roughly okay so what does that picture look like here we go there's one there's two there's one thing here if I draw it pretty well maybe my drawing is off but hopefully you get the point this thing should be about a third this distance should be about a third the fact that they're uneven is because I'm a bad artist not because that's the intention of the thing this is mu1 this is mu2 okay and this distance here should be about a third the 65 thing it's a standard deviation okay how do I sample from it first I pick a cluster pick one either one or two one of my clusters I picked the first cluster with probability 0.7 the second with point three then I pick the relevant mean so if I picked two I will use mu 2 I go over here then I will sample from a gaussian and generate myself a point let me hear two use the appropriate gaussian okay so imagine that this were the process that we're generating your data you just get to see an instance of the data your goal is to say what's the most likely process that generated it okay and that corresponds to this intuition like there exists some clusters so far so good oh and then you repeat repeat yeah please go ahead and ask questions no yeah so that would be if they if I wrote their probability distributions entirely these are still unit normal gaussians this is a great question I drew it as two gaussians I didn't draw the Phi 1 and Phi 2. I Incorporated them just in this first set so I would pick you know Phi 1 I would pick one with probability 0.7 like one 0.7 right and you could imagine re-weighting them and and normalizing them but then it's just a little confusing visually and I'm not that good of an artist because we don't know how many points come from every source so remember going back to to this one up here we had like the oops I went way too far back here we had lots of points from this one source and this point you know they're 70 coming from here 30 coming from here 15 or 20 coming from there and so they they we can't we're just not assuming that they have the same if we force them all to have equally sized clusters we would probably put two cluster centers Here If This Were seventy percent of the points and we would we would use that to explain our data that would be in this case sub-optimal right that's just our modeling choice we know that the Clusters have different number of points in our application so that's why we we fit them we can't assume they're the same if we knew them perfectly we knew the things we could we could sneak them in there but we'll talk about where we sneak them in later [Music] the shape they understand they understand the shape really yeah yeah awesome sure um foreign that's kind of good man I mean but like should the second Peak be actually shorted at first yeah that's a great question so you could imagine for folding the files in to try and make them both probability distribution density functions where there was one distribution density function and then you would fold them in I've drawn them crudely having the same height and putting the probability function up front to say these are the gaussians and then there's a height but you've got it perfectly yeah that's another way to visualize it which is if this is beyond my architecture artistic abilities like that's Legions Beyond so yeah wonderful question no no no no no that's a lion I I do not have a lot of my ego tied up in that yeah [Music] awesome so what's going on here the the trick that we're gonna do is we're going to assume like okay so every time you set one of these values you give me a distribution right that gives me a distribution so now I have an infinite number of these things that are out there with different settings of Phi different cluster centers different samplings so now imagine your head like I grabbed one of them and then I did the sampling process and it generates some data okay great so we understand that piece now I I what what happens though in our problem is we're going to invert that process so we see some data just to XIs themselves and now we want to select among all those infinite things that are out there which are parameterized now by these fives and mu's and sigmas which one is most likely the one that generated our data and intuitively we know as I said like if the cluster centers are super far apart and our data is all in the middle that one's probably less likely that one that has a cluster centers closer we're going to be precise in a second about how we fit that but you can kind of see where it's going to come from is this model so in the same way we use maximum likelihood before we'll get there we're going to do a little bit more intuition we're going to say that the parameters that generated our data are the most likely ones after we've seen the data so we'll condition on the data and try to invert it to find it so this this thinking of like the latent forward process is super weird right and so yeah you're exactly right does that make sense though we think about the process and then we're like oh if we inverted it that's kind of what it must have been right this yeah go ahead and do that for various sets of different parameters exactly right the the thing is we want to be exactly right we could just guess all the parameters or try all of them promise there's infinitely many so the questions can we solve them and find them faster than that right but if we could just in theory you know there's well and theory is a weird statement because there's uncountably many of them but like we could conceptually try all the parameters and then see which one was like closest in probability you know had the highest likelihood score and we're going to try and that's going to be our gold standard of what we want to pull out you got it perfectly please foreign if there's only one value then you know everything came from one source and you're just fitting a gaussian and the most likely estimate for a gaussian is as you saw from GDA and everything else is average your data to compute the mean and then compute the variance from that and then you and then you have it perfectly wonderful awesome we got this all right so let's see the algorithm that does this because it mirrors our friend k-means okay okay we're also studying this by the way because this same pattern will repeat itself in our next lecture it's a famous algorithm so it's fun to know about and it's important pedagogically I don't know if you actually have to do anything with it in the class but and it mirrors k-means so like why are you teaching me these two things they seem to differ in small ways and that difference like k-means seems a lot more intuitive than this whole infinite number of models that we're selecting among but we want to get to that world view so that we want to relate the two okay so one there's an e-step okay so this em is very famous and we'll come back to that in a second here we guess just as you put it perfectly we guess the latent values which in this thing are the values of the zis okay so we guess all of the cluster centers their distributions okay by hook or we just figure it out okay that's our first piece we'll see how we do that in a more intuitive way and then the m-step we update the other parameters so if we knew the distributions we knew my observation one if we knew where every Point came from then we could just run GDA on it we could just run you know find which gaussians are in each set we just fit all of them there's a slight twist that we're going to not know where precisely every point is we're going to know them a distribution so we have to do something a little bit more complicated but not too much more complicated okay this is our first example of a very famous algorithm first example of em is like a very very famous algorithm that people like use for decades I don't want to oversell it I have a colleague and friend who says you only run it when you don't know what you're doing and he's right in the sense he's kind of a curmudgeon it's a good dude but he's right in the sense that like if you knew exactly what you were looking for you wouldn't run this algorithm but that's precisely where it's interesting because when you run these kind of em algorithms you know something about the setup okay all right so let's see mathematically what this looks like and in your head you should be thinking how do I what did I do in k-means the e-step so here we're given the data and current values which are guesses for Phi Mu Sigma blah blah blah all the stuff Sigma 1 mu 1 mu two whatever U1 all the parameters okay and our our do is we have to predict Zi okay 4i equals 1 to n all right now I'm going to introduce the notation here following our notes given x i by mu Sigma okay so what's going on okay this is our goal okay so what do we want we want to compute these weights which I'm just giving a new notation to because Z's will be changing as we run and all the rest we're given the data point condition on that so we know the point we're looking at we know all the rest of the parameters the likelihoods the the frequencies with which we're sampling from each one of them we have our current guess we have our current guess of how where the center of every cluster source is and it's um you know variance and now what we want to do is we want to compute How likely a particular point is to belong to a cluster intuitively if it's really close the probability should be high to the cluster Center if it's a really you know if it's really far away it should be close to zero but it's continuous it's not going to be zero itself okay because the gaussian doesn't go to zero you know anywhere okay so how do we do this well it's nothing more than Bayes rules Bayes rule okay oops Sigma oops over all right okay all right so all I've done here is something really simple I've taken x i which I've conditioned on it and I've kind of divided it up so this is like kind of the likelihood of the data being generated with this which is a probability and then these two things being jointly done together then I'm gonna I'm gonna Factor them out as I sum over all of the different probabilities okay so this is going to be equal to probability of x i okay oops probability of x i z i given J probability of z i J over sum if you use l here instead of J because I don't want to confuse what we're doing z i equals l comma the whole thing p z i equals l okay so the point is we know all of these functions okay oh sorry and everything here by the way has like is has this stuff we know all that information so that's everywhere okay okay so we know all of the estimates in our guess so what is this probability right here well if we knew the the that it was a condition that we knew that this point came from the cluster well it's just nothing more than our friend the gaussian right this is this character right here uh which I'll highlight this is just a gaussian we know that from the model right so is this right so if I can be really explicit about it it's like you know X above x i minus mu I Square over 2 Sigma I square right times some normalizing constant 1 over the square root of 2 pi right so we know what this is what's this one those are our fives so this character here oops I shouldn't use that color this is Phi J this is Phi l so I've decomposed the problem once I know it into estimating all of these quantities which I already kind of mechanically know how to compute okay so the key point is we can compute all the terms compute all the terms now this isn't super surprising so maybe you look at this and you're like wow that's just a bunch of weird notation what the heck is he talking about look this is just k-means it's saying in some kind of generalized sense it's saying I need to compute a probability distribution function okay k-means doesn't need to do that but how does it do it it says well I consider the probability that this x I was generated from this J I Know How likely it is that a point came from it I know given How likely once I'm in the point what the probability of this data point is and then what I'm going to do is I'm going to compare it to the likelihood from every other cluster and that's the probability right that's all that's encoding instead of having that hard assignment and saying which one's closest like and K means this was just a Mac said look at all the other clusters and you picked the closest one now I'm going to average over it and I'm just averaging over it with Bayes rule and it looks like notation is a bunch of notation but it's not super scary it's just things you know how to compute go ahead what's they wouldn't be zero but they'd be very close to zero so what would happen in that scenario so in your point let's say x i is really close to this particular J so this thing is very it's not going to be one because the height of the thing is not exactly one but it's going to be some high number let's say you know 0.5 or something so some big big number okay then over here let's say 5j that's the other term so if Phi J like were equal across all the Clusters we'll come back to what happens when it's not in a second if it's equal across all the Clusters we get you know 0.5 times you know one over five or something or five clusters one over five and then we compare that likelihood to the rest of these and because you said these were really really far away what happens here you get this term repeated right so you get exactly that one term plus a bunch of things which are super close to zero and so as a result this thing will be very very close to one yeah does that make sense now what happens just in that inference if if Phi J were very very close to zero so now there's this trade-off as the point you know if the point is like you know there's the cluster Center and it's closed but I think it's extremely unlikely like my 5j is like 10 to the minus 10 and I've only seen a thousand points I still don't consider it very likely that this Source actually generated something okay in that setting and that's all Bayes rule does in general is trade off those two things does that make sense yeah awesome okay right now back to the oh please the probability of x i is the probability density yeah so I use them okay so yes so you can do this because you can use the likelihood ratios in this way but there and this is the correct thing to do from Bayes rule but it's a little bit sticky because like this is just a PDF but the fact that it's like normalized up to it allows this thing to go through but yeah you're exactly right yeah it's a little squishy but it it works wonderful question awesome so I hope I hope what you got from this is and also I wanted to come back so that whole weird rant about like how the data are generated is now mechanically the way the math works that's why I keep ranting about it you pick a point you generate the data that's like assigning the likelihood score you wrote when you wrote the generative model you told me how likely it was right that given you were in a in a in a in a particular cluster that you would generate this point that's the normal distribution and then you compare it and bake them off against each other and that gave you a probability distribution that's it awesome now there's the other step this step is much less interesting so here we're given all those wijs J which is our current estimate test of p z i equals J okay for I goes from 1 to n j goes from 1 to K okay and what we have to do is estimate the observed parameters the non-latent parameters now this is conceptually interesting because right we split our thing into latent which are not observed the zi's are not observed how what the probability is that you're in a particular class and the things we did observe the frequency you know how many are in each class you know their cluster sizes and all the rest like you can measure those things okay and we'll do that using mle because that's the tool we use by the way we use mle a lot in this course um but they're it's not it's very powerful but it's not everything and come back to that later but that's it's a principle okay so for example what is 5j what is the estimated frequency well we sum over all the points I goes from 1 to n of w j i this is a there are gas of the fraction of elements and from cluster J we can we can make this a little bit more rigorous and we'll do it next time and the point is is you just do mle I don't want to go through these calculations because they're kind of boring um but we should you should go through them at least once and we can make that rigorous and we'll do that when we do the mle thing so I'll do it in class so don't worry if it doesn't stick now we'll do it on Monday in a little bit more generalized setting but what I hope you get here is if I know the wijs I have to compute the estimate of 5j and then I just average over the probabilities and we'll make that we'll do all the math to expand that out but you already know how to do this this is just the mle stuff that you've been doing for the last K weeks okay so far so good no we're going to uh we're not going to be we're the this is the the right answer we're going to derive this in more generality so you can solve for a larger class of models on Monday oh fractional points sorry that's really terrible I said elements and then I don't know why these are the points points from this should be sourced I kept using I was trying to make source and Target in independent my head and I did not use them carefully in these lecture apologies product points from first gen is the rough intuition of why this is okay means kind of this similar to you you pick a set of means which are like your sufficient statistics to describe your problem then you re-average over them and what's going on here is you're picking a set uh now instead of just picking centers you're picking distributions then what you're doing is you're re-weighting the distribution saying if that was really the background probability of all these linkages then this is what your most likely cluster sizes would be this is how what you know how much points you would have in each okay does that agree with your guess and you just cycle that again and again until you get back to the guest the same way you're doing okay means awesome all right so yeah uh uh a bunch yeah a bunch Yeah so basically an em model can be applied whenever you have a latent uh Z like this and you want to do some kind of decomposition to it yeah it's going to have this this two-step pattern uh where you have an uh latent variable and then I mean it's not neces like there's no requirement that any of this be supervised in some sense the latent variable is like your guess at supervision we'll see ways to inform uh so for example we'll see an algorithm where the first way it was derived actually did use em but there's a more clever way to solve it provably but em is the general form of like there's a latent variable that you don't see and you have a model for it you estimate that parameter and then you solve like a traditional supervised machine learning or um kind of estimation problem under the covers yeah and that's very very general your latent variable here is clusters but we'll see it could be you know distributions and all kinds of fancy stuff later please students at least initial class or for example kilometers so no so oh great question I see where the confusion comes in no so here it's like we are almost memoryless so we had those you know Phi and and sigma and all those other things and now just like in k-means we throw away some information and we try to reconstruct it and here given our link of pis we try to compute all of those observed parameters which are going to be the mues and the sigmas and the phi's and blah blah blah the rest of the observed parameters so it's like given the linkage function we do that again and then what we would expect is that those parameters will not move around so much over time and that's when it will converge similar to the way K means word great great question yeah yeah uh there there is nothing that has such a crisp solution to my knowledge I don't actually know one but that's a that's a really good question how do you initialize this and and what situations can you can you better initialize it um it seems natural enough to think about one but I don't know a proof for one yeah it's a great question you can post it on Ed and I can dig around and see if anyone did that my suspicion is yes because K means plus plus was such a important thing it's a wonderful idea other questions all right okay so looking at this trying to think what I should tell you um all right so we have about nine minutes left let me see all right so what I'm gonna what I want to do is I think I want to I want to just tell you the steps that we're going to go through next time because I think over the weekend this will be too much traditionally I teach it Monday Wednesday but I'm going to want to redo this I think that's what I think so right now what I want to do is uh have a detour into one thing that we need which is convexity and Jensen's inequality so I'm just going to draw the basics here and then I'll give you a sense of what the EM algorithm looks like now the reason we need this is that this is a key result and it confuses people every year so I spend more time on it and hopefully it confuses them last sometimes people say it's trivial and then I'm very very happy so if you think it's trivial then I did my job okay all right so what we're going to need to do the reason I want to tell you this is what we're going to have to do is we're going to have to have a mathematical abstraction of like this going back and forth and is guessing back and forth and that's going to be basically two different functions one function is going to be a lower bound of the function there's going to be the actual loss function for the totally you know crazy probability distribution that has all the infinite models and everything jointly together okay the actual likelihood function and what we're going to do is we're going to have an approximation of it that says given we have a particular guess of the zi's we're going to have this lower bound and we're going to have to move between the lower bound and the upper bound and that and the lower bounding function and that is going to be facilitated by something called Jensen's inequality and it's worth understanding because it's something that you can use so I'll just get started a little bit on it and draw some pictures and then we'll we'll pick it up next time okay a b element of Omega and I would certainly rather answer questions because you know we can cut into later lectures so don't we don't need to rush okay a b is n Omega okay so let me draw this okay so something is convex if the line joining them oops is inside them okay so like if I take any two points in here any A and B and I draw the straight line between them or euclidean space so it's a straight line then if it's convex then no matter which points I pick the line between them is going to remain in the set this is a convex object so like an ellipse or a circle or something like that in contrast here if I pick points A and B here this is not convex okay and these These are going to be you know um yeah we're going to care about these quite a bit okay so what does this mean in symbols and symbols we have to check for all Alpha for all a b and Omega which is our set Omega Lambda a plus one minus oops 1 minus Lambda B that's the line between them is an element of Omega and Lambda here is an element of 0 1. okay that's all this picture is saying this picture in this math are the same okay and you need to check for all right right here clearly if I put a here and B here there's a line between them the reason it's not convex is there exists one pair of A and B for which I I go out of this set okay now we're going to use it when this bottom piece is a function right we think about the function going up to Infinity the graph of the function mean convex and what this tells us is when we if we look at chords that is points between the function like this they should always be lower bounds to the function and that will be a little bit opaque but it'll feel like you think about a function that looks like this the x squared function the that set will be convex that's the canonical convex function okay all right so what we're going to see is we're going to go from these definitions of convexity on sets to convexity on functions and that's going to allow us to basically prove the following statement which we will prove next time which looks mysterious but is not you'll see this is greater than F of e of x okay so there's going to be we're going to show next time just I want to I want to you know give a little bit of a road map we're going to use this definition of convexity to prove a theorem this theorem is called Jensen's theorem and it's effectively immediate from the definition like once you understand the definition of this for functions if f is convex this is true F convex a function is convex if it's graphis convex we'll draw that out okay once we know that we're going to need that for convex and concave functions that's going to be a key building block for what we do for the next couple of lectures we've talked about convexity once or twice in the supervised setting um but now we're going to need it in a little bit more detail okay all right so let me wrap up and tell you what we did today so that you remember what I want you to take away so we started with this idea of the difference between supervised and unsupervised learning we then went through k-means which was a really simple heuristic algorithm but you know in a hard problem that would find these clusters we talked about this idea that you needed to be able to model the problem to be able to solve it you had to input either K or something and you had to be able to check or at least visually inspect the answer we then talked about a generalization of k-mean what's called mixture of gaussians and the only generalization was rather than belonging to a single cluster deterministically you tried to find this probability distribution over everything that led to a bunch of notation but the notation still basically had this two-step procedure underneath the cover of Guess The Centers and then check How likely they are the How likely they are and K means was the distance in GMM it had this more complicated probabilistic model we like that more complicated probabilistic model because it's going to allow us to model even more sophisticated Notions of structure and then I think I'm just going to go through that on Wednesday so that you have all the mathematical details there together and we'll we'll do a review of that thanks so much for your time and attention have a great day"
Stanford CS229 Machine Learning I GMM (EM) I 2022 I Lecture 13,"Stanford CS229 Machine Learning I GMM (EM) I 2022 I Lecture 13

hello uh so welcome to our lecture on the EM algorithm so just to figure out where we are in the flow because we kind of have this flow of looking through a bunch of these unsupervised algorithms we got kind of got our uh you know hands dirty with k-means and GMM these were our first two unsupervised algorithms and what we're going to try and do is kind of generalize what happened there so that we can use it in many different settings and and move on from there so last time we saw these two algorithms kamines in in GMM if you don't remember the GMM algorithm was the one that we had these photons that we were trying to fit gaussians to maybe three gaussians that look like that don't worry about if you don't remember the details just roughly what we're dealing with and the big idea we encountered was this idea of a latent variable and the latent variable in this setting if you remember was this fraction of points that come from a source so we didn't know how many points were coming from each one of those light sources that were out there we had to estimate that once we estimated that then we would be able to go back and fit all the different parameters that are there okay okay um awesome so and that the fraction of points we also had to figure out the linkage the probability that every Source was coming from a point and then we could do the estimation and the main thing that I wanted you to take from both k-means and GMM was this main idea that we kind of guess the latent variable things is a great way to put it and kind of how we have in the notes you guess where the where they're probabilistically linked that is what's the probability of this these points belong to Cluster one this point belongs to Cluster two so on and then once you have that you then solve some estimation problem that looks like a traditional supervised learning so that decomposition is quite important and we're going to try and uh kind of abstract that away and then we would estimate the other parameters that's what I mean by a kind of traditional supervised thing now today what we're going to do is we're going to take a tour of em in latent variable models and try and cast them on a little bit more principled footing because last time the kind of the calculations were yeah it kind of makes sense that that's the average the weights in a cluster we'll try to derive the action that we're doing there from a more principled framework which is mle doesn't mean that it's right maximal likelihood is just a framework it happens to be though the framework that we use throughout most of the class there are others in machine learning by the way but this is the one we're going to use so before I get started on kind of the rundown any questions there I'll start to write oh oh please [Music] what is the first step in here now what do we guess we could guess randomly an assignment of every point to the cluster the probability remember there was this dij if you don't recall we'll bring that up later don't worry about now but we had to guess for every point x i which of the K clusters it belonged to and with what probability we could for example initialize that to uniform we don't know anything right and that's the thing something we may have some other heuristic guess that was what was going on in the k-means plus plus we have a smarter initialization but that's the how we get the process started once the process has started we just keep running those two Loops again and again and hopefully it will improve and we'll capture in what sentence it improves you'll see this weird picture of a curve that we go up and that's going to be the the loss function awesome okay so we're going to look at the EM for latent variable algorithms and this is where it applies this is what it's for is dealing with various different Notions of latent variables and I'll I'll say this right now maybe a little bit cryptic and I'll come back to it either at the end of today's lecture in the next lecture when we pick these latent variables there's kind of a little bit of an art what they're doing basically is they have that decoupling property if we knew this thing that we couldn't have observed then all of a sudden it becomes a really standard statistical estimation problem and somehow we are assuming structure and that's what we're putting into the latent variable so we're going to see walking through this today that structure we're assuming there's this probabilistic map out there that says how often you know how likely every uh point is to go to Every cluster in other cases we'll see more sophisticated variants of this idea but it's it's actually fairly profound that's the that's the real key idea we can abstract all the algorithmic details into en same way we did for you know the the election the exponential family stuff okay now before we get started I want to take a technical detour and so it's really important that we have sign posting here because I'll say why is this guy drawing these weird pictures the technical detail is I want to make sure that you understand uh this key result which is convexity and Jensen's inequality and the reason is I'll refer to this thing as we go through we're going to use it it's not like I'm just teaching you something for your health like this is actually going to be used in The Next Step it will actually in some sense be the entire algorithm like if you understand this in the simplest way then understanding the algorithm will make a lot of sense so become a clear point where we apply Jensen's inequality where we make it tight those are the things that we're going to think about as we go through it okay so we're going to do this technical detailer and I'm going to try and show it to you in pictures because I think it's the most intuitive way to understand kind of the basic cases if you already know it don't worry it's just another another proof that you'll see then this will allow us to go to doing the EM algorithm as mle and what I mean is we're going to be able to write down a formal loss function a likelihood function right that's what mle is we write down this loss function and then we maximize the likelihood and we're going to show that this actual algorithm is actually under the covers maximizing a likelihood function all right then I'm going to come back and I'm going to put GMM into this framework and this will answer some of the questions that we kind of you know iteratively heuristically answered like why are we estimating those parameters in such a way and that will allow us to say yep GMM is an em algorithm and so it'll give us a principle to solve for all the weights if you remember there was those cluster centers the mues and the sigmas The Source centers mu's and sigmas and fractions and we're going to solve for all of those and this gives us a principal way to do it because we're we're sorry we're in this mle framework okay so we'll exercise it basically exercise the notation and then we almost certainly will not have time for this today but I combine the notes and we'll go through that continue to go through them on Wednesday we'll go through what we call factor analysis okay and factor analysis is another model the reason I want to show it to you is it's different than gmms so it's a occupies a different space and it will kind of force you to look at the kind of decisions you're making right what are you modeling here and in particular we'll model a situation where like traditional gaussians kind of couldn't fit the bill because we're modeling something that's huge and high dimensional and we have to assume some structure to be able to get the whole program to work and by comparing these two and what's similar to them hopefully you get a pretty good sense of of what em is and all the different places that it runs okay all right okay so far so good so if there are no questions we're going to go right into our technical detour which will lead then into the EM algorithm as um as mle okay all right okay so here's our detour this is convexity and Jensen so this is a classical inequality and what I want to show you is that Jensen's inequality really is like convexity in another guys okay and it's a key result so I want to go slowly that's the only reason we're doing this okay so don't think that there's something super mysterious going on there isn't hopefully if I do my job well you'll just look at the pictures and be like oh yeah that makes sense okay here's the line I see what's going on okay so first we're going to define a set as convex when we did this last time but just recall a set is convex if oops if four any A and B element of Omega is that Omega sorry for any of it the line between them is in Omega so I'll write this in math so it's precise because n Omega okay so so what does that mean so let's draw the picture first and we'll draw the math here's a convex set so it means no matter how I pick a here's a and no matter how I pick B the line the straight line between them the geode stick between them those straight line is in a set this is convex okay in contrast just to see there's not a trivial definition this thing here which I drew very crappily but that's okay I'll draw it like this because actually you'll see why the bottom makes more sense to me in a second here we have one example so if I picked a here and I picked B here yep the the line is in the set but that doesn't prove its convex it has to be convex for all of this choices and here if I prove if I put it for B lo and behold this would not be convex okay so let me draw write the math while I have the picture so this is in symbols for all Lambda element of zero one so this is how a parameterized going on the line a b element of Omega Lambda a plus 1 minus Lambda oops accidental stroke Lambda B is an element of Omega okay clear enough what that means this is just the line between a and b right I'm just saying that no matter how I pick a and b and Lambda this thing still remains in the set which is just capturing this picture cool all right now we're going to use we're going to apply this to functions so given a function uh for right now we make it uh one one dimensional GF we're going to find the graph of that function as a set of X Y such that Y is greater than f of x okay f okay this is my definition so a function is going to be convex if it's graph is okay as I said okay so let's draw an example of this so here's my function here's you know zero here's minus one here's one and I draw this character okay so this okay so the the Shaded region here so this function by the way the one that's in my head is going to be f is equal to x squared so if you're trying to if you're trying to correct for my artistic shortcomings this is f equals x squared it's a parabola it's a parabola kind of a bull shaped function all right now no matter how I pick the points and clearly I should really only have to worry about picking on the edge so if I pick a point here a and I pick a point F of a I pick a point B and pick a point F of B the line between them right goes here it's not necessarily a straight line across I just happen to pick it that way go up it go down do whatever once okay now this function here will imagine we'll talk about a point Z later so I'll just say there's like a point Z that's going to live in the middle okay and this is this is B lives here let me erase zero and one because we don't really need them their values are kind of unimportant to us there's just so you knew what I was drawing okay I'm going to draw a all right awesome all right now what is this function well this is uh I think it's x minus 1 x minus 2 squared looks like this and then it is graph is everything up here and this is not convex for the same reason I could pick a point here I pick a here I pick B here and the line between them is below the set so this is a convex function this is not convex okay so let's look at this in symbols so so clear enough hopefully like trivial like oh you just drew two pictures twice and one you said was a function graph and the other one you didn't the function graph was open to the top but that shouldn't be really disturbing so far so good all right so what does this mean means for all Lambda element of 0 1 Lambda A F of a plus one minus Lambda these are as tuples b f of B is an element of Omega what does it mean to be an element of Omega it means that if I take any Z that's on the path Lambda a to 1 minus Lambda B so any character that comes in between here in here then it had better be the case that Lambda F of a plus 1 minus Lambda F of B is greater than F of Z right so this is now Z F of Z this is z this is f of Z does that make sense just translating the definitions directly in the more cryptic language we usually just tell you every chord is below the function I'm sorry it's above the function sorry I'm drawing the wrong way above the function what does that mean well a chord is just anything that connects two points here so like this character would be another chord it lies entirely above the graph of the function where the function actually lives here that's not case I just found two points so that the chord between them is actually below the function so it's not convex and intuitively the reason I drew these shapes is that convexity for shapes probably makes more intuitive sense 2D shapes but now hopefully you see they're really the same thing so your geometric intuition and the function intuition are the same modulo that we change this definition here okay all right now let me see all right so one other one other bit here so we're gonna we'll actually prove this I think why not sounds like a fun thing to prove if f is twice differentiable um and for all x f double Prime of X is greater than zero then f is convex okay so this says these functions really are bull shaped right second derivative being positive means that they have this kind of positive curvature that looks like they use right they're First Dimension the first derivative goes up and down but they're kind of always trending the first derivative is always getting more positive right it's negative on the left hand side positive on the right hand side that's what it means by bowl shaped okay so this isn't super this isn't super hard to prove but just because I think it will stall for a little bit in case you want to ask me questions I'm writing out a Taylor series for this F double Prime let's get a minus d square okay plus let me drag that guy so it's a little clear okay and this is this a to a is just something in a b so maybe you remember this from um you know your Taylor series all I'm saying is I can write F of a as some point F of Z plus some first derivative information plus some second derivative information and then I'm using the second derivative of remainder so I'm saying there's some point on the interval where this is true okay same thing for B this isn't super important for your conceptual understanding by the way like this is just to show that you know you can um that you can you know do what you want to do here that this this makes sense to you eight squared and okay now I claim it's convex so I just take what's the obvious thing to do I'm going to multiply this by Lambda I'm going to multiply this by one plus Lambda so that I get I have to make a statement about this right that's what's in my definition above okay well that's just the same as adding F of Z for Lambda plus one minus Lambda that's just F of Z so that's good that appears notice here that I get 1 plus a uh times Lambda plus 1 minus the Lambda times B well that's just equal to Z exactly right so I get 0 here plus zero right this is just because Lambda a plus one minus Lambda b equals e Plus these things are all positive plus some constant that's greater than zero so that shows that this thing uh this inequality holds F of Z please yeah this is this is Taylor's theorem great question so what's going on here if you remember Taylor's theorem is you can keep expanding and then you have the last term which is the remainder term and the remainder term says there exists some point that lives in a to B such that this holds with equality and I'm just using the remainder form of Taylor's theorem by the way if this this is really not important for your conceptual understanding I just want to show that it's one line that the statement which is like sometimes mysterious about derivatives and causes people's heads to explode like why do the derivatives connected to the convexity It's like because of this this is all that's going on awesome you can freely forget this and just use the fact this fact in the course okay okay stalling done any more questions Awesome the real reason we want to go through the derivative thing is otherwise this this thing which we actually do care about this strongly convex this definition feels like it comes from space aliens otherwise if for on a domain is if F Prime of X is greater than zero strictly greater than zero this is strict equality here okay that's where the strongly comebacks comes from or so this is actually strictly convex doesn't really matter but okay so for example f of x equals x squared which I told you was in my head well this gives me a simple test right its second derivative is two that's greater than zero it is the prototypical strongly convex function okay you also saw those one halves floating around this is to make this parameter sometimes with the curvature one doesn't really matter okay the other function that I had which you can check and graph yourself is x squared x minus y 1 squared this is not convex compute the derivatives you'll see but it's the one that looks like the two bumps right that's a it's a quartic so that's what it looks like has two bumps positive discriminant okay awesome so at this point what do I care that you know not too much honestly about this what I care that you know is that there's some way that you are familiar with geometrically what uh convexity means and you know that there are these tests in terms of the derivative the second derivative being non-negative is a good test for convexity and if you have a stronger condition you can get the strong or strict complexity okay all good all right now what we actually need Jensen's inequality now if I've done my job well this mysterious looking statement once I show you the connection you go oh okay that makes sense it's because it's actually just saying something about convexity but it's got a fancy name and it's so useful so it's the following statement the expected value of f of x is greater than F of the expected value of x so long as f is convex okay why the heck would this happen let's take one example suppose X takes value a with prob I don't know Lambda and X takes value B with prob 1 minus Lambda then what is it saying it's saying the expected value of f of x is equal to Lambda times F of a plus one minus Lambda F of B what is f of the expected value of x well it's F of Lambda a plus one minus Lambda B that's exactly the definition of convex the inequality this is convexity okay now one thing that is the other thing I want to to say for this is notice that this does not matter how I pick Lambda later I'm going to define a curve when I Define a curve and that curve is going to be as a result of sweeping some parameters in a high dimensional weird space but basically it says no matter how I pick the parameters of that curve anywhere that lives on this thing that's a probability distribution a bunch of numbers that sum to one in the discrete case this inequality holds and that's going to allow me to build a lower Bound for my function and I'm going to Hill Climb using it we'll see that in just a minute that will become clear are there any questions about this piece here all right now you may look and say okay well this is only in the case when there are two probabilities what happens when there are more you can just repeat by induction you have to do something fancier if you want something that's a full probability distribution this holds even if e is a continuous distribution I won't show you that because we're not going to go too far off we'll stop it you know kind of high school calculus sound good all right all right so now you know Jensen's theorem and hopefully you'll always get the inequality the right way and the weight region you'll always get the inequality the right way is you'll draw the picture of the function and see the chord's always above it which one must be Z which one must be F of z f of Z must be below the quarter of the function that's exactly this cool right now everything is defined in the literature traditionally for convex if you take convex analysis like it's the way we Define things we actually don't want to use a convex function here because we're maximizing likelihood and this is just notational pain right like if we were maybe we should have like you know minimize unlikelihood I don't know what we should have done but this is where we are so we need concave functions and what are concave functions G is concave if and only if minus G is convex right so we flip it upside down okay the prototypical one that we'll use if G of X for example is equal to log of x here's my picture of log of x you know probably not very good is that it's going to look something like this go off this way and notice that if I take a chord of this function right that's a chord it's below court is below right which is what I should hope right if I flipped it upside down the chord would be above cool now also there are functions that are concave and convex right so what if h of X is equal to a times X plus b it's a Line chords are both not a no longer above and below it's it's actually concave and convex linear functions are concave and convex okay ends the detour let's get back to machine learning okay so now we have the tools just to make sure what did I care that you got there you got this way that as long as we were dealing with probability distributions no matter which probability distribution we took we have this inequality we can get lower bounds we're going to use that in a second to draw some curves of a likelihood function that will hopefully be easier to optimize than the original function and we'll try an iterative algorithm that will look exactly like we talked about before and the way we'll conceptualize it is we solve for some hidden parameter we solve and get a that gives us an entire family of possible solutions we solve on that and we iterate let me draw the picture after I give you the formal set okay oops all right so em algorithm has Max likelihood that's Max likelihood I'm just gonna put mle all right so remember this is the max likelihood formulation there's some Theta that lives out there we have some data I from 1 to n these are our data points we take a log of the probability that we assign to the data given our parameters this is a way for us to compare different parameters right and recall these are the parameters params and this is the data so far so good right now we're working with latent variable models so latent variable models mean that P has a little bit of extra structure PX data this is a generic term right this is just one of the I terms it says the function factors this way looks like a sum over Z where Z is our hidden or latent variable p x z Theta this is a latent variable right so remember Z was our GMM latent variable the cluster probability right so we have to sum or marginalize over all the possible choices of Z this is basically saying I don't know what Z is I have some probability distribution that I can compute over my data and Z given Theta and I don't since I don't know what Z is I the term is I marginalize it out means I sum over all the possible values and this will get me back a probability for X right this is a sum over all possible Z's this will leave me with a probability for x or sorry probability for x is that clear ask a question please yeah wonderful question it's a property of the model in a real sense when we make a modeling decision we say there exists some structure out there like there exists a probabilistic assignment between photons and point sources you know one version of the prior is I tell you exactly where every Photon comes from that's clearly a very strong prior if you knew that God speak go do it you just solve GDA If instead what you know is there exists some mapping that's out there then that structure that you're putting into your function and what I'm saying is that mathematically comes down to baking exactly the same okay and this is the mathematical form for all of those latent variable models so when we have that idea about latent structure we'll eventually put it into this mathematical form and we'll see a couple more examples wonderful question in GMM this was exactly the Z there the notation isn't an accent it's the same Z yeah so the example that we had was the basically the whole lecture where Z was the probabilistic linking between sources and photons yeah yeah so that's that's one we'll have more examples later but I want to get through the algorithm in this abstract form and we can shoehorn more things into it and what I'll do afterwards is put GMM right down in this language we need a couple more things please coming from a particular cluster um what is it expert like what is probability of X parameterized by Theta I think actually represent in this case in that like Photon example yeah exactly so remember if you there was I think it was said yesterday by someone here on that side of the room so I don't know if that's facial recognition helps you in the last lecture but was it was like imagine I was guessing all the photon models that were out there so each one was parameterized by some choice of zi and then what I'm thinking about is what I want over that is that like across all those datas no matter how i instantiate z each one gives me a different probability distribution I can sum them up and that tells me given this data no matter is assigned or across marginalization cross always that it's assigned How likely is the data so PX data we've been using forever we use that from the supervised days we just inserted Z and said like well there's this wild Z that we can't observe but it somehow constrains X it means that X like the relationship between Theta and x and that's what the model does awesome question very cool these are wonderful questions I'd much rather answer these than badly draw the pictures that come next we're going to get to those pictures no matter what so there's really no saving us all right let's get to the bad pictures all right so I'll try and leave this more or less on the screen here's the algorithm this is a picture which you know maybe won't make perfect sense to start with but we'll get there I'll go all right so remember what a loss function looks like I'm drawing everything in 2D but of course this is in like horrible High dimensions this is my access is Theta and then what I have and I apologize I will use a bunch of colors I hope this is okay for people to see um if not let me know doesn't look like the most visible color but doesn't look like the least visible and I need a couple this is my loss function L Theta okay so this is I'll write that in black there this is l l Theta okay so this is my lost curve okay this is L Theta here now remember it's not a nice concave or convex function right we wouldn't expect it to be we would hope because we're going to minimize it that it's concave that would be nice if it just looked like this like oh that'd be so great we would just climb to the top but we saw in a lot of the problems that we were after it doesn't look like that it has these kind of weird bends so we had to settle that's another way of copying out and saying we had to settle for these local iterative Solutions that's all we're after we settled for that in kmm for in K means and we're going to settle for that in GM okay so how does the algorithm work we start with an initial guess now again you could ask these colors seem harder to read you start with an initial guess so Theta let's say at time T so it could be times zero right this is just the initial guess whatever it is then what happens is this is mapped up to here which is L of theta T I haven't written anything I'm just giving notation this is just the value of the loss that I currently have I suspect there's a there's something up this this way I'd like to get so how do I do it how do I what's the algorithmic piece what I'm going to do is I'm going to form so the problem is optimizing over all those Z's seems daunting directly optimizing the L's so instead what I'm going to do is I'm going to come up with a local curve okay and I'm going to call this curve l t of theta it's another function I'm only drawing a piece of it but it goes the whole distance right it's some some curve now we'll pick LT usually to be some nice kind of convex function something that's easy to optimize right so we're going to try and get that kind of easy to optimize function and then what we're going to do is we're going to optimize that function we're going to find it's it's local maximum so it's local maximum for the sake of writing is let's say here and then we're going to set that to be Theta t plus 1. okay and this is now L of theta t plus one and we're going to again create some new curve LT plus 1 of theta based on that point okay and the key aspects of the point that I'll write in a second is this point is a lower bound this curve is a lower bound it's always below the loss so it's kind of a surrogate that I'm not overestimating my progress and it's tight it meets at exactly that point so if I did happen to have the actual optimal value it would meet at that point so I wouldn't think and get fooled that there was a higher loss function somewhere else let me write those two things okay so first LT of theta is going to be less than L Theta we'll call this the lower bound property LT of theta t is going to be equal to l of theta t sometimes call this the type property okay Our Hope is LT is easier to optimize than l so this picture the content is we're picking these like that was a really bad drawing of one but these picking these concave kinds of functions which are easy to maximize right that's what I mean by it kind of looks like a supervised thing then we maximize that and this is formalizing the back and forth we take that new maximum that we have which is our new best effort of parameters and then we then do it again and create another curve now the way we're going to create that curve you're going to see in one minute is going to be Jensen's and that's the whole algorithm okay so I'll sketch the algorithm you don't you don't have math to talk about the algorithm but hopefully it's clear like what's going on easy to train surrogate we kind of slowly Hill Climb with that easy to train surrogate alternating back and forth and this is what we were doing in k-means this is what we were doing in gmms as well and just so it's super clear I want to make clear here Phi t plus one this is nothing more than the ARG Max over Theta of LT of theta means I do Ma I do the optimization on the surrogate curve that I created all right I think that this description hopefully gives you some intuition of what's going on because otherwise the math is kind of bizarre um looking but we'll see so this is the rough algo I'm just restating what's in the thing I'm not giving you enough math this is going to be called not surprisingly the e-step this says given 5T find this curve L of t and then the m-step and together at em given L of t set by T plus 1 equal to ARG Max by lt5 right so we could imagine doing some kind of uh gradient descent here but it's not clear how to deal with this marginalization that happens in the middle so if we did like some marginalization or some sampling we could do something that looked like that but it's because we have this decomposition we also know we have you can also imagine that we have like a decent solver for the inner loop because it's this nice to solve thing I would say over time this split of like what's nice to solve and what's not right now I'm pitching it as you know kind of to you as like it must be concave and so it's nice but this kind of just means like I have an internal solver that's like fast and I kind of trust and I have something on the outside that's a latent variable that I'm like splitting up the modeling it's it's one of a number of decomposition strategies doesn't mean it's the only way to solve it though wonderful question cool all right so the question is how do we construct L of t and I claim we know everything else so we'll come back to that claim in a second okay so let's look it's going to go term by term so let's look at a single term in our equation okay all right so I'm going to grab one of these characters just one and work with that okay so how do we construct it so right now we're trying to understand how to create this lft from this function and you should roughly be thinking because I told you that Jensen's will have something to do with this okay now what we're going to do to put it in the form where Jensen's could be used looks wholly unmotivated okay totally unmotivated but it's to shoehorn into what we're doing and there's there's some motivation but it's kind of opaque so let's see what I'm going to do is something which at first glance seems strange now this is true formally for any Q Z that I pick okay please so here I'm just introducing Q this is true for any cue right let's not worry about support issues but like I'm just putting in something that divides by one seems sort of unmotivated to do this now I'm going to only consider we get to pick Q so I'm going to pick cues so that I can use Jensen such that it's a probability distribution over the states such that the sum over Q Z equals one and Q Z is greater than or equal uh to zero okay I'm gonna call this property star okay so I'm going to pick Q as a probability distribution I'll write that in a different color okay why because now I can make my argument one line that's the real reason so how does it work well okay good so we have this character copy and here this can also be written oops oops I don't want to use blue this can also be written as an expected value where Z is distributed like Q right of this weird looking quantity why is that well it's just the definition of expectation it is just symbol pushing there is nothing deep going on but it's important symbol pushing because it means Jensen's Applause oops log of this thing sorry damn it I forgot a log okay I'm just transforming this thing internally into this notation yeah please Q is this function that we picked up here so Q is just some probability distribution and this is going to Define our curve getting just getting a little bit ahead of ourselves we're going to allow the curve is going to be parameterized by whatever probability distribution we want so it's our degree of freedom freedom I'm just telling you something that's going to hold no matter how I select the probability of distribution the tool that I have in my Arsenal to do that is Jensen's inequality now I've turned this into an expectation and in one line I'm going to be able to turn it into a lower bound that works no matter how I pick it up graphically what I'm doing sorry to confuse folks who are copying is is basically show how to construct this LT that's always a lower bound everywhere and that's where I'm going to use Jensen's inequality so let's see that next line we'll come back to this so this is less than I can pull the expectation out p x z ETA over Q Z this is Jensen okay log is concave this is equal to sum Q Z times log p x z Theta Q Z again just simple pushing okay so there's only one content line here okay the key holds for any Q satisfying star okay no matter how I pick the probability distribution this chain of reasoning goes through please I believe the expectation yeah so this was exactly Jensen's inequality so if I scroll back up this was Jensen's inequality but because I was applying it to the the negative of it it's exactly the same piece right it reverses the inequality and so I'm just I'm directly applying that reasoning oh this thing into this thing yeah yeah sorry so this is just because Q of Z is a discrete distribution and the definition of expectation is this is a bunch of numbers that sum to one so this is the dis this is an expectation with respect to some distribution in particular the one where z i occurs with probability Q of Z see I that's it it's again just simple pushing please [Music] yeah so you want to know how we ground it into an example is that what you're asking no no so there's no Phi here so apologies if there's something difficult to read there's a Theta here there's this new q that I've introduced Q is something that I've artificially introduced and I'm just saying that like I've just proved all I've shown here is that I have a way of uh if you pick a cue that satisfies this I have a I have a way of lower bounding this function getting a family of lower bounds to it and I'm trying to give you the intuition of why I might want to do it it's so that I can construct those curves that come later because now this function is going to be much nicer to optimize but we haven't quite gotten there yet awesome okay so this is gonna this whole thing is this gives a family this is just what I was saying there so you're you're right right ahead of it this gives a family of lower bounds namely this is how I get L of T Theta less than L Theta because term by term it's going to be low it's going to be less than or equal to now it's not it doesn't satisfy all our requirements because we have to make it tight so how do we make it tight that's the next piece but right now I have a way of going term by term from the likelihood function and getting lower Bounds at a particular spot and it'll be a lower bound no matter where I am but I have to pick a certain cue to make this operational that's the piece so I have freedom and pick q and I'm going to pick a very specific cue and that's going to give me a lower bound and that's going to allow me to get the curve good yeah yeah so I said I was going to ignore the support we can imagine just for the sake of this lecture that it's strictly greater than zero so I don't run into weird things about what I mean by divide by zero here because I'm controlling the multiplication in a head at a time like it does make sense but like you're right to point that out so just just think about it as greater than zero yeah wonderful question cool all right all right so now how do we make it tight so what we have to do the ins the the intuition here is that we want to make Jensen's inequality tight and the idea is if what's inside is constant imagine there was a constant inside that this term was constant for all the different values of Z the expectation clearly doesn't matter right so if they were all the same then these two would actually be equal to one another right this is some value Alpha and then you would get a sum over all the alphas that were there they would sum to one boom done if they all have the same value here Alpha they would sum they would uh they would be here in the log and they would also sum in the same exact way so as long as this term is a constant that is it doesn't depend on Z I'm in business right so what that means is I want to pick Q such that log P of x z Theta over Q Z equals c now before I had all kinds of freedoms to pick whatever cue I wanted now this is where the probability comes in so Q Z has to be related in some way to p x of Z for this to work go ahead uh what is z or C oh c is some constant this is a this is a constant independent of the uh you know just for some constancy it does not depend on Z independent of Z we don't care what its value is we just care that it doesn't depend on Z in anyway and then it will be exact equality then Jensens will be equality okay all right so what is the Natural Choice well it's that Q Z should equal p of Z given X Theta why is that well this is also equal to so this is because P of X of Z of theta equals P of z x Theta P of X Theta so if I plug these in they cancel out and C is equal to log p x of theta okay so let me make sure this is clear note this just means note well and B I just reuse it reflexively just a signal QB does depend on Theta and X so we're going to have this notation q i of Z because it depends on each different value so each data point is going to get its own different Q which is the log of How likely this thing is okay and we pick those for each eye so we're because we did this term by term we can pick that Q q1 Q2 Q3 all different and we pick them all so they satisfy this equation okay this thing has a very famous name so I'll write that while I was kind of stall for more questions so what we've defined here it's called the evidence-based lower bound or the elbow which actually has a fairly like if you say elbow to a machine learning person they actually know what it is nothing we're making up it's a real thing so the elbow of x q Z equals the sum over Z Q Z log p x c Theta over Q Z okay and what we've shown is that L Theta is less than or equal to or is greater than or equal to the sum because we did this term by term of the elbow of X of I Q of I Theta sorry this is incorrect notation this is Theta sorry the Z is marginalized away the Z can appear again okay for any QR satisfying stock sound good that was just restating the lower bound all I said is we went turn by turn through this thing so it holds for every term that we can pick Qi as long as it's a probability distribution it's a lower bound and then we also showed that L Theta t equals sum I 1 to n oboe x i q i Theta t for the choice of q i above okay so hopefully that picture makes sense again just to recap what's going on here we have this opportunity to pick these these bounds and we'll use them in a second so hopefully become more clear exactly what we're kind of optimizing for here what we're going to do is you'll see how we picked the qis and all the rest in a second but this is basically saying that it satisfies the two properties that we had before we're going to pick where we are on the curve we're going to find this Con this kind of you know bull shape upside down thing we're going to then optimize that thing in a second pick our new Theta T and then repeat and do another curve all right so let's let's do the wrap up and state the algorithm now with our our newly hard-earned language oh no so this is the these are both on the original loss these are just saying these are this is the LT here capital l this is each one of these is capital L basically right and then this one here is saying that at the particular point for that teeth instantiation this is where we are yeah all right so the wrap up is as follows this is how this is what we can now write down the algorithm in the kind of full generality with mathematical Precision although it may still be a little bit opaque we set Qi of Z in the E step equal to the probability that z i given x i and Theta okay 4i equals 1 to n okay so this says that you're going to pick the QI distribution that says you know what's the probability that's kind of you know uh most informal or the exact probability that comes from your model knowing the data and the current guess of your parameters right so you have some Theta at some time you plug it in you know the data point that you're looking at you condition on that you say what are the most likely values of the cluster linkage as we were talking about before the source linkage for this particular point you get a probability distribution over those you set them to qiz that's really what's going on it's your estimate of How likely that is then you take an m-step Theta t plus one equals ARG Max over Theta of LT Theta which equals so LT Theta sorry like this LT theta equals this elbow sum x i the QI data okay your current guess of parameters so basically what it's saying is you give me a current guess of parameters I get the lower bound that's underneath the covers then I optimize that lower bound surrogate I get the Theta t plus one that gives me a new guess of parameters which defines yet a new curve a Qi for each one of what's going on then I go back yeah please Q oh sorry yeah good call q i and this is Theta and I'm inconsistent with the semicolons too so you move this so there's a good distance distance this is an X this is a Q This is a Theta awesome please right so it just as before T starts at zero we have that initial guess let me go from there Theta is our current yes all right why does this terminate and it's basically for something that's kind of not very interesting or satisfying but it does this gives you a sequence that is monotonically increasing or non-decreasing okay so it's possible that it would grind to to a halt but eventually like you know it has to be strict or some other things about how fast it terminates but it's not it's gonna it's a monotone sequence so we'll have a convergent subsequence it's really all that matters we don't say how fast it converges it's a separate issue is it globally optimal well no no just look at the picture and so to derive a counter example you would just find a likelihood function that had those two bumps and you would run it in a particular lower bound setting and what it will do is it will gradually Hill Climb and this this is actually not great like it can't go back downhill right it's got to just continue to go up if it gets locked inside one of those bumps it's kind of toast so in summary what we saw here is we derived em as Emily okay as promised okay so just to recap what happened here we started with this notion around Jensen's and convexity so we looked at pictures of convexity and we got an intuition of what sets our convex and what sets are not we wanted to use concave functions which are these kind of downward facing things the chords are always below them okay those are the loss functions because we wanted to maximize them the reason that was important is we had to do this back and forth iteration given a set of parameters we were going to find a surrogate that surrogate was going to be concave in our setting it's going to be one of those nice functions that we were after we would use Jensen's inequality as a way of constructing that entire curve we needed the entire curve because we wanted to optimize it so it wasn't enough to find a point in a lower bound we needed to find the whole thing that was underneath it so we could run our ARG Max step and that was the setting where we learned all the parameters and estimate that in a way that was hopefully nice and easy to do right which was like you know estimate the means and the variance of the data that we're given we'll run through an example of that this is a necessarily kind of abstract and confusing algorithm the best way to understand it is just to run it through a couple of the different examples em and the next one factor analysis and by the end you'd be like okay that makes sense it's a lot of notation because we're abstracting out a huge number of things that we're doing okay but in the end it's not so bad right you take the qis in this way except the thetas do dissent on them or sent on this case do arguments okay all right so let's see it for our uh gaussian mixture model please uh so the termination condition is not really important or in the classical sense the thing is is that it's non it's non-uh decreasing so that like eventually there's a convergent subsequence of it and not telling you how fast it converges for example and it converges for the same reasons that gmms converge that we were like kind of going downhill if you remember it every time like there was some loss functional that was decreasing and this is just saying there's something that's continually increasing when do you terminate it operationally like you're running this algorithm when do you decide you look and see if the loss of the likelihood function is not changing too much what is too much depend on depends on your data depends on the problem like sometimes point one is good enough if you have a huge amount of data and you're averaging over billions of examples sometimes if you have uh you know only a a small amount of data you want to get to like machine precision and 10 to the minus 16. and so that's the way you decide when to do it this just says that it's not going to oscillate wildly it's a very weak statement what I'm making yeah please part of this is linked to the mle sort of aspect of it oh awesome yeah so we're gonna see the mle when we actually do the the computation here the reason it's linked to mle comes from a very simple piece which is we started in this model where we were saying the way we're going to think about the world was to maximize the likelihood and that was like how we think about our data that's less disturbing to this group than it is so like I guess generally worldwide who think about this because like this is the only framework we've used in the course but that's what I mean we started with L Theta as what we were optimizing and then we derived this as a set of concerns we didn't get to a global Optimum so I don't mean that like we definitely guaranteed that we got the maximum likelihood estimation just that you can phrase what's going on as mle and so when you get into other estimation problems and the sub problems you just apply the mle stuff you learned from the first half of the class and we'll see that in an example does that make sense awesome thank you for the question please uh oh it's tight because we went through this this small piece here which was that if we selected it as a constant in this particular way so before we could pick any q and it was a lower bound as long as we did this then actually this line was no longer an inequality but was actually exact equality and it depended though that selection of Q depends on Theta and X awesome great question yeah and that's just making sure that the picture in your head is exactly right we go up to the loss curve we get something that's underneath it that touches at that one point and then any optimization we do there is actually also optimization on the loss curve itself cool all right um four mixtures of gaussians or we call them GMM sorry all right all right so what's the e-step huh yeah I'm just going to copy down the thing because let's get the generic algorithms let me get the generic algorithm all right all right just so we have it on the screen so where's our warm-up not really a warm-up because we're almost out of time but here's remember if we saw how this worked P x y i and z i remember we factored it as a following this is just Bayes rule nothing nothing crafty going on here not tricky zi was a multinomial okay this means Phi I greater than zero sum Phi I equals one okay and this was remember our in cluster J and so then once we knew given zi equals J then every cluster had a different shape so we had a different mean mu J and a different size or a variance mu J and I'm doing everything in one dimension but in two Dimensions you would have actually the whole covariance would be different these are the cluster size descriptions cluster means okay right zi is our latent variable all right so let's take a look what does em actually do here so what does he know em is very general you can instantiate it right so what does it mean here so Q uh I of Z is going to be equal to p z i equals J given x i and so forth okay now what actually happened here when we wanted to understand what was the probability this is this is the probability that I the ith component belongs in J given what we've observed about x i and what we know about the cluster shapes and their frequencies so if you remember we had this this diagram that I drew quite poorly the last time that said we had these two bumps which were our two gaussians let's say in one Dimensions that look like this this was mu 2. Sigma 2 square this was mu1 Sigma 1 square and the question is you give me a point here this is my x i How likely is it to belong to one or two cluster one or two right that's basically what we're asking what's the probability that this point this ith Point here comes from one or two now remember if we just looked at this and these points and these two distributions were equal where the the fives were equal that is both sources were generating the same amount of information then we would say oh it's probably much more likely it belongs to this function cluster one then cluster two but if we knew if we knew say on the other hand if we knew Phi 2 was hugely bigger than Phi 1. right a billion points came from the second source and only one point came from the First Source we'd probably say it's more likely that it would go to this right it would certainly boost its probability so now the question is how do we automate that reasoning and that is Bayes rule more likely and two so the automate this this is Bayes rule this is all we did last time Bayes rule it just weighs those two probabilities and tells us what should happen that's it okay we ran through exactly those calculations last time right let's take a look at the m-step now in the m-step we have to compute derivatives I want to highlight only one thing here because it's something that causes people pain when they do their homeworks we have to compute derivatives so we're maximizing here over all the parameters Phi and mu and sigma Sigma is sorry all the covariances so these are the sigmas lowercase and the notation above these are all Theta or all yeah all Theta right so Theta is refers to all the parameters of the problem we were breaking it out into mues and sigmas and fives those are all the things we're observing everything that's non-latent that's observed not hidden to us and when we were maximizing over from our elbow lower bound was this sum over z i q i z i log p x of I see I Theta over q i of z i okay this whole thing we're going to call Fi this is f i of theta okay hides a time in our notation all right so this thing is let's write it out because the Gory details will help us oh please there's a question he is just latent so I'm giving you the intuition that's something that's hidden or not observed but formally it's just going to be anything that's a z z is latent that's our definite please foreign exactly right yeah this is exactly the instantiation of what we had above we reasoned about this through ad hoc reasons last time but it is exactly the elbow that we're now going to minimize with derivatives and to make it concrete I am either going to waste a bunch of your time or something will snap in your head and see how these things put together I'm going to write out exactly what f i Theta is so that you can see like what the derivatives are that you will compute on this thing because right now it's probably pretty mysterious to you like there's elbows and there's peas and there's cues and like you can just write this thing down in computers derivatives and that's that's what you do I mean that's how this this whole method works just abstracted it like three orders of magnitude more than it should be okay so let's see that piece oh please yeah that's gonna be so I'm just using that annotation to make sure it's clear that it depends on the eye it's actually just a z that you're summing over and it's summing over for example uh like we're imagining that it's discrete to make our notation a little bit nicer it would be summing over all of the different clusters that are possible there all the different sources How likely are you to be in cluster one two three four five so on you could also we'll see later replace it with an integral if you had something really fancy that was there like if you had a continuous distribution over the hidden States yeah Frenzy what to feed us fee is greater than you know one source of culture yeah yeah so it is in fact because of this right here which I kind of glossed over Qi is exactly setting that is extending this function so I gloss over this really really quickly because it was the same calculation we did last time pzij to compute that we remember we expanded it by Bayes rule we had two different components we had if you knew you were in a cluster How likely is the data point and then we had a term that said How likely is the cluster and those were the two functions that we put in and broke down by Bayes rule it's exactly the same you've got it perfectly yeah all right so let me write out this monstrosity just because it'll be potentially it has it has been in the past educational who knows if it's educational in the future in the future being like two seconds from now all right I'm going to use a notation and hopefully it doesn't confuse you uh Qi equals ZJ so this is the the piece there so this is the weight this wi is the same wi we had before I'm sure you're intimately familiar with all the notation I use in the GMM lecture but it's the same w i j that we had before it's the weight that summarizes this probability just so I don't have to write that whole thing up okay all right so f i of theta is going to be equal to the sum over J because now I'm summing over the cluster centers right the zi notation was still very abstract wji which was summing over this part here log and help us all 1 over 2 pi this is the covariance one-half this is the x of one half x i mu J Square well I decided to write this in for General things why do I care about that oh I see why okay transpose Sigma inverse X I mu J times by J oh I just missed it oh that hurts all right let me scoot so much better here on a whiteboard that's really catastrophic 5j okay let me make sure the brackets are clear I'm going to highlight the brackets like it's a syntax editor so make sure they're all there where they're supposed to be this blue goes with that one so on okay great and that means I'm missing a log perfect all right not so bad oh and this whole thing is unfortunately snap two yeah over w i j right that's just this piece is this piece this piece here this is the probability this is the gaussian remember from our model let's go back up here sorry for all the scrolling this is our gaussian here this is the calcium distribution with Center J I did use a higher dimensional covariance because it's something you're going to have to compute so I've gone from 1D to higher Dimensions the notation doesn't change except for this is what the gaussian looks like instead of a square you you know that already and then there's there's the Phi J which is just multiplied times this you know horrible expression and this x parenthesis is so I don't have to write it in superscript right just expert the function just a bad habit that I always use brackets for this it's historical and I would love to beat it out of myself if it were possible please uh right now the covariance does not depend on Z in our model the covariance here oops the covariance here is something that's that it depends on which cluster right so it depends on J sorry I just want to make sure I understand what you got yeah so I think if it means it depends on J yes the covariance could have different shapes sign can be long and skinny some could be short and round yeah those depend on J so this thing here is a very polite way of saying this guy here should depend on J yeah good catch all right so now we can compute some some fun derivatives okay so let's compute mu J of f i of theta okay we have to estimate the mean right now and I'm going to do actually I'm going to do something slightly harder so apologies if you wrote that down let's do this there'll be just one extra line because it's all linear I'm going to sum over all the data one to n okay so what this becomes is I sum equals 1 to n this is over all the data I get mu J here mu J times and then it's going to be WJ and I'm going to drop terms inside the log that obviously have nothing to do with mu J one half X mu J there's I uh T Sigma inverse J x i j all right and so just so you're clear what's going on here the log you know turns these these multiplications into additions so when I take derivatives like this doesn't show up anywhere because it does Sigma doesn't depend on it and that means sorry it doesn't depend on mu and this doesn't depend on mu either so I'm left with these terms please go ahead just the term here and a function it is it is the likelihood function after we've picked Q at the particular iteration so it's just notation so I don't have to write this monstrosity every time I kept it um yeah wda doesn't have anything to do with uh Lambda J so it should be crossed out is that true let me make sure I'm gonna mix something something crazy here no it shouldn't have anything to do with it but it will be multi sorry all right I see what's going on this is this is what's going on this is one half and this is a minus w a j is multiplied by it wij is uh take the derivative of the log it's going to be this times this thing plus yeah sorry thank you for the the notational issue yeah cool all right we're in business so what happens now well some mechanics that almost certainly will introduce bugs and you will catch and it'll be great that's that's learning happening there and me making mistakes okay so when we actually compute this this is going to be Sigma J x i minus mu J you computed this a bunch of times all right so um yeah all good so we can so when can we pull this thing out that's repeated because it's full rank we can pull it out and and it's linear and we can it doesn't change anything so we want to set this to zero and use that Sigma J inverse Sigma J is full rank that will become clear in a second why that matters so much because when we pull it out what do we get we get here Sigma J uh inverse times sum which is an unfortunate Collision I equals 1 to n w i j x i minus mu J equals zero okay but then because this is full rank the only way that this thing is zero is if it's identically zero right if this were non-full rank sorry the J is in the wrong spot that's extraordinarily confusing if this Matrix since this Matrix is full rank for this thing to be zero means that this blue part is identically zero and so what that tells us is Mu J should be equal to sum I w i j x i over sum w i j yeah that's before okay so so far nothing happened we estimated the means by simply averaging their weighted averages and we computed this before and it's just a matter of computing the derivatives the one that I actually care about showing you by the way is 5j so let me just jump to that because we only have a minute or two left and I want to show you what happens in 5j so 5j is constrained please sure okay sure also I would say you know ahead of time I do post all the notes online please feel free to take a reference to those notes too they will have potentially fewer typos than me trying to answer questions draw and generally be distracted can't focus that well I have to read them many times they still do have typos though so always look at the notes all right let me just show this one thing 5j is constrained okay so 5j is constrained and I just want to remind you of something that you probably learned in high school or you know like freshman year and calculus I don't actually know when anyone learns anything anytime I say something that my students always get upset with me so I should just stop but I assume you've seen it before this moment about that you need a lagrangian okay no you haven't seen it that's fine too I will put if you want I'll post notes about how to compute lagrangians as well if you haven't seen this before this will trip you up in some way so when you compute the derivative with respect to Phi J what happens is you're going to get something that says you have this weighted sum w-i-j times uh the derivative of Phi J log Phi J plus so if you just so if you just take this and compute the derivative it doesn't account for the constraint okay so you have a bunch of numbers that must sum to one so if you think about a if you think about your honest you're on a line let's say that you're optimizing on a line right if the gradient like let's say that your points are on are on this line and you're saying like I want to optimize here this condition that you could imagine for an optimal solution is the gradient's identically zero right the the advantages that's good that's a point but what if the gradient is perpendicular to the line like it wants to push you only perpendicular and has no component moving you along the line right in that case this is still a critical point it's still potentially a minimum does that make sense because it's not telling you that there's a minimum to your left and right it's you know along the line okay so the question is how do you encode that information that you don't you want to kind of screen off information that's orthogonal to the line and I'll write up a little note to show this this whole thing what you do is you introduce this thing called LaGrange multipliers and LaGrange multipliers and if you haven't seen them don't worry these are super easy to teach just say this it's just an extra term here and this multiplier it's not obvious in this formulation what it's doing but this multiplier is basically the thing that's screening off things that are that are that are uh orthogonal to these constraints okay so this constraint here says Theta J is equal to one and you set this constraint equal to zero This this term equal to zero and it says if you're going off in a direction that that would not change any of their values that's okay you get to screen that off and I'll make that geometric intuition I'll just post a one-page write-up for you please remind me in the thread and I will I will definitely do that if you don't do that you'll get the wrong answer that's also a motivation to learn it um and so what ends up happening here is you get something that says I get some I goes from 1 to n w i j over Phi of J plus Lambda equals zero and this implies that 5 of J is equal to 1 over Lambda sum I equals 1 to n of w i j okay and the Lambda is playing a very simple role here it's just telling you like you have to normalize them in some way right now since in this case we can do it in an ad hoc way since Phi J is equal to one this implies the sum of Phi J is equal to negative 1 over Lambda sum i j w i j and this equals negative n over Lambda okay and that's the correct normalization right oh sorry this is equal to negative n oh sorry n over Lambda right and so now you can then go back and normalize and cancel out the Divide everything by one since this is just this sum is equal to one I divide everything by 1 here and that tells me that Lambda must be equal to uh you know 1 over negative n right this is equal to one so this implies Lambda equals negative one by n it's just normalizing it's just doing the average which was weird to look at before but that allows us to compute all the things in the way we'd expect here it's totally natural so if you don't get the general rule that I'm trying to tell you the reason I'm trying to tell you is I think we make you use it at some point this rule so just check it'll come up on a homework I don't think it comes on an exam but just flag something when you have a constraint when you have a constrained probability distribution you have to use a LaGrange multiplier that's all I care about that you understand in this case it makes total sense though because these numbers have to sum to one so if you don't have a normalization constant here you're adding up a bunch of numbers which individually sum up to n right the sum over all of them is n you better normalize them in some way and this is just the principle that tells you you have to normalize them by by this n Factor okay so all I care that you take away if you've seen this a thousand times before don't worry I hope you had a nice rest if you've never seen this before I just want a flag for you when you minimize a function that's constrained make sure you use LaGrange multipliers I will put up a little tutorial about them you do not need to spend a bunch of time on them it's just if you see some challenge where you're actually supposed to some problem where you're supposed to do it just have a little light bulb to go off says okay I got to look up how to do it in this case that's all I care about okay and you'll Trace through it in the notes please [Music] yeah it equals to minus sorry equals minus 1 over n because there's a negative sign everywhere so this minus Lambda is going to be equal to 1 by n so I'm just going to write the final expression maybe that'll be less sorry oh this this arithmetic is what's bothering you sorry sorry this is true yeah yeah sorry I just swap them and even that's backwards from how I should do it I didn't see that sorry thank you right it's just supposed to average out so it looks the same awesome any questions about this all right because it's a probability distribution so again the issue here is Phi J is constrained by the model so if we go back to this model this is a constraint on Phi J so whenever you have a probability distribution a multinomial probability distribution it's not just that the Phi eyes are non-negative which is the constraint we're almost ignoring but is that the Phi eyes equal one and so you couldn't for example set your probabilities to be 0.5 and 0.8 right they have to add up to one here because it's a multinomial so when we do the optimization we could for example prefer an optimization where we make all the probabilities one but that would be an invalid setting and so that means these five J's we have constrained them oops constrain them to equal to one so now the question is when we do the gradient descent which is exactly the gradient descent we did before where does that show up and it shows up in this extra term here which is the LaGrange multiplier this thing is called a LaGrange multiplier the multiplier itself is called that okay and this is the constraint put into the normal form if you haven't seen this before it'll look quite mysterious but what I was trying to do is I'm not going to teach you LaGrange multipliers in this class I'll put up something but the piece is here that it gets you back to an expression which makes sense in this setting and you needed something to average over because these numbers sum up to something that looks like n if you just compute it naively you'll get something that doesn't make any sense please no no there's no Sigma in here Sigma like for solution uh no in this setting the sigma's out here I think what's confusing you maybe is this that these are not the same J's which which line oh oh oh I see I see I see I see sorry sorry yeah I was talking while I was saying thank you for the clarification that was really helpful apologies for that yes it's this constraint here sorry this is the constraint that was in our head yeah yeah and it just makes a mysterious reappearance here all right awesome okay so what is the message that I want you to take away from this two things first message is GMM is an em algorithm okay that's really you know one of the the pieces that is there and this is interesting because we're going to see in the next lecture we're going to see a different example which is called factor analysis where Z has a very different form and it's meant to constrain the problem in a different way we went through in this lecture a couple of different steps we started with that convexity piece so we could get an intuition for what these functions look like we didn't want to use convexity we use concavity and then we went through the EM algorithm which we formalized as kind of back and forth with using these curves over time once we had those curves what was happening was we would pick and optimize on those curves and we were getting these qis right the qi's played a starring role those became our W's here and they kind of add nastiness to all of the equations but not a tremendous amount of nastiness right they just add little weights and expectations everywhere and then we ran exactly the kind of like you know standard supervised machine learning if you like or the stuff that you've been doing for mle for the entire quarter on those those properties then we introduced a ton of typos to keep you on your toes now I introduced on typos because I was talking while I was writing and I shouldn't have done that and so then we saw the two things that I cared about you highlight one is you know how to find means and these are just weighted means and you should run through that calculation to make sure you know how to do it because I'm almost positively will ask you to do it at some point in the near future and then the second thing that I would tell you to do is when you have constraints you have to know how to optimize them you don't need to know the general theory of how you optimized against non-linear constraints but you should review how to do this when you have something that some still want it's not more complicated than what I wrote here but make sure independently you go through it and ask questions I'm happy to ask questions I'm happy to point you for to different resources in the next class as I said what we're going to see is this notion of factor analysis and that is going to tell us how to apply em to a to a different kind of setting which at first glance will look like kind of impossible to do without a latent variable model and it's a pretty interesting scenario um and I think that's all I want to say any last questions before we head out I'll stick around for a couple minutes as usual thanks so much for your time and attention"
Stanford CS229 Machine Learning I Factor Analysis/PCA I 2022 I Lecture 14,"Stanford CS229 Machine Learning I Factor Analysis/PCA I 2022 I Lecture 14

thanks yeah I want to go through factor analysis um and continue our tour of em because it puts us in this position where we are going to have uh to make some pretty serious modeling assumptions to make progress and it's going to kind of force us to walk through uh what that looks like in an unsupervised way so that's what the factor analysis piece is we will then cover a little bit of PCA which is an old standby and it's good to contrast these two uh kind of methods together it's kind of the old more uh you know less basian less probabilistic uh standby that people use a lot in unsupervised learning just to give you a sense of where we are in the course um the uh up next for us will be this problem called a which will be great that's going to be in your homework it's always one of people's favorite homework problems this is the cocktail party problem which we'll go through and if we have time I'm not sure we will but if we have time on our current trajectory we'll go through something called weak supervision which is a which is a setup that looks just like em uh this expectation maximization setting except for we can solve the underlying problem exactly so some folks were asking questions last time hey why don't I run method XY or Z on this and there are these Laten problems where actually you can cut to the Chase and exactly solve the to the right answer you don't have to run this kind of back and forth style algorithm and so we'll see that if we have time and and kind of give you a sense of of why that's interesting this is more modern stuff um that we'll we'll talk about all right now uh on to factor analysis this is what we're going to talk about and this is a setting where we have more Dimensions than we have data points now just by way of history when we used to give these lectures about factor analysis it seemed like kind of an odd situation why would you have so many more Dimensions that you that you cared about in your model than you have data points but actually weirdly like if anything in the last couple of years modern machine learning has switched to that being almost the default we tend to train much much larger models as you saw than we have available data and there's a couple of reasons for that they're not the reasons that are in factor analysis just to be clear but this setting is actually a pretty interesting uh one and so we'll see how people addressed it initially right awesome so let's talk about factor analysis factor analysis all right so we have many fewer points data points then dimensions that's the setting that we're in okay and kind of T notation this is n is much less than D okay so it's worth comparing this by the way with GMM and GMM if you remember we had only a couple of parameters right we had the source centers we had the source variances and we had the fre and we had the kind of fraction that were in each Source but we assumed implicitly we had a ton of photons if you remember from all of the different uh things that were going on from all the different sources and so n was much much greater than D okay and that was kind of implicit in what we were doing but it's worth calling out um and the reason is is we would we would average over some sources and we didn't have to worry about a couple of problems which we'll highlight in this lecture okay all right so let me give you an example of how this happens because you know at first you may look at that and say like well is that really kind of realistic and even though I tell you it's kind of become the default setting it's good to have something kind of concretely in mind so what's one way that this happens so one example which is you know actually Stanford based is that we could place sensors all over campus okay all over campus and let's say Those sensors read tons of information temperatures and wind speed all kinds of things and so they record at thousands of locations at thousands of locations locations and values okay so just a huge amount of information but so D is you know in the thousands or tens of thousands or tens of thousands okay so these are like little smart dust sensors everywhere okay but they only record for 30 days but only for 30 days okay so we get like you know n here is we get 30 samples now the sample we get if you like is a measurement across the entirety of Campus so it's like we're getting a matrix that's kind of in the opposite direction from what we're used to it's skinny but it's not tall and skinny it's n rows and then the rows are really really big okay all right so I would just point out maybe it's not obvious why it will be obvious hopefully in a second that we would want to fit a density to this but it kind of seems hopeless and the reason it seems hopeless is we've had every model that we've had had a parameter for every Dimension and so now there's a huge number intuitively a huge number of models uh that are out there that will all kind of satisfy this data if we were to run something like least squares right if you only gave me five examples and you had a thousand uh Dimensions there are tons of models that line up on those five points right that kind of touch that rank five Subspace but all the rest of them are free so how do we pick from it and if we try to do a probabilistic thing it gets even worse okay so let's let's see the key idea and if it's not obvious to you that's a it's a problem we'll show you something very Concrete in a moment that it's a problem so this is mainly intuition right now this is not mathematically grounded we'll show you the math in a second and what will happen is you'll see our equations will just break in some fundamental way and so our key idea to get out of this is that we're going to assume there is some structure because that's the tool we're using right now random variable that is not too complex and not too complex means that I can estimate it that's really what it means operationally and captures most of the behavior captures the interesting Behavior right and this is the tired maximum in this course that you know all models are wrong some models are useful right it's not going to be a perfect model of what's going on but it captures most of the interesting behavior in this relatively compact way and so we'll see at the end of this uh little section we'll see a concrete generative model that that builds with these building blocks um and allows us to recover those parameters that's the key idea okay so let's see the first problem with why GMM would have challenges here um and see the first first example and actually we're going to look at something even simpler than than GMM we're going to look at fitting a single gaussian okay so let's try and fit a single gaussian in this situation so I can make more concrete my assertion before like it's hard to fit models here so let's fit one gaussian okay so the gaussian has two parameters recall there's mu and sigma Square so how would we be tempted to compute them so first we want to compute the mean of our data well that actually seems to kind of make sense we compute the sum here oops sum over all of our data 1 to n right so just make sure let me write the data because I should have written it out we have data points RN element of Rd just to make sure those types are in your head okay this is okay this works fine we compute the mean that seems fine it doesn't matter how high the dimensions are right you can compute the mean and it's a sensible Sensible thing to do right may not be a great estimate of the center right because you may not have all combinations of the directions but it's it's still an okay enough fent the trouble comes when we look at the coari and I'm going to write here the coari in the full general form because we need to talk about the dimension because the dimension is high so I can't just use the one Sigma and if you remember you have some expression which roughly looks like this x i minus mu t Okay so that's a fair enough quantity you can form that quantity but one thing cause it should cause us a little bit of pause what's the rank of well generally we've been assuming when we did linear regression and everything else the rank was at least D the number of Dimensions like it was a full rank object right but the problem here is that D is much bigger than n and I've just shown you that Sigma can be written as a sum of N rank one vectors and if you remember your linear algebra that means that the rank here is less than which is always true the minimum of N and D which in this case is going to be strictly less than D because it's n is smaller than D that's what we're assuming okay so this isn't full rank so where does that cause us a problem well we have to go back and write our favorite function the gaussian likelihood and see where this causes a problem likelihood okay so remember our favorite thing look like this uh Sigma is equal to at 1 over 2 pi Sigma 12 x x i minus mu transpose Sigma inverse x i oh there there should be no x i sorry should just be X so I get through this the bottom of it uh so on okay awesome why is this formula problematic if Sigma doesn't have full rank well there's two parts that kind of don't make sense right one of them is this if it's not full rank the inverse doesn't make sense right it's not even defined and second this is the determinant what's the determinant zero zero now you're toast because that's one over zero this is now infinity or something like it or undefined no matter what it's going to cause you a problem if you start to normalize by something like this okay so how are we going to fix this right so this by the way just to make sure that's clear this determinant is equal to zero okay now we're going to fix these issues just one second we're going to fix these issues by changing the model and that model is going to out and the thing that I want you to think about is we're going to try and make that coari full rank okay and we're going to see various assumptions that we can place on the underlying noise model this will make sense in a minute that allow us to insist that Sigma in fact is full rank you had a question yeah I just Zer oh because the determin is the product of the igen values and at least some number of them are zero awesome okay so the way we're going to fix this is we're going to examine these simple models now the reason you know I'm going to put these models pedagogically and why I'm doing it in this order is the final model is going to take these building blocks and put them together okay so you know if you were like really into the suspense of what's going on with our sigmas here like I've ruined it for you that's what's going to happen in about 20 minutes but what I'm going to show you are different ways that we can get around this challenge so that we can estimate the coari get a gaussian likelihood for that setting and they're all going to boil down to ways that we kind of make sure that our coari is parameterized by something fewer than all of the roughly n squ parameters coari has to be positive 7 a definite so but it still has a lot of parameters okay so it's not all full not every Matrix is a coari matrix but a lot of them are okay all right so let's go through it now let me just recall the mle for G because it will make our uh life a little bit easier here uh call oops the reason I'm recalling this is we're just going to I want to store some facts or remind you of some facts that will be useful now one thing just so you're clear like on on using this um these are just helpful things to know about um like kind of computing these em models again and again so hopefully this is useful practice um even if you're not Computing these so remember we the way we fit these things to data looks like this we sum over all of the data we take the log of 1 over two times the likelihood uh X of oh sorry there's a negative 1/2 here that doesn't change the story 12 x - mu transpose Sigma inverse x - mu and so on okay now the this is equivalent to minimizing the log which we which we kind of do everywhere and expand out which is equivalent to minimizing this and you've seen this before I just want to remind you where it comes from sum I = 1 to n xus mu transpose Sigma inverse this is why I consistently dropped the one half because it doesn't matter but it is distracting okay all right all right now one key fact which we will use again and again and I'm just going to dispense of once is if Sigma is full rank the coari is full rank then Sigma U of this function here which I'll just call uh uh F of mu or fi of mu is equal to the sum I equals 1 let me extend the purple blob pink blob 1 to n Sigma inverse X IUS U implies mu = 1 by n Su I equals 1 n so I apologize if this is too pedantic if not what I'm saying here is this is a plugin if we know no matter what that Sigma is full rank then I can use this estimate for Mu okay so I was saying before that the average was no problem so long as Sigma is of the right form I can always do this and remember why if this is full rank I can pull it out this is the same argument we went through last time so ask me a question if it's not clear because you have seen this before and so then we satisfy from you we can just use that as a plugin for everything we do later sure so if it's not full rank so let's imagine that it had some null space then it could be then the this xus mu could be satisfied by either being identically zero which would be in the null space no matter what or anything that's collinear with the null space that has like a so if the null space is you know a particular direction then it doesn't Define mu it can be anything along that direction um will be set to Zero by this right so it it means the mean wouldn't be the unique minimizer of it right cool' be zero plus that thing go ahead oh like part and the sum that's exactly right and the summation it was just so I didn't have to copy it again yeah this is just exactly this and this is yeah this is the exact calculation we've seen a couple times this this is after heavy computation of the first line that we have done here yeah so this is basically saying these two are equivalent right we we saw this before that if we wanted to maximize likelihood it was equivalent to minimizing this loss functional I want to use that because I want to this this is a lot simpler and it doesn't have like all kinds of like extraneous terms and things that I need to to deal with and so I want to get to the point where I can uh just operate on this so that I can give you some mathematical facts when I draw the pictures that's why that's why I'm doing it yeah you could do everything in terms of the original equation there's no harm or foul from doing that and you know yeah you may prefer to do it that way cool and we're using the fact that like the constants don't matter and all the usual stuff we go to minimizing okay so let's see our first building block there's a picture coming I don't know why I think this will not peque your interest but I'm still going to tell you there's a picture coming that I think is very fun to draw uh that uses all the building blocks together and I'll highlight for you when we get there it's a it's a real privilege to draw this ridiculous picture for you and it's going to be really disappointing after this setup so I'm very excited for that's maximizes my enjoyment okay so here I want to think about the first building block that we're going to draw this is not a fun picture to draw but it's it's okay so suppose that we assume that every direction makes has independent and identical independent and identical correlation or coari okay so if I think about the noise ball what does it look like I'll draw 2D here oops come on Snap two so what this means is that our model of kind of gaussian noise if you think about it so this will be Sigma is equal to Sigma s i okay so that means it's it's I and the only free parameter is this Sigma squar which is a scalar this is a scalar okay so how many free parameters are there precisely one right the structure is the coari is Sigma sare * I so what does that actually look like it looks like I have a point which can be centered anywhere the gaussians can be centered anywhere they can have arbitrary means but their coari structure looks like circles okay one Circle second Circle they should be concentric the limitations of my notability skills okay there could be another one down here okay they all have the same okay so you may look at this and say so this is like the picture of these Co variances are circles but I'm just trying to emphasize what this restriction means and so that circle is just parameterized by two point by two numbers you know vectors the center the MU and then its radius okay and that's what Sigma squ is okay so in this situation what is the mle what is the X well we know mu because this is full full rank why is this full rank because it's scal the identity as long as Sigma squ is bigger than zero this is a uh this is a full rank Matrix right rank is exactly n so what is the mle well we're minimizing now over one free parameter and it looks something like this 1 to n x minus mu except for when we take the transpose here it's all times a scalar so I'll pull the scalar out in front and it will be minus Sigma squ okay plus d log Sigma squ right so the determinant of this thing is the dimension time Sigma squ well it's the determinant of this thing is Sigma to the 2D and the log of that is D * log Sigma s okay determine as a product of the igen values all the igen values are Sigma squ how many igen values are there D so I'm just saying in a very complicated unnecessarily complicated way this is true then I'm just taking the log and writing it in that form okay so just for notational purposes let Z equal Sigma squ what does this actually look like well it's minimizing 1 / Z times this this thing is a constant here this does not depend on Sigma Square in any way so I'm just going to call this c c plus d log Z Min Z so just take the derivative what do I get minus z/ 2 C + D / Z equals z and if I've done my job correctly this is going to be Z is equal to C c n d IE Sigma equal c n d okay just to write it to make sure it's just to unpack it so you understand so you don't get lost in the weird notational pieces this is basically what I'm saying this is the estimate okay IUS okay hopefully this makes sense what is it saying it's saying average over all of the different variances that are there treat them as if they're basically ND of these things that are estimates that you're seeing Sigma ND times average over them and that's what you should get so another way to describe the rule here is subtract the mean and square all the entries substract mean and square roughly okay okay so what do you know at this point oh please question on the second line where we write the Min aren't we missing a sum over the um oh sorry so yes these are part of the two things so when this comes out they should have an ND here sorry yeah the this my simplification on the fly to simplify my notes did not make sense that's a great C catch this thing is going to be C and this sum is inside each of the ends and so this should be ND sorry about that wonderful catch awesome please yeah so the the idea here is we have some direction and so one intuitive way to think about this is there's some radius that makes sense and you know how to fit a radius on any Dimension right so you know exactly how to do this if it were one-dimensional and what this is basically saying is I Collapse all the variants in all directions and it's almost like I just lay them out in a line and average across all of them and what I'm saying is that's not too surprising this math is the correct way to understand what should happen that intuition is not going to help you too much but at least let you type check that like it makes sense and it has the right Dimension and what's great there as was pointed out was this ND is just averaging over all the entries right like and the N has to come from from from averaging over all the entries uh so Sigma squar is a scaler so it's just going to be a single number but then we're going to multiply it times the identity so if you picture it it's just going to be that number number repeated on the diagonal yeah please [Music] hype right so yeah so and any model is always going to be a hypothetical situation we're always talking about what are we modeling here and so what we're modeling here is let's imagine a situation where this would be appropriate it'd clearly be perfect in the case where we had noise that we knew was was actually the same magnitude in absolutely every direction and didn't depend on the underlying data we'll talk about procedures that try and make us close to that in the later part of the lecture but ignore that for one minute that will probably never be true empirically right for a variety of reasons but this is a pretty good approximation if they don't fluctuate too much do you want to pay the expressive power of having to model all the different fluctuations in all the different possible ways all the different ellipses that you could possibly have and this is saying like if no if they're kind of an if they they have the even distribution then this is an OKAY model cool let's see a slightly uh let's see our second building block there are not lots of building blocks there are two okay let's see our second building block all right building block two so Sigma is going to look like this it's going to be diagonal Sigma d square okay okay now what does this look like in my head these are AIS aligned ellipses what do I mean well they may differ like this oops that's pretty good all right so these these this noise parameter is still parameterized by a mean which we know how to set this is clearly full rank why is this clearly full rank well because all the numbers are greater than zero it's igen values are all positive so it's full rank it's clearly it's also by the way clearly a p is clearly positive semide definite or positive definite because all those numbers are positive okay so it is a coari matrix and this is what it looks like it's basically saying that ites the ellipses the errors are not correlated across multiple Dimensions they're basically independent on each of the dimension and so the airor profile is going to look like a flat ellipse all right all right so this is basically saying I have Dimensions that differ wildly but I don't care about how the often they interact right that's roughly the model that you have in your head right like there's a lot more variant in component one than component two that's worth modeling to me but not at all how they interact okay all right so we're going to set Z equal to Sigma I 2 as above same thing we did above we're going to minimize now over Z1 to ZD these are all scalar values all positive to doesn't matter too much sum J D oh J equals 1 erase that one to d z minus 1 J times x IUS mu J 2 this is now a scalar Plus log Z okay and this is inside the parentheses are inside this thing just write it just because it's the way should associate okay oh that should be J see if I made any other bucks probably we'll get there all right so far so good what does this look like well the reason I wrote it out like this so first be clear about what we did here this broke out per dimens so really this is just D independent problems right so what what should we expect we can go ahead and compute this thing but it's going to look like a problem we've solved many times before J sorry this J that's the typo there J square plus log Z J which implies Sigma j s = 1/ n sum J equals 1 to n sorry j is the dimension I = 1 x IUS mu i s let me make sure I have oh mu J mu J all right let me write that again so that it's clear since there were a bunch of typos there sum I goes from 1 to n x I jus mu j s okay so what's going on here um the thing that's going on here is we have D independent problems that means we have D one-dimensional problems and this is exactly the estimate that we had for one-dimensional coari there's really nothing else going on there were a couple of bits here that were using the J mean and the J component it's like every component was handled independently okay all right we are ready to talk about our Factor model okay so what is this what is the purpose of me going through all this one is I want you to get very I want you to be pretty comfortable with kind of going back and forth between hey what is my mle what does the structure I put in and how does that change my estimate okay that actually is pretty interesting you put in the different structure it reduces the number of free parameters and that allows you to estimate things with far fewer amounts of data we're going to combine that now in a pretty interesting way which is what the factor model actually is our Factor model okay and other reason to give this is it's an example of a more sophisticated generative model than we've typically been used to yeah go back to the sure you explain the graphical interpretation of these sure so because so in general aarian Matrix is an ellipse you can picture that ellipse right where the different directions correspond to the igen values right uh and so what's going on here is that instead because we were putting it on uh as a kind of a on the diagonal that means that the stretch is only on the on the individual axis so like you have something that's access aligned like the major axises have to be aligned so when would that be a reasonable model it be a reasonable model if you thought like there are different amounts of spread or different amounts of variance for every Dimension right the previous model said all the variance in any direction was effectively the same there was one parameter so that why it was a circle now it says there's different amounts in different directions but I'm not going to model their interaction in contrast a general coari Matrix can tilt and say like no the principal Direction which we'll come to when we talk about PCA is in One Direction and I'm a little bit setting up for those kind of calculations later is why I draw these right so this this says I don't care about interactions between the features rough very roughly speaking okay basic that spread in each Direction same no no they're different there's a different parameter for every so there's Sigma one and sigma 2 are potentially different numbers right that's why it's that's why it's an ellipse rather than a circle they're all the same it would be the circle cool awesome fantastic so let's look at our parameters there are going to be a lot of them but it gives us a more interesting generative model to look at and if you kind of Wade your way through this this you understand em you understand how these models work and hopefully it gives you confidence that you're not looking at just one setting okay r d byd it's going to be a diagonal matrix okay all right so let's see the model and then the world's I don't know it's not really that remarkable a picture but I enjoy it all right PX of Z so we do the usual thing here that it factors as a lat model this is exactly the same since Z is our is our latent model okay by the way mu is an RD this is a linear transformation that we're going to learn this is a diagonal transformation hearkening back to what we just talked about and here is the way X is distributed X or sorry Z start with z is going to be some 01 so it's just some random noise Vector in RS for Su s smaller than D okay this is this I'm going to call this s because I want you to think about it as the small Dimension so the latent structure is small we want to pick the latent structure to be much smaller D is thousand 10,000 a million a billion something like this we're going to pick s saying there's a small Subspace that characterizes and classifies all of its Behavior that's like the compression that's the bottleneck then we're going to say x and I'll walk through this slowly in a second is this expression Epsilon okay all right so the model that generates X and I'll write this more compactly X is not just centered at the origin it has some mean z is going to be mapped from the small Dimension to the large Dimension by a transformation that we're going to learn so imagine the sampling procedure is Z gets selected if you knew Lambda you would map it up into Rd you'll shift it by Epsilon by mu the the mean that you're going to learn and then you're going to put some Epsilon noise here okay all right now let's get the model for these things so Epsilon oops Epsilon is going to be normal it's noise so it'll have zero mean and it will have f as its parameters this implies by the way now we have all the information we could also write X is distributed as n mu+ Lambda Z Plus 5 please oh what is the meaning all yeah great question so let me just say so this is I just want to annotate this is the mean so I'm going to draw what goes on on this for an example in one second that will answer this a little bit better than I'm able to because once you see what's going on but the intuition is you're going to sample in the small space you don't know where it is that's your latent variable that's your link to clusters that's the thing that you say like I don't know what it is but I know there's a small structure lurking out there then Lambda says from that small space I'm going to go into the big space right the D the D much higher dimensional space and Lambda I'm going to learn that transformation that says how do I go from the small space to the big space and then the big space is going to be the actual dimensions so imagine I was I was actually had these sensors and they were getting temperature readings all over campus and they were getting wind readings all over campus well clearly they're correlated right like if I put a bunch of sensors all in a line they're not independent readings right there's some sensor and then there's some map that tells me from the reading I got what should I expect at all the sensors with like pretty high probability like you could imagine such a map existing and so the Lambda is that map that goes from the kind of low Dimension to the high diens we're just doing it in a very abstract way and we're not specifying what those Dimensions even mean to start with right please um You probably don't want to set it greater than n for a variety reasons but like yeah like you'll see in a second what the conditions are when you go to solve it great great question there's some condition that's lurking there you need to be able to estimate there's no free lunch right but yeah you it's going to be smaller than d right d is a billion and N is 20 there exists some s where we could potentially solve this that's the way to think about it not we can solve it for every awesome right so let's see an example of this whole thing where D is equal to two so not that high a dimension but like good for drawing s is equal to 1 and N is equal to 5 okay and our model is X mu plus Lambda Z Plus Epsilon and let's see how the forward sampling works so how does it work one we generate Z1 to ZN from n01 so what does this look like all right so here's here's zero and we get let me see if I can draw this nicely we get a bunch of samples so maybe here's zero let me mark down zero zero oops that doesn't look right here zero so here's the let me draw the the density actually so the density looks like something like this right this is the gaussian in you know Loosely interpreted artist rendition of gaussians then what I'll do is I'll sample so maybe I'll sample this point first I sample over here sample here sample here sample here those are my Five Points how did I pick them I don't know I sampled so like this is z oops this is z 1 this is Z2 I'm not remembering the order that I did it and it doesn't really matter Z3 Z5 Z4 okay so I just generated those in one dimension all right now two what happens so now so far I've generate I've done this piece right so this character is handling this okay now I map by Lambda so let's suppose we've already learned Lambda so Lambda is equal to one two so what happens next now we're getting fancy take this copy paste so now we need the x-axis back so these things get mapped here's the line one two which goes to the origin okay what happens so that then these are all mapped onto the line oops it's a little small but good enough this one's mapped all the way up here okay down here and down here okay so this point for example is this point is Lambda Z3 this point is Lambda Z2 so far so good so this whole thing now we're in dimension we're in the high dimension so we've gone from our small Dimension up to the whole dimension of all our sensors think in your head Dimension 10,000 not Dimension two okay now three we add mu what does that do I'm put that in green that's this piece copy paste mu let's say is this is this Vector here this is Mu we add me to everything and that translates all our points now by mu and there's one off the screen over here somewhere okay this point is oops mu plus Lambda Z2 right it's this point translated by mu then four we add Epsilon noise and this Epsilon noise is full dimensional so I'm going make this in purple this character here right this is the equation that we're dealing with what happens next we get some purple stuff and it goes like this oops make it a little bit thicker goes like this goes down here goes over there but it's full dimensional noise it lives in a d dimensional space and that is our final expression so for example this character here oops this character here is mu+ Lambda Z 2 plus Epsilon 2 and Epsilon 2 is the noise and no notice the noise is in different direction for each one right so I want this this is in by the way this is in the high dimensional space in in dimension d okay so what's going on here we were talking about that sensor and temperature example so just kind of walk through the Z's here were generated in one dimension let's say that was the underlying true temperature that you were going to see several samples of it maybe at different times of the day right because Z was collected at various different times for different data points right we were collecting it many times in the day then we mapped using Lambda to say like oh if the temperature were kind of the the true temperature or some hidden temperature was you know 50° then all of the other temperatures that are nearby are going to jiggle in a predictable amount predictable amount meaning Lambda Maps them from wherever their value was to the high dimensional space right but the problem is there's still some residual noise that fit may not be perfect right we can't predict things noise means like just stuff we don't want to model and that's where this purple comes in that's the Epsilon Jitter that's in in the high dimension does that make sense and that's that's our underlying model and the data we see are the purple dots so the data are the purple dots that's what we actually see in our the data are the purple dots okay just to make sure it's xal mu + Lambda Z Plus Epsilon all right please oh it said assum that so so even though the sensors give us data Dimension no Dimension D which is really huge but are we assuming that the actual thing that we want to model like say the actual temperature Dimension much smaller than that's exactly right so much smaller than D so what we're assuming is we have again I think the illustrative example which will get you most of the way there is I put a thousand temperature sensors in a row so I get a thousand numbers every time I measure them but if they're all close enough it feels like there should be one temperature and they're all kind of you know there's a mapping from them that mapping we're going to learn that's what Lambda is Lambda takes us from that one true temperature to our guess at all the different temperatures but we're aware that that's imperfect and since we're aware that that's imperfect we also allow ourselves some error Epsilon in each one maybe one temperature sensor is damaged it's dirty someone walked by it someone blew on it who knows that's what Epsilon models does that make sense and so it's the small to big like it's like and that's what allows us to learn this because we're saying although there's this huge amount of free parameters we only have little amount of data we have to make some compression and so that's what the latent structure is doing please um I have two questions so plus is that you essentially assuming the major assumtion of is that this is something close to it's linearly it's a linear form of data just having a bit of noise in it that's yeah so here we're making an assumption that's right we're making an assumption that there is a small distribu there's a there's a distribution uh that's that's low and that there's a linear map up into the larger space you could imagine more sophisticated models that had a learnable nonlinear map into the larger space um and there are various reasons that may be hard in limited data regimes you may not have enough data to learn that piece um the second question pertains to S so s what would happen like and therefore it's easier to imagine what's actually happening but what would be the case when s is notal yeah wonderful question and we'll get into this a little bit in PCA to and sometimes they're interpretable sometimes they're not but let's imagine when we come back to it we'll have this language of principal components in about 15 minutes that will maybe able to give a little bit more precise answer so I may punt there in a second the idea is that s captures kind of the important directions of variation so like if you imagine the temperature sensor so I have temperature and I have electrical current that are there and maybe there's also some you know uh wind thing and if I combined all three of those I would get a really good estimate but I need an estimate of like the wind speed a location and all the rest then s would be some mixture potentially of those kind of you know true directions that are out there now often that story that we tell ourselves is difficult to verify because we don't actually get to see the latent variable and in fact we can get away with making a much weaker assumption which we'll make in PCA which is basically that there exists some low dimensional space that captures our system and that turns out to be a fairly robust kinds of assumptions it's not that we know there are only these three parameters if we knew it was wind and temperature and we were just learning the map we would just feed that in right we would just measure that and feed that in we're hypothesizing that there exists this small bottleneck and that's what's going to allow us to learn in this setting and to me that's the really rich pedagogical reason to teach this is like like it kind of forces you to think about like what can you really recover this is more of a statistics view of this part of machine learning but what can you really recover from data right [Music] so yeah you're just saying this is there exists something small you got um do you do this for every do you create this transformation for every data point wonderful question so here Lambda is across all the different data points this transformation so so if you look at like this notation here like you'll see that Z is generated once uh per data point so there are n of those things n Laten samples there are also n Laten noise samples those are different for every data point that are there but Lambda is shared across all of them as is me in the model now if you knew something like you knew there were K clusters and there were K different uh Maps you could potentially fold that into your model or k different means but this is the model we're looking at now doesn't necessarily need to be that way but for now that's what we're looking at one question do you understanding is we start from Z and then we work our way from we generate Z first then work our way to the data but how do we estimate like s or do we start from oh great question I I get exactly what you're saying so when we went up here remember in this model model so one of the messages is we start with this model and in the generative way of viewing things like the the the real reason I like to keep this in the in the class in in a very fundamental way is because like it forces you to think about what is actually in the model like when you see the EM thing it's kind of in a similar situation where you're like oh there's just this one kind of hidden aspect to it this Z is latent to us so the part of the model is you pick s because it's one of the parameter so you have to tell me ahead of time I want you to model with you know a size s model and then you run this whole thing forward and then our goal is among all those things that could have gone forward what are the most likely settings of the parameters in that model so we're not learning s in some sense we'll talk about in PCA when we're learning a linear Subspace we can effectively look at a particular measure and see if it's a if you know adding one more Dimension would be there and I'll come with some caveat great great questions awesome all right okay so we need one technical tool to make this whole thing go let's do some technical tools I'm not sure that these are super useful to prove here um but I will happily Point them out to you um as we use them all right so here we're going to use this notation you probably have seen this before if not don't worry we'll review it right now X1 is an rd1 X2 is an rd2 and D equal D1 + D2 okay so this is just a nice way of partitioning this is because we're dealing with something that's linear and we can have this kind of block structure okay we can also do this for matrices and that will allow us to State some theorems just a little bit more concisely Sigma 1 one Sigma 21 Sigma oh no so this should be 21 sorry why it's backwards one two Sigma 22 maybe I okay I hope I didn't do something notes all right so this is D1 this is D2 this is uh going to be D1 and this is going to be D2 all right and sigma i j if I've done my job should be oh it's going to be R that's why I did it the backwards high that makes more sense Sig my day is going to be uh d uh J or hold on let me actually change the numbering just to make it consistent with the notes sorry this doesn't really matter conceptually but just to explain it okay one two all right so the point is is I can I can Factor this in some way and it makes sense to multiply a matrix in an obvious way if it's compatible with the blocks so like if I multiply this Matrix here x * Sigma I can write it in terms of these blocks hopefully that's clear right and it would be be Sigma 1 1 * Sigma 21 and that would be what I would want to put there okay all right this is a very widely used notation and it's going to let us state two facts about gaussians that are really helpful if you're not familiar with it just take a look at it um and see so first one is the marginalization identity for gaussians X2 px1 X2 okay for gaussians this has a nice form this is not true in general this is why we love gussian so much really are that it has this really nice form px1 is going to be equal to a normal it's going to be distributed like a normal mu11 sigma1 one okay this is called marginalization it basically means I can grab the mean that I want to grab and the coari I want to grab when I marginalize them out and this is because gaussians have this nice property um about you know being independent in these directions okay this is a much nastier statement Fact Two which we will use px1 conditioned on x2 is also gaussian pretty remarkable if I'm going to be honest like not a lot of distributions are closed in this way what I mean is closed if I take if I do an operation like conditioning do I get back the same distribution that's actually pretty unlikely but gaussians it happens and so again it makes some of our calculations easier this is marginalization this is conditioning okay and so what are these two values this is going to be equal to mu1 plus sigma12 Sigma 22 and I'll put up notes and you can look and see the the notes do this X2 minus mu this looks super mysterious it's actually not but it's not probably worth going into too much detail if you remember your Matrix inversion lemas if you don't again don't worry these are not super conceptual like important details it's just how this works you can go through and I can gladly upload a proof for you if that makes you feel more solidly grounded if not you can just use this okay and these are these formulas when I say it's not conceptually important you may think that's a copout maybe it is but really the reason is it doesn't matter to me because there's only one way these formulas make sense the types all work out you have d1s and d2s and you're multiplying them in the right way otherwise formula is wrong okay okay so this is the Matrix inversion LMA if that helps you great I'm happy to prove it the thing that I care that you take away is that we have these formulas and we can use them later so when we do a conditioning step we can use them okay when we do a marginalization we can use them okay and this is I I realize this is like impossible to appreciate at this moment probably but it's um It's relatively rare that we can go from uh distribution condition on it and get back a distribution right that we have the same what are called sufficient statistics but it happens here just that General please I think both of these we are assuming very deep dependencies between not just within different data like between data samples but also within the features of the data samples right uh so I wouldn't look at it through that lens these are mathematical facts about gaussian distributions these have nothing to do with data per se this is just true about any kind of wild gaussian that you would want meet and so you would use it that way we are going to we are going to show in one second that that crazy model I just showed you can be written in kind of a nice way uh using these things so how do we use it so remember we have two variables n0 Sigma right all right and this is since Epsilon of Z the expected value of Z is equal to zero and the expected value of x equals mu now we don't know Sigma yet we just say there's a coari out there we're going to derive what it is in one second but the point is is that whole Factor model basically boils down to some gigantic gaussian and that's going to be really helpful to us the problem is like this form like this is going to be data this is going to be hidden and so we're going to have to deal with marginalization and conditioning and all the rest of it to be able to recover it but the point is we can write that model super compactly now I think if I wrote that first probably wouldn't be super happy maybe you'd be really happy I don't know maybe you're happy folks which line so and again you can type check these things just make sure it makes sense it's got to be a d byd map or an N byd map for everything to make sense and the types checked out if there are typos which I'm sure there are okay so let's try and now our job is to try and figure out what is Sigma and by the way this is kind of remarkable like we went through this pretty elaborate discussion about um various different models you know how the model worked and lo and behold it's just some nice gaussian what is the so this is the so it's because it's block one one is the outer product of Z with itself well what's the outer product of Z with itself it's just the identity why is it the identity because that's the way we sampled remember we sampled in the low space this is using this fact oops is using this fact right so e expected value of ZZ transpose that is exactly I okay what is sigma2 well it's going to be the expected value of Z * x minus mu transpose what is that actually equal well that's going to be the expected value of z z transpose right so x minus mu equals what it equals Lambda Z Plus Epsilon right that's just the model I just subtracted off the MU on both sides so it's going to be z z transpose um Lambda transpose Plus E Z Epsilon transpose now these two things are independent oops Z and Epsilon are independent so boom this goes to zero this we just concluded oh sorry this we just concluded was the identity so this equals Lambda transpose and sigma 21 oops I write in proper in the other color Sigma 21 equals sigma12 transpose because that's it's a coari matrix positive it's symmetric okay the last one or you can just compute it take my word for it but you don't have to take my word for it is this one x minus mu xus mu transpose we just use this formula again that's going to be the expected value of Lambda Z Plus Epsilon Lambda Z Plus Epsilon transpose we have some multiplications to do we know that all the cross terms with Epsilon are going to cancel out so this is going to look like and I this will maybe look mysterious so I'm GNA maybe I'll go a little bit slower here um then I wrote my notes Epsilon Epsilon transpose okay so why notice you're going to have an Epsilon times this term in the transpose but the expected value of Epsilon is zero so those ter cross term is going to cancel the only one that's going to remain are this one and this one where the epsilons are multiplied by itself so both cross terms Fall Away what does this equal well we just saw this is a random variable this is a deterministic parameter of the problem this is going to be Lambda Lambda transpose and this is going to be plus the FI why did that happen this fi is the model sorry for the scrolling soe avert your eyes if it makes you nauseous is this character sound good so let's write the whole model in summary and again you can check the types on this thing make sure I didn't make some silly mistake okay and we're good so now it's in this nice gaussian form and oh my gosh I claim you already know how to solve this Eep what is Qi of Z I won't go through the whole M algorithm remember is p z of I given x i and Theta what do we use here we only have two rules yeah use the conditional that's all we got use a conditional mstep well it's just a it's a normal distribution where we filled in the x's and we're marginalizing out after seeing the X's that are in there or conditionally on the Z's we have closed forms you already know how to do this okay so what is the summary here we saw here this factor analysis structure that allowed us to have a lower dimensional space had this elaborate kind of sampling and moving around pretty wild uh set of generative models and it basically boiled down to a oneliner to Sho and Horn into the EM algorithm we had to use some fancy you know tricks with how you deal with normal distributions if you didn't use those fancy tricks for normal distrib you would have to do some empirical extra work right you'd have to understand if I had a distribution that looked like blah and I wanted to marginalize or condition it what would I do on top okay yeah please oh because once we have this these are just like so think about what happens once we have the estimates for the Z then it's just conditioning on the Z's and removing them and we know how to estimate the parameters of a like this becomes just a stand oop where's the formula sorry for the Mad scrolling crazy scrolling is here once you know Z you have a guess of Z then this is just fitting a gaussian with a unknown mu and sigma and you need to figure out what the sigma is that you're fitting but you know how to do that you so I guess this is something we've done you know maybe K lectures in a row and it is I agree with you like it is abstract and weird that that falls out and I want to say it in that abstract and weird way so that you're not afraid of it because you're going to you know if you want to go into these things you want to use these things like it'll look very strange but once you get it in that form it's just kind of a plug and chug kind of mentality to to get all the results from things you already know derivatives you already know how to compute cool oh go ahead expectations so that's just the definition of coari so because I know the the distributions I want to compute what Sigma must be and I'm just trying to tell you the reason I wrote it that way is to tell you there's nothing else going on in the model I'm just Computing the expectations I know the coari of the individual objects and that is telling me like that I can write it in this form cool all right let's use 20 minutes to do PCA and we should be in good shape all right PCA all right so PCA I'm gonna draw this little diagram to explain where we are principal component analysis so we looked I want to fill out this table you say why do you want to fill out the table I don't know just how I do it I like that it fits also you should know it it's a important algorithm' be weird if we didn't teach it although probably a bu of you have seen it before if not don't worry about it so one of the things that's inside machine learning which is really kind of interesting historically I won't bore you too much with like you know the various tribes of like machine learning Theory and how how we got where we got and who fought with whom at which conference although it's kind of funny because it like so bizarre but there are kind of two schools of thought of doing machine learning and uh you know at a at a kind of a broad level there kind of the basian school of thought which is everything should be written down as a probability distribution maximum likelihood we write these priors we use conjugate priors we kind of crank through the model and we have this forward story and then there's kind of another Camp that's like kind of more the frequentist style world just from the stat side that was like we don't want to use probabilities we just want to have these kind of nice estimates they also like maximum likelihood they also like B rule but they differ in some ways of like how you approach modeling and what a valid model is like how you know to trust a model okay doesn't matter at all the point is almost everything in machine learning has kind of these two different approaches and I'll say at one point in my career early in my career I built a system which was like very much in the probabilistic camp and then became kind of disillusioned with it and moved to doing something else um but there are merits to both approaches PCA is what we're going to talk about so that's just historical context I just want you to know there are many ways to Model A lot of these different problems and we're going to talk about this one here and I want you to explicitly contrast it with factor analysis which is the probabilistic version kind of a PCA in the same way K means and GMM have a parallel structure okay you saw K means we did GMM they had a nice kind of rhythm of how they were solving the underlying problem so it was nice to put them together um these don't have that nice Rhythm but they they have something else that is nice to see all right so let's see PCA please oh okay so we're gonna be given pairs here and this is kind of a weird example but it's something that makes uh hopefully the main point clear okay so imagine someone gave us a data set and this data set had a bunch of cars on it and the cars are going to be rated on like kind of highway miles per gallon and City okay and these are gas cars these aren't uh electric cars okay so they're kind of scattered all over this okay and I actually went and got the real data it's not that interesting but you know there's a couple that are substantially better substantially worse than the line okay now imagine these cluster up here just to give you kind of complete the story these are maybe hybrids oops these are hybrids maybe these are SUVs they have bad gas mileage or trucks or something and these are economy cars I don't know okay clear what the visualization is is saying okay so we ask a question which at first seems kind of strange but you know it's it's a question we could ask nonetheless what does it mean for a car to have a good miles per gallon now some cars are going to be better on the city some cars are going to be better on the highway we can kind of like a single direction or a single kind of way that says what is the component of principal variation right for good miles per gallon and that's kind of what we're trying to ask and by the way PCA is typically employed in these settings when we don't know too much about our data we want to visualize it in some way or get some understanding right modern methods kind of incorporate a lot of its ideas so you don't need to run it usually as a standalone ahead of time anymore like they worry about these features and things but it it focuses on one thing I really think is great so how do we deal with this all right so the first thing we do is we take our data in the PCA worldview and we Center it okay okay now what does it mean Center we compute mu which is equal to 1 by n Su IAL 1 to n x i and this is and then we subtract mu from every Point here to get it to get the center of the data set at zero okay should be balanced in all ways this is just to make sure that the scale the raw numbers don't matter because we're going to talk about geometry and also we're going to use some ideas from linear algebra and linear algebra likes things that go through the origin right we like linear Maps linear maps go through the origin so we want to like the origin has a special role linear algebra this is still Highway this is City okay now if I look and I just eyeball it roughly speaking it feels like the component of of variation is something like this right and what I mean is if I look along that direction that tells me like the more I go along that the better the highway model is there's some variation that's that's around that but that's like good miles per gallon and and Below as you go those are like slightly worse so along this this line is kind of the principal direction of variation it doesn't capture everything right like there's this character way over here there's some character way over here there's an automated way of kind of den noising that his data sets and other things that I'm looking at and so what what we'll talk about is this direction mu1 which will typically be a Vector is a component of principal variation okay this is very intuitive I'll Define it formula in a second you1 you1 okay and if I look I can still describe all my points this is just Linear alra by something that's orthogonal and I'll call it U2 okay now I'll construct U1 and U2 because I only care about their directions to be unit vectors okay and so you can kind of think about U1 as how good is the mile per gallon and U2 is kind of explaining its first variance right like it's the first if you thought about it as probabilistic it would be like the first direction that they vary a COR varies and then like what's that second order effect you can imagine doing this in higher Dimensions all right formally one second we can write x i equals Alpha i1 * mu1 plus Alpha I2 mu2 and these are scalers okay this is just linear algebra please go ahead you had a question yeah we're just eyeballing it for now I'm saying that like looks roughly correct we're going to find how do we find that line in just two minutes five minutes so let me once we write the definition let's come back and you and then hopefully your intuition and my intuition will be a little bit closer right but yeah that's a great question you're exact you're thinking exactly the right way why is that the right line why is this Maniac drawing that line right now the thing is is we may only decide by the way to use this as dimensionality we may only keep this component okay because it explains more variation and that's going to be how we Define it variation okay so what we want to do here just so you're clear so here I drew it from two to one just as I was drawing before but think about if I had thousands of Dimensions you can't possibly visualize thousands of Dimensions it's really tough and I want to get it down to say you know two or three or five or 10 right I want to find those principal variance okay and so PCA also functions as a dimensionality reduction method okay so here we showed just a two dimensional plot but imagine I gave you the cars have thousands of numbers and you want to get a very succinct description of them as just five numbers or I gave you 10,000 sensors as we were talking about before and you wanted to figure out give me three numbers which really describe every different individual sensor and its variation from that number it's very similar to factor analysis right like it's looking for that low dimensional Subspace that you want in fact it is formally a low dimensional Subspace it is a dimension of Dimension two or three or k underneath the covers and we'll come to that in one second okay so at this point this is intuition you should have more questions and answers let me State the algorithm that we're going to do the pre-processing that we're going to do and then you'll see why some of these variation things are please go ahead I think what you're proposing is that yes X has number of principle but like most of them are noise except for the initial that's what are you saying I'm saying great question so imagine I gave you this X1 to xn that live in some high dimensional space d then what I'm going to try and do is find a smaller number of directions but those directions are not necessarily aligned with the axis which means they're not like component 5 seven or 9 there's some mixture of all of them and so U is not aligned with an axis right it's some mixture of of the of the highway Direction and the city Direction and I'm going to find that direction and then I'm going to say that's the direction that the data set varies most in there's the most spread in so I want to keep where you are on that line I'll project you on that line then maybe the second the third and that's going to be like a third kind of you know a three numbers that best capture you in the data said if you're an individual data point uh not noise but I'm just not modeling them yeah please I guess what happens the relationship relationship yeah how does It capture the direction oh wonderful question so we'll come back to that in a little bit effectively what we're going to do is we're going to capture some variation that's there so there may actually be some of these hidden ation that are underneath the covers and we we won't model them well or we may only model them well in different pieces of data there are methods if I know that there's a nonlinear relationship between a lot of the different elements of data that like I can I can capture here we're assuming there exists a linear Subspace and that's a fairly robust assumption so people do use more Advanced Techniques like tne and umap which do have actually the assumption that there's some very compressible nonlinear map that explains what's going on but PCA is kind of the the the first and the the cleanest to describe yeah awesome question okay so we Center the data by the way just I'm going to review this in one second I'm just trying to write so we can get through this uh mu we know what it is 1 by n some I 1 to n x i okay we may need to rescale the components why is this well what if one component was miles per gallon and another was feet per gallon they're just just different by a factor of 5,000 there'd be a lot of variation in the feet per gallon right not the miles per gallon the numbers would be different so not only do we want to like Center them and get them both centered around zero we want to rescale them so that they have exactly uh kind of the same width and what we'll do is we'll just divide by the variance by the sample variance right in that direction so that all the numbers are kind of like roughly normally distributed you should you should think okay so we're going to assume that the data are pre-processed if you actually do this method you should do the pre-processing so let's see PCA as an optimization problem all right oops Monkey Business I'll live with it okay all right so here's what we're trying to solve and this will get back to some of the questions that were asked about hey what do we what do we uh why do we think this is a dire principal Direction okay we need we need a little bit of of uh of math before we can make that precise X okay so what we want to solve right what we have is we have some direction mu1 we have some other direction mu2 which is orthogonal this is mu1 we have some direction mu2 which is orthogonal we have these are unit vectors mui equal 1 are unit vectors and they're orthogonal mui muj equals zero or equal sorry equals Delta i j sorry orthal okay so how do you find the closest point on this line to X well this line just to be clear can be parameterized as T * U1 for T element of R right it's just a line one-dimensional line and so to find Alpha how do we find Alpha above closest point that's the following expression alpha 1 equals the argman over Alpha of x - Alpha mu1 Norm Square okay right so hopefully this makes sense if I want to find the closest point intuitively it should be on the line that's orthogonal here it's not a very great drawing but that's where it should be it should be projected onto this line and we can minimize over the whole line by minimizing over Alpha we do the standard thing here we compute the derivatives we multiply it out right so this is equivalent to when we multiply it out ARG men x² + Alpha Square * mu1 s minus 2 Alpha mu1 dox this is Argent over Alpha this doesn't change this is a function of the input this is one so when we go to compute the derivative what do we get we get 2 Alpha minus to um UI 1.x okay or said more simply Alpha equals zero right this is the gradient with respect to Alpha so Alpha equals mu1 dox okay this is saying the dot product in this direction tells us exactly the closest point this is just differentiating this expression with respect to Alpha okay all right now you can generalize this to any set of directions orthogonal directions so if I have mu1 to Mu K element of Rd and X element of Rd how do I find the closest point to the Subspace span by mui here we're going to use the fact that mui J Delta i j well it's going to be the following argman alpha 1 Alpha k x minus sum K = 1 to n or sorry Jal 1 to K Alpha J mu K now expand that out again exactly as we did before and you'll see that Alpha I is equal to mui or UI dotted into X this is the inner product if you like okay we call this quantity x minus some Alpha J UJ sorry this is UJ UJ s to K okay the residual okay so we have everything we need rushing a little bit here but so the point I want to make is if you have a Subspace and you want to find the closest point in that Subspace to X that's the same as minimizing over all the possible directions in the Subspace this distance squared distance squared makes the computation easier then Al I if you solve it is going to break apart and the reason it's going to break apart is when you multiply it apart like this you're going to get products of mui and muj they will all go away so all you'll be left with is a bunch of the alpha I and Alpha JS that are multiplied in exactly the same way as the 1D case and you should check that and this is you know if if you're Rusty on this this is from your linear algebra class and I'm super happy to write it out in more detail if that confuses you but for now hopefully we can we can just move through this okay this thing here is the residual that says how close am I what's my distance from X to the Subspace and that's going to feature prominently in what we do next please so so what we're going to try and do is we're going to try and find a low dimensional Subspace and I'll describe how we're going to do it next and PCA when we're talking about what's the principal component of variation we're going to search across all the dire when we're looking for the principal component we're going to say which one of them has this for example there are two ways we can find PCA but has the smallest residual or what will be equivalent maximizes the amount of alpha the amount we projected onto the subset okay so there are two ways you can do this so we can find PCA by one maximizing the projected amount that means trying to make alpha as large as possible right so if you give me a Subspace which is a section of of vectors I can compute the alphas by solving this problem I want to compute something so that a Direction so that for almost all the data points my Alphas are as large as possible so getting back to the earlier question about why did I say that direction was the principal component of variation is because if I put it in that direction I was kind of explaining the most and the residuals the errors were small dually these residuals for every Point are as small as possible and we're going to minimize those in one second yeah go ahead we just we did so awesome question no not necessarily right because for example think about even if we we we normalized it there may be One Direction that explains for example the temperature how far it is away from the heat source to my thousand temperature sensor example and there's one direction that explains that but there still could be variations in other directions that are not captured by any of the process and so they would be orthogonal to that so it's not necessary that the um that the all even though all the directions on average are normalized that for any particular Point like if there's a correlation would would would be the same okay so we actually normalize all the axis with the same scale so all the axis are are are put into the same scale by taking individual component and Computing sample variance yeah multiply by a diagonal matrix please when we taking Al how do we take the AL is it the distance between the line and orally or great question so here alpha alpha is is basically T so it tells you how far along the line you're going to go so Alpha is constrained to live on the line so when we minimize over alha we're minimizing its coordinate along the line and that's well defined right there's only there's a closest point here right we know that kind of intuitively from you know undergrad whatever whatever geometry we took right like that there's a closest point but this proves it right awesome cool please yeah so it's either you want to you want to capture as much of your data as possible and in the linear case it's either you want to you want to capture as much as possible or minimize the residual would turn out to be equivalent because here if you look at it because these are unit vectors making Alpha JS bigger like if you if I had two sets of U's you know you and I'm comparing them if the alphas are bigger for this one on average than for this one then that says that they capture more of my data right because they're a projection the alphas are defined by the use yeah all right so let's find the component principal component of direction right so what this boils down to I'll just do one of the methods the maximization one you have to do the other one in your homework by the way so I'm one reason to pay attention is this says look over all the directions sorry you okay so it says look over all the directions that are out there dot them into the data set this is Alpha right this is the alpha we were just talking about and try and maximize the one that captures most of it so if you imagine me sweeping that line across the earlier data set where we were looking here right here and I'm sweeping that line here here here my are all from the origin sorry go away here here clearly this captures much more the alphas are much larger than they are here right just take the dot products right because it's just projecting onto a relatively narrow band doesn't capture as much spread I want more spread right it's quadratically more spread okay so how do we solve this question here well we need a couple of facts okay maybe we will pick this up in a little bit Yeah I that's probably best so let me ask me any questions about particular spot we'll talk come back about how to solve uh this kind of equation in a second any other questions about this piece yeah you go back to essentially like this point only what we trying to so what you by maximizing projective softes you're just trying to ensure that whatever U we choose all initial on they Maxim the data point exactly right they they capture the most of the data so think about when we were going back to our earlier example this is this is important we were going to have U1 and U2 that was like our basis right that's formly what it's called we want to capture so it turns out that the the squares of alpha 1 and Alpha 2 sum to the square of xi's components like their Norms are the same okay that's just a mathematical fact because these are these are this is what a basis is so so the the point is is we want to capture of these two we're going to throw away Alpha 2 if we only are allowed to compute one component so we want the the U that we select to be the one that for most of the data captures some relevant information right like if we found something like this direction is mostly orthogonal to where the line that the data lives on so almost everybody's going to have like a projection that's really small like the data when I project onto the line is just going to be clustered really really tightly together whereas if I cluster if I project onto this line it's still going to be nice and spread out and this mathematically just says that it corresponds to either making sure that the alphas overall are very close to the X's so they're larger on average or that the amount that I lose this is the residual is the amount that I lose from the other directions is equivalently small and and for for PCA and ukian space this is the the case not relevant for this class but there are other di there are other kinds of geometries where that's not true this happens to be true for ukan geometry if you decide to take nonukan geometry it's a great course what is the oh X I let me just write it again nice catch yeah eyes don't show up very well with this pen in my handwriting which is Criminal since we use them so frequently just assume it's an I but that's all it goes on right so we're think about what this equation is saying what this optimization is saying it says pick over all the U's that are there we're normalizing we come back to why we normalize it doesn't really matter too much but what we want to do is explain as much of the data as we can okay awesome so uh next time we will cover a little bit of igen values and how we solve this this is a this is an igen value problem and we'll cover the cocktail problem uh as well and you saw a little bit of of PCA thanks so much for your time and attention talk soon"
Stanford CS229 Machine Learning I PCA/ICA I 2022 I Lecture 15,"Stanford CS229 Machine Learning I PCA/ICA I 2022 I Lecture 15

so welcome to our continuing work on understanding unsupervised learning uh we're going through two kind of stalwart algorithms which are pretty fun today we're going to finish out PCA which is this old standby we use it for dimensionality reduction sometimes visualization it's kind of a core canonical unsupervised algorithm and we'll talk about that today and I'll try and refresh and be a little bit complete there because I know we got cut off kind of in the middle if we have time uh well then we're going to go through ICA we will have time for ICA this is a fun problem this is this cocktail problem you'll implement it actually and it's a one of the ones that's kind of a highlight every year people talk about what are the favorite homework problems they did and the reason we're going to go through it is it's a place where you make a kind of a different assumption about how the noise works it turns out if you make the kind of traditional gaussian assumption here which we've been making kind of through class it breaks and so it's just nice to to have an exposure like there's more interesting things to do and it's also kind of a fun problem if we finish those two and there's some chance we do but if we ask lots of questions that's actually even better I'll show you some stuff about weak supervision and I have some slides prepared for that which are not in the main deck but if we get to that I'll I'll post the slides week supervision the reason I would want to talk to you about it is it's a more modern technique it's behind some products that you've actually used in the last while and it's kind of one of these things that looks like an em style model but you can solve it in a very direct way and since we're not having another exam in the course you won't be responsible for it it's a little bit more advanced and so I can talk to you about kind of how that works without hopefully freaking you out Okay cool so that's what I want to do today so let's talk about PCA all right so PCA if you recall we were looking at these unstructured things I'm just going to run through some of where we were last time we're looking at the structure and our structure is a Subspace and it's a non-probabalistic method so the two things are we wanted some structure that we were looking for underneath the covers remember GMM and k-means were there's a clustering kind of structure here we want to look for a linear Subspace and we wanted the non-probialistic version of that we looked at this kind of contrived example where we had pairs of Highway and City miles per gallon and we were plotting cars and we kind of intuitively would guess like you know their hybrids up here their SUVs down here and trucks maybe and economy cars in the middle and we wanted to understand what is a notion of good miles per gallon that took into account both Highway and City and this was a way of motivating effectively that what we wanted to do was to kind of draw a line along the direction that explained most of the variance that was our intuition most of the ways that they varied inside the data set that was kind of that was descriptive so the way we did that is our first mathematical thing is that we centered the data recall we took the data we subtracted off the mean and literally last time I just copy pasted this data and centered it at approximately the center of of the data which is Mu and we transformed it and the reason we transformed it is we're using a linear mapping that's what we want to do and linear spaces go through the origin so this just lets us use linear spaces and have one fewer degree of Freedom when we map them special understanding although you should think about it but it's really critical if you run these methods if they're Askew there will be no line through the organ origin that potentially goes through your data once we did this I want to remind you of some things that we're going to use from linear algebra we have this idea around we're going to prove formally what we mean by the direction of maximal variation and we kind of thought that this line intuitively would be that variation that is when we projected things on the line either we had them maximally spread out that means we captured most of the variation that's that's actually what that means or we saw this dual thing in a second that the residuals the amount of error in our prediction on that line was small okay and we'll talk about that again in a second if you don't remember your linear algebra remember that you can write once you have an orthogonal basis you can write any point in that space as a um in that orthogonal basis namely every one of the vectors that are orthogonal U1 U2 in this example and their distances along those lines we're going to recall the math that does that but that kind of gives you a new set of coordinates for your data if your data were a thousand dimensional it was given to you in a thousand Dimensions you could run PCA underneath it and find the direction of maximal variation and maybe it's second and third components and then you could take that thousand dimensional coordinates which are a thousand numbers that you were given and compress it down to three and that's what we mean by dimensionality reduction what are the three best numbers if you like in some sense that represent your data set okay now just remember that fact that every Point can be written in the basis and really it's like the coordinates of like how far do I go along U1 then I'm going to go along the U2 Direction and get to X1 this is Alpha One and this is Alpha two okay as we'll see below from last time now the convention is that U1 and U2 are unit vectors because we only care about their direction we don't care about their length or their magnitude and this is just what you do there and so here U1 was kind of how good is the miles per gallon and mu2 was the difference between their Highway and and City miles per gallon roughly those aren't formal statements that's just to give you an intuition of what this basis looks like so you can kind of have an intuitive feel of what you're doing you want One Direction which is the principal component of variation and if you had a thousand different uh uh points you would then look at the other data sets and say what other direction is the second principle component third principle component and if you remember your linear algebra that is just how you write down a basis okay that's all I'm describing but we have to order those bases in some way we have to figure out which components do we pick first and among all of them and that's what we're going to solve in PCA all right okay so this is all just saying X can be written in this form and we may hear compressed from Dimension two to Dimension One what that would mean was we would probably just keep the number alpha one that would just be we would treat X as its projection onto this line and just treat it as Alpha U1 okay and that's what we mean by explains more variation okay so when we we're going to find these directions today with some caveats and we're going to think about thousands of Dimensions to tens of Dimension and this will be a dimensionality reduction because we're going to only keep those components so before I move on there's a lot of linear algebra in there ask me questions and I'm super happy to unpack this because we're going to assume this information so we try to that last time going forward but I'm super happy to answer questions and and derive whatever's here that's unclear and if you have the question almost certainly somebody else does so please go ahead and ask please one of these Alpha which is basically the prohibitions are negative ah so they can be negative right so for example in this direction U1 is going this way if you were to have a negative it would mean that it was in the negative direction of that these are signed so they they can certainly be negative they're not positive scalars here that's why we worry about their squares for how much of the residual is because they can be positive or negative coefficients just like you can have something that first component is positive or negative and the positive tells you to go this way and then negative tells you to go that way [Music] are based on we don't know yet right so what we're going to do is we're going to try and find so all we know at this point that we're trying to make sure that we get there is that we can take an X and we can write it down in a basis so X has its original thing that it's given to us and RN some large space we can write it in new numbers alpha eyes for these uis and this is a different basis for the same underlying space and then what we're going to try and do is say how do we pick a good set of uis to represent this and what I'm trying to hint at is that we don't have to pick a complete space we don't have to pick n orthogonal basis vectors we're going to pick the top K in some sense and that top K is we hope captures the most variation and we're going to make that precise in the next couple of uh you know write-ups awesome questions please any others wonderful point okay so that's exactly what I mean we're going to say how do we find those directions okay and last time we talked about this pre-processing we're going to assume this for the rest of time I would just say at some point reflect on this we have to Center the data that's because we're going to look through lines that go through the origin we want linear subspaces so that makes sense that we would want our data to kind of have a chance for these lines to explain the maximal variation that's why we Center that's important we will also rescale the components so that they have about the same amount of space so imagine if one of your components was miles per gallon and another one was feet per gallon the feet per gallon because it was you know 5 000 times the numbers were five thousand times larger they would be at vastly different scales right and if you go through the calculations below because you're taking squares and doing things because they're at different scales that kind of makes them you know unreasonably important right so one of the things that you'll typically do is then scale by the variance in each Direction and then allow them to kind of all be spread kind of about a gaussian because they're centered component wise and then divided by that variance that's just the other piece of pre-processing that that goes on okay all right so now we've done that we've we've done those pieces we need a couple of bits of mathematics just you know to remind yourself of what it means to find these kind of components so here we have two components U1 and U2 and their unit vectors as we talked about before and they're orthogonal that means their dot product is equal to zero they're perpendicular to one another okay now what we want to find one of the things that we have to find are kind of a subroutine intellectually of what we have to do is to find the coordinate Alpha One on this line it makes sense that it's the closest point actually on this line to X right that would be the point that we would project it on that's what projection means actually it's the closest point on the line in this sense okay and euclidean geometry so what does this line This is the set of all T times U1 so that's any scalar positive or negative as pointed out that scales this entire line and so basically our optimization problem to find this component Alpha One Is to find the T that gets me to this closest point now geometrically it will hopefully be intuitive that that should be perpendicular right to this line this line here will be perpendicular and the reason is is the rest will be explained by U2 okay all right so let's find out how we find the closest point to the plane and I'm going to write it out now so how do we do that just remind ourselves so how do we do that so alpha 1 is going to be equal to the ardman this is just rewriting what I said overall Alpha such that we have here x minus Alpha U1 square the square just makes our life a little bit easier kind of mathematically squares Norms of squares of norms are just easier to work with doesn't really make any difference in the underlying piece but this is just saying among all the oops among all the alphas let me get that among all the alphas which are running up and down this line I want the one that has the closest distance to X that's what I'm calling the projection okay so what does that equal well this is the same as doing the ARG man let me write it on the next line actually the ardman and I'm just going to expand this Norm out so it's x squared plus Alpha Square U1 Square minus 2 Alpha x dot U1 okay and this is I'm just writing the dot product in a notation hopefully not too confusing this is just a DOT product okay just to make it clear without having little tiny dots to look at all right so a couple things right away this term is one because it's a unit Vector it's a unit vector and this term is irrelevant for Alpha Alpha's value here doesn't change the value of x x is given so this is equivalent for us when we take the derivative oops we take the derivative with respect to Alpha of this expression this expression looks like to us Alpha Square minus 2 Alpha X U1 okay and when we take the derivative with respect to Alpha of this thing it's the derivative with respect to Alpha of this expression that equals 2 times Alpha which is from the first one minus X U1 okay now to set this equal to zero this is equal to zero this implies Alpha equals x dotted and E1 comma that's all that's saying all right so this is just saying something you may have forgotten from linear algebra or you're now remembering which is that the dot product of a unit Vector is actually a projection all right so far so good all right okay now one piece here is that we can generalize to higher dimensions to more components and it's worth actually thinking about what this looks like right so the point is when we when we write this down we're going to have here U1 to UK for some value of K this is just for an exercise for arbitrary value of K element of Rd some X that's also living in Rd and then here we're going to calculate coordinates alpha 1 to Alpha D such that x minus sum k equals 1 to D uh Alpha k u k is smallest clear enough this is finding the closest point in the Subspace instead of a line we're finding the closest point in the Subspace hopefully clear and you remember this if not please ask a question super easy to explain all right so by the same basic reasoning you compute the derivative you unpack it you have to use the fact that the uiks are orthogonal why when you expand the squares right you're going to get products of UK dot into UJ and those will cancel out and so you'll basically have just a bunch of Expressions that are that are present in which Alpha K is going to be x minus UK and this is only because they're orthogonal only because orthogonal UK or orthogonal okay and you can do that derivative very very quickly okay now this quantity here is important that's a terrible highlight let's use this one this thing here is called the residual right all right this is the residual and what we care about when we're going to do PCA is we either want to we either want to find that we want to find a set of points a set of directions such that when we do this projection onto them the sum of all the residuals is minimized okay so this this tells us the residual for a given point and a given basis and now I have an optimization problem right and I'll write it out formally in a second but intuitively you can think about what's going to happen is I'm going to pick you know K of these U's that are going to be orthogonal there are many of them that I could pick many orthogonal uh bases that I could pick I pick one of them then I project all of my data onto that set I measure how well I did by either how much I captured in the data set or by how much was missing which is the residual and this is the residual I then have that per point so I sum up over all of those and this gives me a score of how good that basis was and among all the bases I want to pick the one that minimizes the residual or maximizes the projected Subspace so let me write some of that down and then you ask questions if you like so we can find PCA by two things by the way this is not this is a this seems like trivial uh potentially um that the maximizing the projected space or variance and the minimizing distance are the same it's actually not true in general for for other geometry so it's actually quite a nice thing about euclidean space that doesn't matter to you but just kind of a comment it's residual all right so let's do this one this is the one we're going to do in class maximize the projected space so let's do it for one vector so now we want to pick among all the possible U's what's the debt what's the version uh what's the particular setting of you that explains the most about our data and so what that's going to be from our our previous discussion is this Max U over Rd subject to U equals one this two Norm is equal to one over an average over all the points although of course such a constant doesn't really matter because we're maximizing but it's nice to have the right scale U dot x i Square okay so what we're saying here we pick a direction and this direction we want to get the largest dot product right how much this is the project this is X1 projected into this this is the alphas the sum of those Alpha squared I's we want to be as big as possible so we want to pick the direction among all the directions so imagine in two direct and 2D you're just kind of spinning around and you're you're judging how great the Subspace is by maximizing how much is present okay we need a couple facts to solve this hopefully the the point is clear we need some facts to solve this okay so first fact we need we need to recall is let a be a symmetric and square Matrix kind of makes sense okay then it's a normal Matrix and in particular it can be written like this you Lambda U transpose where u t equals I the basis is orthogonal and Lambda is diagonal okay not all matrices are diagonal not all matrices are orthogonally diagonal but if it's symmetric and square it's called normal then it has this and Lambda has has a nice interpretation Lambda I I the since it's a diagonal matrix equals Lambda I and we call these the eigenvalues Lambda 1 and they're real okay by convention so we order them this way just because it's nice to talk about them is Lambda 1 being the big one Lambda n being the small one okay now if you don't remember your linear algebra maybe this doesn't seem mysterious to you but if you think about the like underlying model like there's no reason in general these things should even be real value they could be complex valued in general but if they're symmetric and it's nice then this happens they're real and they can order them which is really nice okay so we're going to use that fact and if that's confusing to you to remember what happens when you diagnose over the complex plane don't worry about it at all just take this as a fact okay all right so these characters here as I mentioned these are the eigenvalues all right so recall if x equals sum k equals 1 to n Alpha k u k where U1 u n equals U we can write the following thing we can write alpha x ax sorry is equal to U Lambda U transpose X this is equal to U Lambda sum k equals 1 to n Alpha k e k oh sorry e k sorry let me write it like this uh I don't want to write it that way yeah okay perfect yeah Alpha k e k what's going on here U transpose U is exactly the identity so all that's going on here is since it's what when I dot product UK into one of these which one survives exactly UK right if it's different if it's UJ that's different then they're going to contribute zero so this becomes Alpha K in the standard basis this is a standard basis that's confusing ask a question what's going on here again X is written in this form what happens when I multiply U transpose by X it multiplies it by each of the uis only one of them survives for the kth term only the case one survives because otherwise they would be zero and so I get this then when I multiply it by the diagonal matrix I get U times sum k equals 1 to n Lambda K Alpha k e k and then I get back to sum k equals 1 to n i multiply by U again I get Lambda K Alpha k u k because again when I multiply e k the basis Vector right this is the vector where it only has a one in the kth position by U it selects out UK and I get back and so what this means is this is a long-winded way of saying if I multiply a in this basis all it does is act by multiplying by the eigenvectors this is an eigen decomposition if you remember it that's all that's going on right please [Music] nice property that we can get all of these because once we have the use then you can you know user that is yeah yeah so I think the the we haven't gotten back to PCA yet we need one more fact hold on just one minute and we'll come back to that we're just recalling like facts from your linear algebra class yeah but there's a great point you should be thinking exactly that right so let me just point out one fact here which is that if I take the max over all the unit vectors of X transpose X a it can also be written and this is the formula we've been we've been kind of hinting at as Alpha Square sum k equals 1 to n Alpha K Square Lambda k okay so now if you think about this how do you find the principal eigenvalue how do you find the largest way to express this well since we only get to spend oops since we only get to spend our Alpha squared along the components and we only have a one unit to spend what maximizes this expression well we want to put as much value go ahead because when we multiply here by X Alpha times x is going to be equal to this expression and then when we dot X in again we get the alpha squares because they pair up to one another yeah apologies if I went too fast great question so we have the alpha squares times the Lambda case now the Lambda case because they're let's imagine they're you know one to ten where would you put all your mass if you wanted to maximize this well on the largest one right so how do you maximize the amount of mass you put on the largest one you put in our in our notation Alpha One equals one because the rest are all then equal to zero this is going to be the largest one and that's the principal eigenvector imagine they're strictly different does that make sense please for many years yeah we're here um yeah yeah so how does it then suddenly become this one is just the fact that it's diagonal so e k times a diagonal is just the case eigen unit Vector times that value and so that pulls it out pulls out Lambda K which is on the diagonal yeah and then when you have it when you have a unit Vector multiplied by a matrix it just selects out that column okay but is this clear if I want to maximize this I set Alpha One equal one right because we know Lambda 1. now what if Alpha One what if Lambda 1 equals Lambda 2. that is there two eigenvalues that are hot that are that are that are present Lambda 1 equals Lambda 2. then it turns out there's an entire Subspace of solutions right I could pick any alpha 1 and Alpha 2 such that the squ that they're Square sum to one anywhere on that Circle and if Lambda one equal Lambda 2 equal Lambda 3 now I can pick anywhere in a Subspace of size three that's going to be important when we think about how well-defined PCA is because it's only well defined what the principal component of variation is if Lambda 1 is strictly bigger than Lambda 2. right if there's no Gap then you can it doesn't the coordinates aren't well defined anymore I can pick anything I just described does that make sense all right please so alpha 1 is equal to one and the reason is so if this constraint this Norm constraint means that I have to pick among all the alpha such that they're Square sum to one so if among all the ones that square sum to one which one's going to give me the biggest value intuitively I want to put all my mass on Lambda 1 because Lambda 1 is the biggest value right and so that setting I would set alpha 1 equal to 1 and all the alpha k equal to 0 for K greater than one that would be the value that I would pick there because that just that one's guaranteed it you know I can't really do better than that if I slide off even an Epsilon amount of mass well then it's going to a smaller eigenvalue and because I I lost that mass I would get you know Epsilon Square times that okay you can also just compute the derivative using lagrangian if the intuitive thing doesn't make sense which we did two lectures ago cool all right so let's go back to PCA and say exactly where do we get this this UI the thing I wanted to point out that is here I'm going to come back to this point about what happens with Lambda 1 and Lambda 2 if you missed it don't worry I just care that you're aware that what we're doing in the maximization now back to PCA okay so recall I'll just go up here sorry I'll copy because I'm extremely lazy where is our PCA where did we do this oh here I just want to like make sure you realize I'm not like doing something strange and changing the expression this is what we wanted to deal with well this expression here we can rewrite and we can rewrite it as x i transpose U transpose oh sorry X transpose U transpose x i sum I goes from 1 to n one to n okay let me drag that and give myself a little bit more space here so it's not crowding you too much okay so this is equal these two expressions are equal I'm just expanding out the square but this is pretty nice because now I can pull out you oh sorry I wrote it backwards I don't want to do this let me write it the other way it'll just make my life easier in the next move you too I'm so sorry about that expose transpose you okay sorry that was that was foolish it's true but foolish okay so now I can pull out the use because they're on the outside this is U transpose sum I equals 1 to n x i X I transpose U okay why is that because this is linear so I can pull this out of the sum and all of these things are are paired up this thing here is this is a quantity that you may remember previously this is the covariance of your data why is it the covariance because you subtracted off the mean this is the sample covariance and so here I can push the one by n inside let me do that too 1 by n and then it's really the sample covariance okay this thing here so then what will you be if I want to maximize this U well it will be the Principal eigenvalue right this is an eigenvalue problem just as we went up here it's now of this form we can verify is it symmetric yeah it's a symmetric it's a sum of these are each are a sum of symmetric matrices so the covariance is symmetric it is actually really a covariance matrix it also happens to be positive semi-definite that means all the lambdas are non-negative so that's good news we didn't need that but that's nice to have and now when we look at the U's as we go through there we have to pick a direction which direction do we pick well we're going to pick the direction that corresponds to Alpha One which is U1 right which is the basis when we do the decomposition here it's the principal eigenvector of this thing so the best you to pick is the principal eigenvector of the covariance right some of you are nodding some of you look like I said something horrible so please both ask questions this is a covariance we're going to sub this covariance Matrix into the a awesome what happens if we want two components what do you think we pick anyone the first two that correspond to the largest two eigenvectors three the largest three if the first two are equal and you want One Direction Lambda one equals Lambda 2. now you got a problem because you have two potential representations for it but you can still pick a component of principal variation and that's the way that PCA is potentially undefined okay all right cool that's all it is so how do we represent data with this just to make sure it's clear well we map x i I some let's say we picked D dimensions x i minus u j u j namely this is its coordinates in the top K eigenvalues that we picked on average this captures the most variance in our data and we just keep this is a scalar this is you know what we were calling Alpha J earlier we just keep these k scalars okay so this map what it does it takes in a thousand Dimensions let's say I started with a thousand dimensions then it's going to pick five which five well they're going to be a blend of those they're not going to be any individual Dimension they're going to be this these eigenvectors of the underlying covariance they capture the most the reason we like them is they capture the most of our data they throw away the least amount of our data which is the other residual interpretation and this gives us a map that goes from you know R let's call it Big D down to R Little D okay and that's in what sense this is a dimensionality reduction and we can use that for example to take our data and take it from 100 dimensions and project it onto two and visualize it draw it on a map um yeah awesome peace [Music] yeah great question so you want to know how do you pick K or D here Little D so let's hear in contrast for the last three uh lectures I've been telling you I have no idea how to pick K here I at least have an idea oh go ahead oh I'm just confused I think I confuse you with something so let me make sure this let me just call this K just for now to make this K is less than what you're thinking of as D how about that that's a great point so it's decomposed into D dimensional vectors but it only now takes K coordinates to represent it so again going back to the two-dimensional picture we have a two-dimensional picture but we projected everything onto the line and so now we can represent things by just its distance on the line the alpha one so we've taken two scalars which were like it's X and Y coordinates and reduce it to just one which is the Alpha One does that sound make sense awesome all right so let's see how we pick K here how do we pick k and this won't be super satisfying but you know whatever be fine okay so this actually does have some have some kind of trick so what does this actually mean so this is this is basically looking at the trace of a which is equal to this sum on the bottom Lambda I okay so if you know if you remember your linear algebra but basically what it's saying is what I want to know is how much of the space am I explaining so imagine I have 100 dimensions intuitively if I get the top 10 of them the worst case is that they explain point one of my space meaning that my sum of those eigenvalues are about 0.1 and I'm throwing away roughly 90 of my data it's not what it means but just kind of think intuitively what this is saying is that traditionally people will pick K here so that they explain a lot of their data and so if your data does your data like you know as a Rough Guide if I pick 10 components of your data and do PCA on it do I capture 90 95 then that means that was a good selection and you can now compare K's based on how their eigenvalues are ordered if it goes down to 10 to the minus 10 right your fifth component is 10 to the minus 10 you don't need it pick the fourth instead just pick bases of four so now it gives you a way to actually start to compare them and you can get error guarantees you may worry Computing these eigenvalues is super expensive because you have to compute like an SVD on some Mega Matrix but you don't have to and the reason is this is a trace and if you remember your Trace equalities you can just sum the values on the diagonal to get the trace which is the sum of the eigenvalues please and that was like too large um and say that this quotient came out to be like very very high like 0.999 right would you saying that you're overfitting yeah it's exactly right it's like it's a finger overfitting you don't need to do it and so it's kind of like if you're if you're keeping those extra pieces of information like intuitively you don't you want the smallest K that you have most of your data so you can tack on more but it's kind of like what's the additional value to do it traditionally you'll use K as I said PCA kind of is a visualization technique or like to get some rough sense of the data and so you try not to have K higher than two or three and it just provides a sanity check like if you run PCA and the first two components of your uh you know your eigenvalues account for almost nothing in your data then it's not really clear what conclusions you can draw right the other place where it can cause you pain is the thing that I keep illustrating which is that if Lambda 1 and Lambda 2 are equal to each other which on real data is actually very unlikely but numerical issues could make them kind of come together then the coordinates could be pretty fragile right so I run PCA once and I get you know one comma two comma three as my you know sorry Alpha One Alpha 2 Alpha three but then I run it the next time and because I chopped it three and the first four were equal some new coefficient comes in right so really like those are the instabilities that you worry about the most if like your lambdas are a bunch of lambdas are really really close to each other then your coordinates aren't really preserved and anything inside the Subspace goes so that's the other problem with PCA it really only makes sense when the spectrum is is separated and people don't usually check that and as a result they uh lead to erroneous conclusions so you tack on too many you get there and then also you can have these issues about um you know non-fidelity in the representation those are the two main ones you got it perfectly okay awesome any other questions about PCA now you know it you love it so much all right great all right let's talk about ICA all right so ICA sounds very similar to PCA we only change one letter but it has nothing to do with it one has nothing to do with the other so that's refreshing uh but they have um something about them that I really like so all right so first I'm going to tell you the high level story of ICA which is this cocktail party story high level story then some key facts these key facts are useful because you will run into them in your homework and I just want to highlight them and then we'll talk about the model and the model will be the least interesting part so here's how it works here's the high level story we have people and by the way I think their homework is the world's most boring cocktail party I think they like count numbers to 10 or something so it's not like you're going to hear some salacious gossip you're just going to hear people counting to ten some TA from like five years ago all right so anyway so you have people here are two people they're happy those are people one and two we have microphones over here mic one mic two okay now here's our problem when the people speak they don't speak directly into one microphone they're just ambient microphones and so what happens when this person you know compresses the sound waves is that boom it hits microphone one but also it hits microphone too similarly when person two speaks they speak in some way that same this is supposed to be the same wave by the way so like the wave didn't change based on them but it just took longer to get to the microphone we're not going to worry about length too much but the point is what microphone one and microphone two uh C is a mixture of the sounds they don't just hear one person they hear two speakers simultaneously superimposed on one another and the goal is to take what we record at these microphones and recover kind of the time series of what they actually said and since if we had the wave that's just the air we could play it through a speaker and we would actually hear what person one and person two said and you'll be able to do this and it works which is kind of wild now I do want to emphasize having built things that look sort of like this this is a naive way to look at the problem in the sense of like what you would build if you were doing this in industry but so what it gets you the core principles and you can look there's whole things about speaker identification and in fact like there are certain companies that when they ship their products they brag about how they can identify different speakers in the home so like as weird as this is like people ship products based on their ability to do this okay all right please ask questions if the setup doesn't um make sense two we have data X and we'll see it for some time we're not going to assume things or make this simple the people aren't moving around in the room uh they're not changing just just to make our lives a little easier okay so we need to look at what does the actual data look like all right so as I mentioned oops speaker one is going to be kind of this time series that looks like this okay now we don't actually see the whole continuous thing what we see is we see speaker one we don't even see this by the way this is what kind of we get samples at various regular intervals that's the way we conceptualize the problem this is like how audio processing works right um two at a time one so on let me just draw this and I'll talk and these should be evenly spaced if I could draw properly and had enough patience okay time one the point is is what you see is that you just get these measures of intensity so s t j is speaker J intensity at time t okay now we want to recover this if we had this we were in good shape right if you think about how you actually record audio you know kind of high-end audio is usually recorded around 44 kilohertz right give or take you can probably understand people much lower kilohertz I don't know exactly where it breaks down um but my point is is like this is actually how digital recording works right you sample at a bunch of points you get the intensities and then you play it back through a speaker okay all right and then there's speaker two which I'll also draw oops and then just so it's clear like we sample them at a bunch of points too maybe speaker two more loquacious I don't know S one two oops those look terrible I don't know why I'm gonna fix this but I am and these are sampled at the same time point so my drawing doesn't do is imperfect in many many ways uh two two s t two so on blah blah blah okay and these are going to be sampled at the same time points just to make our lives easier okay all right so now we don't get to see this as I mentioned we don't see S1 and s2's Time series right we get to see the microphones only XT 1 and xt2 which are sampled at the microphones okay so far so good so we need a model of how we're going to do this and of course kind of our model from what we described above is going to look something like this the time that what we observe at microphone J at time t is a mixture of something from speaker one and something from speaker two okay so microphone J just to make sure it's clear here Mike J sees a mixture and we're going to assume this mixture is fixed right just for the moment right if they're moving around the room that's not true any longer but we're going to assume it's fixed for the moment okay so I can write this compactly as X of T equals a of s of t all right now what do we know here this we observe this is the data both of these are latent we don't know the mixture we don't know the speaker intensities okay for the moment I'm going to assume that the number of speakers and the number of microphones are the same you can imagine because there's a matrix here that you know if I have only one microphone this is going to be substantially harder actually impossible to to do the Reconstruction but I'm going to take advantage of the fact that I have different mixtures at these microphones okay so assume I have number of microphones the same as the number of speakers okay so is the setup the high level story and setup clear intuitively what should happen right let me write some math sure it is a is actually the mixture so a right here this model says that what we see at time T it's from at microphone J is some fixed mixture of what speaker once at a time T and speaker two set at time T we're not modeling any delay or anything like that so they make their the sound they go ah and then it hits the microphone and then the you know the mixture of person one and person twos uh pressure hits the microphone at the same time they don't they don't actually matter the physical units don't matter in any way but you can think about them as any unit of pressure that you want okay is a is unitless a is just a pure mixture you can multiply and add things because the s's are the same type cool all right so let's make this a little bit more mathematically precise so we're given X1 X n element of r d and D is the number of mics and speakers what we have to do is find S1 to SN that's also amount of Rd okay so I'm preferring to have the notation over time we also are going to find although we really don't need it for the model this a that is d by D okay such that X of T equals a s of T now if we estimated a right there's a pretty easy way to find what we wanted if we knew a someone gave it to us this problem is Trivial just take the X's multiply by a inverse and you have yourself the S's right now the terminology is we call a the mixing Matrix for the reasons I just outline a the mixing Matrix and W equals a inverse which will use the unmixing Matrix so why would I introduce and bother you with this it turns out that W is actually the right way to write a lot of the guarantees and you'll see why in a second okay so we're going to write this as right w is equal to W1 transpose to w d transpose and I'm just doing this so that I can write the following so that this is just one way of writing the inverse so that sjt equals W J times x t nothing happened here I'm just giving you notation of how I think about the mixing and then this inverse is going to be important the inverse is kind of obviously important because we want it if we had the a we would multiply by its inverse on both sides and that would tell us the speaker that we were after right so w is what we need to multiply by okay now the things that I actually find interesting about this model so some caveats we talked about some of these a does not vary with time we're assuming that time if it did we have to use something more complicated so we're not going to do that two this is more interesting to me there is inherent ambiguity in this model I like when there's inherent ambiguity when you can't tell two things apart because it forces you to understand what the model is doing so one thing we can't determine speaker one versus speaker two we have no idea who speaker one and who's speaker two in real life so it can we can only determine up to kind of how we permute their Time series okay so speaker idea is opaque to us maybe one time we were on the algorithm are indistinguishable we don't know the labels okay of course we can tell that there's one person who's you know saying numbers in English and one who's saying numbers in French we just can't tell who's doing what in this model the other thing which is maybe a little bit more subtle is we can't determine absolute intensity and I'll just write the equation and it's because we're multiplying two things together so notice that if I take any constant multiply it times a and then take that same constant and multiply it times s this is still equal to a of s of t so we can possibly only know that the person we can't tell how loud S1 and S2 are we can tell relatively how loud they are but the mixing Matrix we could multiply by a constant and it wouldn't you know wouldn't change anything right it was the scaling would go through and because we're multiplying them in our framework we can't determine this either now that has a surprising surprising thing and this is why kind of I like to teach it intellectually oops surprising the speakers cannot be drawn from a gaussian distribution we're going to have to make some statistical assumption but they cannot be gaussian why suppose they were x i would then be a normal drawn with some mean from aat but then as we saw before if u t u equals I then a u generates the same data what does that mean s that any rotation of a generates the same data multiplied by U and the reason it happens is because the data here are rotationally invariant because they're Co their their covariance Matrix is a times a transpose and that is not sensitive enough to tell about all these rotations the same reason we love gaussians because they were rotationally invariant means that that symmetry we can't recover anything in this problem and so that at that point you may think gosh there's no way we're going to be able to do this we have all these symmetries around the speaker and we can rotate the X the the intensities in any way we want what are we going to be able to do and it turns out you can recover something here and weirdly enough as long as the uh distribution roughly speaking is not gaussian and is not rotationally invariant you can recover it and that's kind of remarkable and it's worth thinking about okay now the algorithm is going to be so trivial the algorithm is just going to be gradient descent and mle we're just going to set up a likelihood function and run everything that we've been running so far okay that's it all right now we need one trick one half second you know two minute detour because this this causes problems every time people look at this it's just one little detour about how random variables behave under linear transformations under linear transform all right all right and this is just the reason I'm doing this is It's a key confusion if you remember your I don't know I'm not going to say what you should remember some calculus thing just basic you know change of variable formulas for integrals this will make sense but we can draw it in pictures and you don't need to know that at all so here we go so just imagine I have something that's uniform on zero one okay and now I have a new variable U which is equal to 2 times s what is the PDF of U in terms of s now we're tempted to write P of U X over 2 equals P of s of x now let's take a look at the PDFs so here's the PDF of U it goes from 0 to 1 and what's its height well we integrate over the entire thing it's one right this is our PDF of you sorry of s sorry PDF of s this is the uniform now for you what happens we go from zero here's one to two we know we have support from zero to two because it's a uniform distribution right you grab a point here you multiply it by two and it's going to sit somewhere in here oops go away right it's going to sit somewhere in here but what is its height it's got to be one half so this relationship is clear so PS of X is going to be equal to 1 if x element of 0 1 it's 0 otherwise pu of of x is going to be equal to PS of X over 2 times one-half okay and that's just the normalizing constant and so this key issue here is this normalizing constant yep normalizing constant that's it so when we do this in higher dimensions we have p u of x and we want to map by some linear a oops so that is like a u for example right what happens right right oh sorry a s that's the way we're writing this because I want to keep the notation a s equals U what happens in higher dimensions well we still have p u of X is equal to PS of a inverse X but we need a normalizing constant here and how does it take if you imagine I'm taking a box and I multiply it by a a matrix a what how does the volume change that's the determinant that's all a determinant does takes a box and the volume of it is going to be exactly proportional to the assigned the absolute value of the tournament this determinant is signed because it's a oriented measure but that's what you get okay you can convince yourself in two Dimensions pretty easily in higher Dimensions it actually requires a little bit of work potentially to do it and this you probably learned as your change of integrals form change of volume integrals formula uh at some point times the determinant of w okay okay now the thing that I used here was the fact that one over the determinant of a is equal to the determinant of a inverse when I did that okay so the point is when you do change of variables you have to take this determinant into account you probably did this with a Jacobian at some point in your life and if you didn't don't worry about it it's not that big a deal it just says if I take a box and I and I map it by a linear transformation what's the volume of the Box going back to this uniform case and then if you care about how you would probably prove this you just think about like integrals you break out into tiny little boxes that's it whatever space you're integrating so this we did it for one box but you can do it for many okay so we're going to use this formula for the rest of our time please okay so yeah so the the idea here is that we have a probability distribution in one space now I want to probability distribution in my new space so I have from zero to two so I'm going to take the X and I'm going to divide it by 2 and whatever the height is over there I'm going to get it so if the height is one I get it the height is zero then that should be right as well but when I do that the problem is is the probability distribution I had I'm using that value will be one right when I do that and I need to just multiply by some constant so I'm thinking here about the PDF for that random variable yeah so so basically you can ask you as a product it's a random variable and so a random variable is nothing more than like some function on the it's just an integral yeah awesome question all right so once we have this fact this problem is super easy ICA is mle why is that P of s equals sum J equals 1 to D s of SJ okay this is where we use the sources are independent we have to assume this in the model and they have some distribution that's not gaussian but not gaussian this equals so then P of X is now equal to the probability J goes from 1 to D of p s w times x times the determinant of w but now this is something that we can compute this is written in terms of X and our Matrix a and we can just do gradient descent on it okay so how does that work here's the key technical bit right we're going to set PS of x proportional to G Prime X 4 G of x equal to 1 plus e to the minus X inverse there's nothing really magical about this function except for it's not rotationally invariant and then we can solve the likelihood of w is sum from time goes from 1 to n um J goes from 1 to D log G Plus W J x t plus log determinant of w okay so maybe this looks pretty intimidating but what happened here this G is just some likelihood function I don't actually care what it is that's the thing that's weird that's the thing I want you to confront it's not that it's like some specially chosen function when we would pick the gaussians we were picking it because of computational and kind of other reasons oh it only had two moments and we could compute everything we wanted this is basically saying honestly I can pick almost any G I want any likelihood function I want on the speakers as long as it's not rotationally as long as the the measure isn't rotationally invariant there will be a unique solution if I look at enough data that's kind of fun that's kind of an interesting thing to think about you say what G do I pick well I'm going to pick this one because I know it's not rotationally invariant people pick other ones how do you pick a measure well you pick the problem the PDF proportional to something that looks like a CDF what does this function look like it's just a sigmoid function so I pick the probability distribution so it's kind of low to high it's not rotationally invariant on each component and I'm done that's kind of wild okay I'm not super worried that like you grock absolutely everything here again these lectures are not supposed to walk you through line by line and read the book to you what it's supposed to do is give you a high level structure for how this algorithm is going to go and what are the key twists now the one thing that's that'll probably scare you is you're like well I don't know how to compute the gradient of the log of a determinant and weirdly enough that actually has a form so that is actually a that's an object that you can compute and compute gradient descent on and so you just run gradient descent on W you have to do some derivatives in the old days I guess now you don't have to compute derivatives anymore you have Auto diff software for you which will kind of do it automatically like Pi torch will do this for you or jacks or something but maybe we make you do it by hand I don't know if we actually do that but we do you can do it it's not bad you look it up or you drive it it's not super hard but it is weird that it works okay and miraculously I'm not going to have to convince you of this in class you run the thing and you will hear someone saying one two three four five some ta and it will work even though the what's at the microphones is a mix pretty wild all right all good oh by the way why so what is the log of the absolute value of the determinant right it's the determinant is the product of all the eigenvalues right so if you take their absolute value and take their log it's going to be the absolute values of the sums of the eigenvalues now that looks a little trace-like so you can imagine why this is actually relatively easy to compute but your W small you can brute force it for these kind of things okay awesome any questions about this please this piece right here yeah so what we're doing is we have P of the speakers each one of them some probability distribution I'm being vague at this point when I was writing it because I wanted to basically get to the point we can use any distribution that's the weird thing basically any distribution so long as it's not rotationally invariant so I don't know what PS is yet but just imagine it now I'm going to move it to a distribution from SJ to a distribution on X's so the speakers I can't observe I don't get to measure them but I do get to measure the X's now I only have one variable that controls everything that's w now I can do gradient descent on W I didn't know how to do it on their product before I'm setting S and W as a product but because there's only one now I can run gradient descent on it and it happens to be kind of nicely concave and all the rest of the stuff I want [Music] exactly right and the thing and the thingard here that I want you to confront is and when the assignment is like if you use the standard distributions you don't get you don't get a unique answer and weirdly enough almost any distribution you use will work we happen to pick this one but you could have picked something else and that's kind of wild so right that's a weird thing just can't just have to be able to distinguish it and what you think about there is the prior on the speaker doesn't matter too much they just can't mix in some awkward way you can't rotate the speakers intensities and have it make sense you have to be able to distinguish kind of up versus down fantastic other questions all right so I'm going to spend please I want this Jeep yeah like how things exactly you can get like how do you get the deal this one is exactly proportional yeah you don't okay so I can take any function I want and normalize it as long as it has like some very it's smooth and other things and normalize it from like minus one to one or minus infinity to Infinity as long as it's integrable and make it into a probability distribution as long as it's positive right positive this function's positive I want it to go kind of uh I wanted to go from low to high so it has that nice feature to it and I don't care about anything else but now that's a CDF right because now I have a function which the integral of which is you know zero the zero to one and then what I'm going to do is I'm going to take a differential and that's what gives me the PDF this is just probability Theory stuff but yeah and you're like the thing that should bother you and is bothering you is like why did you pick this function and then I'm telling you that's disturbing go pick other functions still works but there's one function that if we use which we've been using the whole quarter all sudden everything breaks that's weird it just means that like not every prior matters sometimes priors get these like you know undefinability things and it's coming from this rotational invariance and that's weird it's going to bother you but then you run the code and you're like oh my god it works and you can play with it see how close you can make it to rotation the invariant if you motivate or you just do the homework and turn it out either way but that's what's interesting right I'm going to lose our last little bit of time together to tell you something kind of different which was sometimes about weak supervision this would be less mathematical and potentially have more bizarre pictures we didn't get any activity on the thread I want to tell you about one Trend since this is our last lecture together will take over and we'll see if we get through this all right okay so I want to tell you about weak supervision and I want to just motivate it for you in the last 20 minutes of why we care about it and mathematically the reason we care about it is it's an example the underlying model that I'll show you is an example of these em style models but we can solve it exactly in a lot of situations these are my slides and I have to give these quinoa to give a keynote this morning if you have two more this week on random conferences they have lots of weird pictures on them just ignore them okay so the mo let me motivate for you why I care about weak supervision and then I'm going to skip over some stuff and then we'll try and get to the mathematics of the model okay so the reason we started thinking about weak supervision and others too this thing we call data Centric AI was that machine learning at least in a supervised setting has three pieces it has a model which you're learning a lot about in this course it has training data which we've talked about a whole bunch it has Hardware and the thing is which is weird outside this class and for a variety of good reasons models have become Commodities so all of you in this room can go download you know Google Microsoft Stanford's latest models just the culture of AI is that we put everything online and there's been a ton of investment in in that so if you want a state-of-the-art natural language model you can download it like go to hugging face download it you want state of the art Vision model there's a tutorial somewhere that will allow you to do it you can get it instantaneously and because of the cloud Hardware is a big problem it's not a big problem people can get access to it okay still interesting problem but it's there but training data is hard this was the original idea and the reason training date is hard is it's the way that you encode your problem about the world it's how you label it now when I tell you about uh you know supervised machine learning the first day we have X and Y Pairs and I never tell you where they come from you have X and Y pairs they came from God herself they landed there and you're like well this is where machine learning stars but that's not at all how anything Works in machine learning it doesn't start then and it's not like hanging out with Beyonce it's like living in a sewer so if you want to do this stuff in Industry it is much more like hanging out in a dirty filthy sewer you have all these data streams you don't know where the heck they came from they all have weird errors they all have weird correlations with one another and it's a huge mess and so we started to look at this problem a couple years ago with many other folks in the field and we just wanted to put mathematical and system structure around how to make this less awful okay that was the motivation right now I'm not going to bore you with the fact that like people actually do this and and all the rest it turns out that like you know this is an interesting area but I'm just going to show you a toy example so here's a toy example and a little bit of light math okay so here's an example of something we may want to do and generate training data and there's a whole longer talk here of of how exactly this works let's say you want to do named entity recognition you want to label persons and hospitals in text okay you just want to know you have some mentions or these mentions of people or text Saint Francis could be a person could be the hospital Bob Jones probably a person but I guess could be a hospital too okay so here's how these weak supervision Frameworks work they basically allow you to have these little voters that you write to reuse training data that you had before an existing classifier that's already voting person you just throw it in here's another kind of classification rule uppercase existing classifier says person you put that in and then Hospital well I have a dictionary of Hospital names it's called distance supervision so this one votes hospital and the point is is each one of those noisy sources in the sewer is telling you something different and all you want to do is determine How likely is answer a versus answer B okay so this is how it works in a particular engine this is the first system that did it there's many others since then don't worry too much about that but what it did is it estimated this graphical model and I'll show you how it works in just one second because it will look very familiar to you you take in all those sources and you try to estimate how accurate is every source and how correlated is every source with one another and then you make an estimate over all the labels for every point how probable How likely is it a person a hospital or whatever okay that's what you want to do so we model this as a generative process okay there's the graphical model there you've probably seen something like this and the instance here is we want to learn the Y value that's there the label that's there without any hand label data so we want to look at the votes and somehow deduce the why okay there are reasons you want to do this that are Beyond this but we're not going to have any label data when we do it and we want to learn the correlations and the accuracies okay awesome so it's not at all obvious that you could do this the reason it works is because you can see these voters on many data sources millions of data points every point that comes in every voter registers yes no or indifferent and you can look at their observe that they're overlapping judgments and estimate their accuracy and their correlation if you knew for example Source One was always right you just count how often every Source agreed with Source One and that would tell you the accuracy of source 2 and Source three okay but they could be correlated as we saw that first function and second function called each other they could be correlated in some way and you'll have to deduce that okay all right so here's the way that you solve these underlying problems I'm just going to go through this very quickly because we don't have all the math but it basically looks like those covariance matrices that we saw from before the problem here is that we want what's in the red we want to know how often does Y which we don't see how often this thing let's make us make it zero or one so it's true we don't see how often Y is correlated with one source we only see how often the sources are correlated with one another so set another way we can only observe what's in Sigma sub o we don't get to see how often the sources correlate with the ground truth okay now here's the thing that is pretty interesting it turns out that the inverse of this Matrix has a structure this is one of the most beautiful mathematical facts that comes from the graphical models course and I'll just state it here without proof it turns out that every time this graphical structure is missing an edge there's a zero in The covariance Matrix okay this allows us to write this in terms of The observed values and the and some rank one parameters and these zi's basically say if they're one they have perfect accuracy and zero their total noise okay now why that's interesting is it lets us set up a bunch of these equations that basically say we know the left hand side is zero because let's imagine I know the structure I know how they're correlated you can solve for that if you want it and it turns out this will be kind of weird stuff it turns out that actually here you can complete this as long as you have enough of these linear constraints now a couple of things here first Z and minus Z are solutions so this gives us back to those symmetry questions I wanted to highlight you can't tell if everybody's correlated with the ground truth or the exact opposite you have to make an assumption if you had a bunch of malicious labelers who were all telling you the exact wrong thing and correlated each other you would get fooled so you have to make some assumption there the second thing is when zi is out equal to point is equal to zero that means it's a coin flip if I added a labeler that just flipped a coin this should give you no information and indeed it won't it will it will Zero out every constraint that it's in with sanity checks you can't kind of make your job easier by adding these labelers it turns out it won't bore you there's a great paper from version from like 2014 2015 that measures the notion of rank in a continuous way it's called effective Rank and this tells you the right statistical scaling for this problem the closer those Z's get to zero when can you recover The Matrix super fun stuff it's a nice geometric piece okay so you heard all this stuff I just wanted to share with you it's stuff that I end up working on you probably think at this point like there's no way anyone would ever use these crazy covariance matrices but I just want to share one thing with you which is wild to me because my students did this it's actually used in all these applications it's still in production all these years later and this is a way that people cope with the fact that they have lots of noisy training data and put them together so in search ads in Youtube Gmail and products from Apple and so this weird framework of how you program training data just something I wanted to share with you and the underlying model is like classical from what you've seen except for instead of solving these em problems you solve these weird covariance Matrix inverse problems and you can solve them provably now turns out to be important for a variety of weird issues of like estimating Source quality you don't want to rely on Em which we talked about doesn't have a unique answer for this we can tell you exactly where the unique answer is thank you for your time and attention and thank you we'll see you on Wednesday"
Stanford CS229 Machine Learning I Self-supervised learning I 2022 I Lecture 16,"Stanford CS229 Machine Learning I Self-supervised learning I 2022 I Lecture 16

today we're going to talk about self suppress learning this is a lecture that um that doesn't have a lot of math but it's going to be all about very recent works like probably like in the last three or four years at the most so and these are kind of like a pretty interesting kind of intriguing kind of like Concepts you know but nothing very complicated everything is kind of simple um so so basically I think you know about probably like 2013 2014 I think you know deep learning new networks you know this kind of ideas start to take off and and we have this kind of kind of like a revolution of AI in some sense like we have a lot of amazing results in the last about 10 years right after deep learning took off um but in the last two or three years I think we see kind of like an emergent new paradigm of AI which you know she's still based on deep learning new artworks but I think kind of like in some sense um the kind of the paradigm shift a little bit towards this kind of like large-scale unsurprised learning or self-spice learning so that's that's kind of like what we're going to talk about today so I guess uh um uh last few lectures I think Chris talk about Express learning so those are mostly ideas you know like more classical ideas like that was you know developed like before deep learning took off and today I think we're going to talk about the kind of new type of five spice learning you know which is not that different from the old ones but uh you know with new artworks and with some you know more uh kind of like a um some kind of like technical differences and and also like some differences in the conceptual way so um so I think um in some sense this is called like a like a actually we also write a white paper about this uh like a bunch of extend for people like actually there are 100 people more than 100 people on the paper so um mostly stand for researchers stand for faculty and students so rather right direct um we wrote a white paper on this we call it Foundation model um so in some sense it's just a name so um um it's a name for kind of this large set of ideas that involves kind of like pre-training online surprise data and then kind of like use it for a bunch of Downstream tasks I'm not sure whether you know any of these passwords but that's what I'm I'm here for so I'm going to Define some of these words and and tell you what's kind of the pipeline and the Paradigm is so basically um uh kind of like the the main thing uh for deep learning is that you know we use like a new networks to face you know larger data than we used to do before like 10 years like 15 years ago and and typically at the beginning when you use deep learning you use it for uh surprise learning right you have a lot of data uh maybe a million data image night and you have labels for them and then you pick right work to predict the label and it turns out that even audio network is pretty big um as until data is also somewhat big you know like you can see good results and sometimes you can even make it work on small data so but all of this requires you know some amount of label data right so um and then um recently people figure out you know maybe we should use unlabeled data as well right so when you use unlabeled data then you have you know much more like for example for tax data I think there's this data set which has like 40 trillion uh words in it um so um but if you have label data sets then you probably have a million labeled documents you know and also label has has specific meanings you know maybe you need multiple labels for a documents very many times so in many cases so if you if you say I can tune with unlabeled data then you certainly have availability of a lot more data so I think that's kind of like a uh uh the kind of the main kind of the driving force so now we are training on unsurprised um uh like on unlabeled data so and this kind of fight so and how do you tune with unsurprised unlabeled data I think often um people have you know different words for for the same kind of concept I think one of the way to call it is because self-surprise learning so you supervise and as you'll see you're going to supervise your model with um yourself your like the input itself so so you don't have to use any label and sometimes you know um as I said you know in a white paper we call this family of models called Foundation model um Foundation it's kind of like uh I will explain this word a little more um and some other bars words include pre-training I'll explain on them as well and adaptation so I guess I will just um start by explaining pre-training so in some sense the way that people do this right now is the following so you have this so-called pre-training stage where you um pre-tune description oh so large scale and here large scale really means very very large scale and label data set it's not always the case that you have to use unlabeled data set we'll see actually in some cases you can use the label data set for pre-tuning but you really have to make a large scale um and an unlabel is that is kind of more common than uh the label data set and also you push a large model a very large model and then this the second step is that you somehow adapt this so adapt this preachment model to some Downstream tasks actually often you can download adapt to my adoption tasks not only one um so but the adaptation to 90 task is the same as adapting to one you just do it one by one so you adapt to some Downstream tasks um tasks and these Downstream tasks are often labeled data set so and for Downstream tasks you know you have different settings you know describe you know in more detail but generally you have you know a few examples you know in a downstream task uh for example you know um one example could be that you preach on unlabeled text right that you can download from internet right so you could have like a chilling words you know like documents uh like a lot of documents with your children words download from data from internet and you preaching your model on this unlabel data set uh how do you personally I'm going to tell you and then after you get this model this model is preachment you know with no labels so you don't really it doesn't really solve any particular task and then you say I'm going to have a downstream task about tax maybe I care about sentiment analysis meaning you care about whether this sentiment a document or sentence is positive or not right maybe for Amazon review where you care about whether the reviews is positive or not and then you say I'm going to have um you know a small number of examples for sentiment analysis where I have I have a small number of examples with documents and label the positive or negative label pair right so maybe like a you know a hundred or a thousand kind of like pairs of documents and label and then that's called Downstream task and then you want to adapt your prediction model which is very general with generic um to the specific task using some additional kind of like tuning so so that's the uh the general idea and sometimes you know the down of course you know kind of the distinction between these two steps is that this time you you may still involve some training some kind of tuning in this adaptation step because you're going to see the new examples on a downstream task the sentiment analysis does right and then uh you have to kind of do something you know with those examples so there is a there's possible a training of transition step in the adaptation uh step as well but uh the difference here is that this step often involves very large scale data and this kind of generic it's not like about task specific uh learning so here is really about task itself and also often times you have much fewer data points uh than the approach winning step you know sometimes you have a thousand sometimes you have ten thousands but generally not by not a lot sometimes you can even work with you know even zero uh examples in adaptation step you can sometimes still adapt to the task so so kind of the intuition is that the future new step is about you know learning the generic structure or the kind of like the intrinsic structure of for example texts right or the for example if you do it for image you are learning the intrinsic kind of like structure of the images maybe the kind of the the intrinsic features about images and then the adaptation step is more about the task itself right so um for images you can care about the many different tasks you know maybe you care about recognition maybe you care about you know classification or maybe you care about in a different kind of like labels right so so you can have labels of different granularities you know a bunch of different tasks right so this step is more about tasks um so um so that's the kind of like the the general kind of like intuition right you approaching a lot of data so that's uh give you the intrinsic structure or the best intrinsic representations uh for this kind of data and that representations are useful for Downstream tasks so that so that you don't have to use like a a lot of examples in Downstream tasks right because you already learn some interesting weapon stations that helps you uh for the downstream task um right so so basically you know the kind of the one implication here is that you know you expect that this is uh this this pipeline is going to do better for than just preaching just training your your model only on the downstream data set so that's that's the the basic goal right you have the Baseline is that the trivial baselines that you you just directly tune your model on the task Downstream task um but generally you don't have enough data for that but if you do this kind of like transfer um it's transferring from the unlabeled data set to this one then you may do better and and we call this kind of like preaching model also Foundation model so here I call it pushpin model right so you can call also called Foundation model and a reason for the word foundation I think the kind of like the implication is that this uh this person model is kind of like a um a general model that you know general purpose model that can do a lot of different things um and that's kind of the foundation and this is the adaptation that's a good question but I'll talk about this yeah okay any questions uh so far so I'm going to tell you okay so this is general idea I'm going to tell you some kind of like uh I'm gonna you know start with some simple notations and and tell you how do people generally do this and you're more kind of mathematical way Okay so so pretty training so [Applause] um so I guess so let's say you have some data something like X1 up to accent so you have ended up points and and could be very big and there is no label here so this is unsurprised you know pertaining of course sometimes you can still have labels in it but let's say we focus on a case where you have uh no labels and you have a model so let's specify the input and output of the model so typical you can think of this as you know the model let's say is called V of theta and sometimes you feel this as a feature extractor a feature map um but it's a learned feature map so this is something that maps apps to V of theta if Theta of X and let's say visit of X is some vector so and I guess you can as we have kind of seen this kind of like we have this kind of feature map often you call this representation features you know there are many many names for it reference then quotation slash features sometimes people also called embeddings especially if you talk to more mathematical people because in my field this word embedding is used on in a similar context um so so this is a representation of the of the raw that you have right so the raw data could be text could be image and this is a vector so this is something we are familiar with right so in both you know new artworks we have like we have used the last but one layer as the open station in a feature lecture uh this is the the feature map although like in a feature in the kernels or in the kernel lecture we this is the feature map but in a kernel lecture this feature map is given uh and designed but um later when we do the new artworks this is the Learned feature map and here we are still learning a feature map and we are still learning a feature map feature extractor whatever you call this so and then but you but here you learn this you know without um labels so you say you have some protein loss maybe some protein loss there are many forms of positioning laws as well here I'm just only giving one form something like probably nothing maybe some some laws no um like a sum of laws on single example right so you can Define I'm going to define the laws for different cases um sometimes these laws can also depend on multiple examples sometimes they can depend on labels but something like this you have operation loss okay and then what you do is you just optimize this loss this IOP um and let's say obtain some say the heart so so and I'm gonna call this my protein model or maybe that's called we can call it Foundation model or something like that okay so it's nothing fancy so far yet I haven't told you what the protein loss is um there are many many different ways of ways of doing pre-training I'm going to tell you a few ways but so far it's just you have some Define some loss function but the loss only depends on the unlabeled data and then you minimize the loss and you get some um some model okay so and then I'm going to do the adaptation step which I'm going to be a little bit more concrete because it's a little bit easier um so in adaptation is that let's first clarify what kind of data you have right before talking about how do you do it so what data do you have so you can also it's a down it's a it's a label data set it's a label prediction task the downstream tasks even though I think you know if you look at enough papers there are many other papers talking about different type type of tasks but here let's say we have some labeled um on some prediction tasks and we have some label data set so let's say we have a few examples you know let's call it X task one why task one so and so forth text task and T y task s and this is the number of you know down Downstream examples which is you know presumably much smaller than um like I'm going to use NT the num as the number of Downstream so um on this one you know this is is supposed to be much smaller than the number of unlabeled examples in a perching step so and they are actually a few different settings you know so one setting is that when NT is zero so this is called zero shot learning so when T is zero it just means you don't have anything you don't have the data set so it's called zero shot learning we will see how do you do it you know sometimes you can still do zero like you can still do the task without even knowing anything about a task um so and when NT is relatively small I think uh this is called Fuel shots and what do I mean by small I think in the literature maybe five is typically considered as small 5 10 20. um but maybe not more than 50. if you have more than 50 examples I think of course there's no nobody really defined what what exactly means but I think if it's more than 50 examples um per class in the downstream task I don't think you you would call it few shots so mostly like 10. so um but of course you know there are cases where you can have more examples maybe like I think I've seen cases where you have like 10 000 examples or maybe even a hundred thousands in some extreme cases so um but the the end there typically could be like a billion or even a trillion sometimes 100 billion so okay so how do okay so this is the setup you know I'm giving this information and then how do I uh do it so a high level the first thing I'm gonna do is I'm going to first Define how do I even uh how do I adapt right so I'm giving this model still your heart so what's the what's the model to predict a label I need to have a model that protects the label so here the Taylor hat predicts a feature Vector but not the label so so the first thing is that I'm going to um typically I'm going to have a some kind of like or maybe like me this one one way to do this so This which is called linear um Pro often people also cause linear hats like a um something like this so the kind of like the the idea is you know you are you probably should be familiar with this um it's just like you take this feature fee say the height X and then you apply a linear classifier on top of it you say I'm going to take an inner Paradox of this feature with some linear class for doubling and W transpose X is my prediction model for the downstream task um let's say suppose you have a regression model then you just want this number this to be some real number and if you have classification then this is supposed to predict the probability of the label one of the label so so it's just almost the same as what we do in what works right you just you have some feature and you apply some linear height on this feature so so and then so it will do linear probe what you do is that this is the we have defined a prediction model and then you can Define how to you can say how do I learn this W right so here no no that say the Hat here is fixed so I'm only learning W when I'm doing a linear probe so how do you learn W you just say I'm gonna just tune w um with some loss function with some um loss function on the on the downstream examples so basically I just do something like maybe one over NT and minimize over w and I have a loss function which is the sum over the loss on the downstream tasks assuming you have Downstream tasks right if you assume you have Downstream examples if you don't have Downstream examples you have to this doesn't apply anymore I'll tell you how to do it we don't have Downstream examples um but suppose you have Downstream examples then you just minimize the loss something like the loss that's called L task and this loss function is something that compares the label and your prediction of your model the prediction of your model is something like this right this task for example could be just you know L2 loss you know mean Square loss sorry this this loss could be just the mean Square loss or could be some kind of cross entropy loss or some other loss that you care about and so when you do linear so-called linear probe it means that you only optimize W so you're only optimizing W which is the vector of Dimension at right so and is W is in all right so you're just optimizing this m-dimensional Vector to make it to make your model um fit on to the downstream task um and this is the so-called adaptations that basically the W is the thing that you want to use to adapt your model to a new task any questions so where the label come from you have seen you have a label data somebody gave you right so you collect the data set I as as the same as what we did before but the difference is that you don't you may not have to collect as many as before I I like if you just uh turn on this data set just from scratch then you may need more than what you do here okay okay um so I think um so I guess maybe the questions what what plans for learning you know come into play here so I think you can in some sense call this also transfer learning so transfer learning used to be the transport in this word you know occurs you know like a um like this this uh this term this term was used way before people have done any of the pro training so like uh so like now like I think the like transferred on your probably like people started like in early 2000s or already even maybe before that so and then at that time transfer learning means what it means that you know if I use protein language it means that you preaching with a label data set and then you do some adaptation so so so basically transfer learning means this you know in the new language so um but now these days when we preaching we push on unlabel data set um and another maybe another thing is that um it used to be the case that um like the like when transferring the the the First Data that you turn on is also is classification task which is kind of like similar so the the final task you care about so but here um in some sense the reason why people introduce new terms for this is that when you preach when this task could be could be nothing like um the downstream tasks I I haven't told you what exactly the task is but as but at least you can imagine there's not a lot of similarity because here we don't even have labels right it has to be something different um yeah but but I think you can still say this is transparent it's not like you know there's no precise puncture between this oh okay so let me introduce another way to do um um adaptation so another very common way to do adaptation uh is that a so-called fine-tuning and it's also pretty easy to understand so basically your model is the same um but you just your model your prediction is some W transpose fee fatal effects so here I write Theta but don't say the hatch to indicate that I'm allowed to change this data so here Theta doesn't have to be exactly the preaching model anymore it could be something um that you can change so then what you do is you say I'm gonna optimize both W and Theta um on the downstream tasks but if I just say this then this just sounds like the standard surprise learning right there's nothing different from Super restaurant you are just treating some new network right that blue transpose V of say X on the downstream task the difference from that is that you optimize but you also initialize with initialization that Theta is initialized to be Theta hat because I say the heart is the prison model so basically you just as if you are doing supers learning but Theta is initialized to be Theta hat and W there is nothing you can do with that right because you didn't know W before so w says something new so just the initialize W to be random so that's so called fine tuning and you can optimize the same laws or whatever laws you care about a question so Theta is the parameter for this V is a function parametricized by Theta so that's what this week no oh I think I just mean this is just um fee is just an uh is a name it's just it's it's a fun you can call it a f sub Theta this is an o h sub data it's just a name for uh for this model that is parametric respect Theta so single fee says that is the new network with parameters data I just need a notation to indicate this function right I can't just write status Theta and Phi Theta you know they corresponds to same thing but mathematically I have to write V subset of something to indicate that I'm applying this model on x so um you know I can give a blame for let's say terminal is p but but it's not um I didn't use it explicitly yet foreign have you come over so I haven't told you yet at all so um just because there are so many different environments I have to somehow do it in a top tongue fashion um so so here for the downswing task I can do it just because these two things are very like a you can use this for almost almost every situation but when you talk about preach meaning I have to talk about you know what you do in computer vision what you're doing uh language at least there's some differences depending on the domain of course there are also kind of recent works that try to unify all of this um but but at least I have to somewhat um like talk about the domains and that's what I'm going to talk about next yeah okay cool I guess uh maybe I'll just do that so how do we approach right so um so I'm gonna approach my reputation how do I do that so let's first talk about um um the the vision settings so um or more kind of like the the classifications like the standard kind of classification settings so so let's just uh you know for some minutes I just think of like a vision so so suppose you have some kind of like images right so there are two types of um pre-training so one type is called a supervised per training you may wonder you know why you know like I already emphasized so much that the preaching should be mostly on unlabeled data but actually for vision because you have a lot of label data all right the imagenet is a pretty big data set so you can actually do supervised per training as well um and this is just exactly what we have seen before so you can just um what you do is you just learn a new network um let's call this new artwork you know maybe let's call it U transpose V of x um uh label data set say imagenet so um and here the notation is that this is the um this is the last layer of the new network right the last layer was linear right recall right so and Phi Theta of X is the all the other layers right it's basically the uh the the last but one layer activation right so so feature of X just denote you know what you do in the first you know uh R minus one layers and then U transpose means the last layer of the network and so Theta of course so basically if you have a new network like I don't know whether this I think we discussed this you know in one of the um the lectures so um in deep learning right so you haven't watched work you have a lot of connections you know and eventually octopus and why and you review all of this as your Phi of theta and you view this as the so-called u and sometimes people call this linear Health right so you just do this you know this is just exactly that's what we do um in the um in the new network lecture and then you just discard then you just uh discard uh you and and just take the fee Theta of X right the learned I as the as the as the protein model so you think of this as the feature and this is a you think of this as a kind of a universal feature in some sense and the head is is special to the tasks you use right to the to the label datas I use right maybe use image nuts the Hat has a thousands you know um like classes you know um that's that's something special but the the features is something kind of more Universal you just take the features and and discard the hat and then once you have a new task maybe not image like maybe some other classification task maybe let's say now you have a new task which is Medical Imaging right like you're detecting from a scan image whether some person has let's say cancer right so then you just take this part this V Theta and then you apply a new hat right I'm using W as the new hat right you apply a new hat and then you fine tune all um just a linear Probe on your Downstream task right so so you remove this hat and I apply something else and then you do some tuning on the downstream task um so that's uh that's the supervised project if you're like uh the medical images are like a different size than like the ones that it was preaching on um what kind of like stuff do we need to do to make it work yeah I think typically if the size are different you just have to upside of it I'm assuming maybe in medical unit is lower resolution you just have to do something opposite right you just make the size bigger I think you can pack some either you can pass them zeros outside or you can just um maybe wrap technique Subway there's nothing fancy uh also you mentioned earlier that like if the class the task isn't similar it might not work as well so like is that a concern here so um so here at least you'll have to consider much because because of that because um CR let's say the proton images you know they have the image night classes they are all like uh like sometimes animals you know all kind of like common objects right um but the only way you discard the Hat you remove the hat and then you add a new hat you know maybe now your new tasks just have two labels right cancer or not right um and and you just apply to that um but so in terms of the matter you can at least you can apply it when the task labels are different um but whether it will work you know sometimes depends on how how different your new task is from the old task so typically if you use an image sniper training you always learn something reasonable about the features um you wouldn't be terrible but you know but if your task for example your Downstream task is really not even common images nothing like that's just some kind of like a random images then probably it wouldn't help that much Okay so so here there's nothing really um that um let's fancy and people kind of like there's other ways like in the beginning of the uh the Deep learning uh era um and there are some other kind of like and the second one I'm going to talk about is uh the so-called uh contrastive learning which is now unlabeled um unlabeled uh like a unsurprised learning algorithm for pro training or some other people call it self-su questioning so so contrastive learning so now I don't have any labels I just um have unlabeled inside I need to Define loss function on unlabeled data sets so how do I do that um so I need to first introduce a notion called Data augmentation this so a data augmentation is um something that as the name suggests you are augmenting one example into uh a artificial example that still makes some sense so and typically for images what you do is you just say uh I have a original image and then you augment um by doing some kind of like so-called run you can do a few things maybe a random crop to crop us a a patch of image as a new image or you can apply a random crop plus a flip you can flip the image you know just with the mirror flip and or you can do some color transformation color transformation means that maybe you make the image darker you know brighter or sometimes you can just even do weird more weird kind of like color transformation you change although the the brown color to the white color you know like you can do some kind of like changing of the color scheme and there are many others you know um you know some of them are more advanced and sometimes you can even learn the transformation but these are kind of the common ones so basically given an image you can do this random operations kind of like you can try to choose to flip or not that's a random decision you can choose to crop which part of the image like you can do some kind of like random color transformation so you have some Randomness here and then give an image X basically you can generate a random augmentation that's called X hat and if you do if you do this again you can generate another one which can be which I'm going to call X tilde and if you do it more and more times you can join even more of these augmentations so these augmentation is used in Surprise learning actually as well uh we didn't discuss them just because they are low level details for super certainly what you actually do is that you can give an image and the label you can generate the augmentation and then just assume the augmentation is your new image so um so you just replaced all the image by the augmentation and and every time you do this you know with a new augmentation like like every time you see this or this image you you replace the image by a new augmentation and and tune with that argument images and that seems to improve the uh in some sense you make the data set bigger in some sense because these are you know every time you augment right you're gonna get a different image in some sense even though they are something similar but still you make an effective size of that side a little bit bigger um so um so that's why people use it in um um uh in Surprise learning but now I'm going to use it in honest questioning um I actually now people call it self-suppressed learning so I'm going to use this to create some kind of supervision uh or create some kind of Express logs what you do is you say I'm going to call this X hat first of all I'm going to Define some notation this is called a positive pair so so positive pairs are augmentations of the same image so one um you can imagine you know one property of the positive pair is that these two images two augmented images are probably should be somewhat similar semantically even though they may not have the same color scheme they may not have exactly the same orientation or the right so they are still somewhat similar right so so what you do is you say uh you are gonna um try to make um so you you're going to design loss functions such that one you want to make a fee of theta of X hat and Phi of theta X tilde um close so basically you say I have two augmentations one is x height and the other is X tilde um you want the these two augmentations to have um similar uh representations in the including space so that's one goal of the loss I'm going to tell you exactly what the loss looks like but this is one goal of the loss but if you just have this goal you can see that this loss function is a little bit unquestionable in some sense because maybe you should just map every image to the same point then everything has exactly the same representation then you satisfy this this goal right so so it's ideal that you should have something to counter balance to avoid you to just collapse everything into one one thing so how do counterbalance the way your counter balance is that you um just to say um you want random images to have different representations so let's erase something here which one I should use here so basically to counterbalance this you the second goal is that you want suppose you say I'm going to have some two random example let's say you have um maybe X and Z so maybe a cat and dog right and then you augment X into X hat as the same way here and then you argument Z to Z hat so because x and z you know are two different images maybe one is cloud on the other stock so the argumentation is probably looks very different as well and the augmentations are semantically very different so then your second goal is to make uh V Theta X hat and V Theta Z hat the augmentations of two different examples are far away so this is the to counter balance to avoid you to collect everything to the same point and actually there's a name for this pair it's X hat Z hat is also called uh either random pair or negative pair so um at the very beginning I think people you call it negative pair which is not exactly right in the sense that x and z are just random choice of two images there's no guarantee that they are exactly they don't have exactly the same they don't have the uh the same class or same kind of like meaning right there is some small chance that both x and z are both from the same class right even you have like a following classes there's still a little bit chance that x and z are from the same class but in most of the cases x and z are from different classes um and and they are semantically different so Random pair might be a little bit more accurate but negative pair is is what people call it at the beginning and I think now people just use this inter-exchangeably so um so basically you want to write random pairs to have different representations and you want positive pairs to have similar representations so um so this is the design principle how do you do this exactly so um there are multiple papers they are kind of like a at least four or five papers that use this kind of like a principle um and sometimes actually uh some papers actually just even drop the number two you just use one and somehow still sometimes it works just because there's some other kind of like counter balance in in a system that can kind of achieve too without explicitly encouraging it but let's now talk about those let's talk about one case where the simplest case we have both one two especially encode it in a loss function which is called Sim clear and this is I think basically the first um first paper that makes this kind of Ideal work um so how do you exactly encode these two kind of principle so it seem clear what people do is the following so what you do is you say you first take a batch of example a random batch of example like in SGD let's call this example X1 up to xB so FB example and then you first do some augmentations so your augment to um X hat one up to X Hat B and X tilde one up to X tilde B as as you can see these two are the augmentations of the first example these two are the augmentations of the second example and these two are the augmentations of the the B's example and then here is the loss function so let me write down the loss function and then I can explain so so the intuition is that you want to design loss functions that basically maybe maybe let's see so oh I don't have a different color today so because these two are augmentation of the same example you want them to be to have similar representations right and maybe the same thing happens for these two right so any any pairs you should have similar applications but if you look at this one and this one suppose you pick something like this then you want them to have different rapid stations and what you do is that you make the loss function the loss function is equal to this this is another button complicated and the first set but it's actually not that hard um so I guess so here the indices is the most important thing so this is I this is I this is I this is I and here this is J sorry my handwriting is a little bit unclear so okay so maybe the first thing to realize is the following so um maybe let's focus on this term maybe let's call this term a and this term B realize that this is also a right so just a little the same term so this is really something like a lot minus log a over a plus b something like this absolutely speaking so and and if you do some simple math you'll see that this one is uh decreasing in a an increasing in b um this is uh relatively easy I guess you can either take a derivative or just at least the increasing in B is pretty easy because this function is this function is decreasing in B and log is a monotone function and you have a minus in front and for a is the the reverse Direction so um that means that if you minimize this log function you are trying to encourage the term a to be smaller because it's decreasing a right sorry you want the term a bigger because the loss function is decreasing a the bigger the a is the smaller the loss function is so you want this a term which is this inner product between actually a term is exponential of inner product but so you want a which is equal to exponential X hat I transpose V Theta X tilde I to be um this U1 is to be big topic right you want this to be big that means that you want this representation of the X hat I and the representation of the X tilde I to be as close to each other as possible you want your inner product to be big right that fulfills our first goal where we want the representation of two examples the augmentation of the the representation of the augmentation of the same example to be as close as possible that's the kind of the first goal and and then you want this B term to be uh to be small right because the function is increasing B and you are minimizing the function right the bigger the function B is the smaller uh oh sorry the the function is increasing B right and you want the function to be small so you want a b to be also small right the smaller the B is the smaller the function is right if you want these two to be small and what are the terms in B basically you have this kind of term foreign where you have I and J here right so basically you want to say that for different when it's the augmentation this is augmentation of the I example this is augmentation of the JS example so their augmentations should have um or should be small so so they're they're in a paradox should be small so that means that the representation should be um far away uh from each other any questions sure also how um exponential is an increasing power if you want to experience yourself actually smaller then it means you want an argument of the exponential the first one is straight so J only shows up once all the others are I so there are other interpretations of this loss function it was um I'm not sure what they are they are easier to understand or be harder to understand than this you know one interpretation of this loss function is that you can view this as a multi-class classification kind of like questions so you are trying to distinguish so basically in some sense you are you can supposed to do a lot of math you can see that this is kind of the same laws as uh as the following hypothetical question so the question is like given this data set I want to say given for example X hat I the I think that the first augmentation of the ielt example I want to distinguish which one is uh is is the positive pair and which one is negative pair so basically let's say suppose you are this is another interpretation which I I personally think is harder to understand um but uh but let me just anyway say it so suppose you have some example x height I and you have the corresponding X tilde I so you can kind of view this else as you want to given this x height I you want to test which one of this these three examples are uh the most correlated with I in some sense uh and I and you want the x height I this one to to stand out in some sense uh compared to the other correlation you want the correlation between these two to be dominating all the others so that you can kind of say this one is my my body in some sense it's my it's the it's the it's the the other example in the pair um yeah so um but anyway you know the the exact form of the loss function is not that important right so there are there are actually other ways to implement this it's not like it's really necessarily have to use this loss function the principle is probably more important than the loss function right so cool so I guess just to summarize this is a loss function that only depends on X right so you didn't use anything about label or self-supervised uh loss function so it sells price because you self-surprise just means you don't use any supervision except some part of your yourself um okay any other questions oh we are fighting in this case we are the specification like because I wish images feature the feature the rapid stations the the function that computes the representation is the feature extractor or the um something like that it makes sense of ultimate blaster that's why I have to transports Okay so [Applause] okay so the next thing I'm going to talk about is how do you do page training when you have language um and there I'm also I'm going I'm going to tell you one method um uh which is also like a self surprised or unsurprised per tuning um but the method is a little bit different so Okay so this is basically discard the very last layer so and use the walls remaining Network to perform like all the possible damage causing the future oh no there's no there's no last layer anymore here right like this field is a is a new network like like this field setup is kind of like uh something right for start from X you know a lot of neurons they draw a lot of your arches something like this right and eventually you also put some embeddings this is free oxide and and that's it that's that's your representation um so you don't have to be scarves but you have to add the last layer when you do the classification drawing is a little bit too flushed cool so Okay so um large language models so the first thing I need to address is that how do I encode the data right so I have for some text where they are discrete words I need to first turn them into numerical numbers um I guess you know if you remember I think uh uh a few weeks back right so we talked about this event uh model right this model that you include data by this very simple binary Vector right every every document is encoded as a as a zero one vector of some size so so those are you know very simple uh so today we're we're gonna do the the more realistic one so you just and but but the encoding becomes easier actually just because um what you do is you just directly encode each Vector um each word uh I I it's just the the display token so okay let me just say this um so so I'm saying that this is um this is a way to encode but the encoding is kind of like a um to some extent much conceptually much simpler so basically the first thing is that let's define what is an example where you have a we have like language so typically I think you know for me the best way to think about think about the example in language is that you think of as as a document or something like that no a sequence of words because you cannot view every word as example then you lose the the examples are supposed to be somewhat kind of like independent with each others right so if you use if you use your word as an example there's too small for granularity so you view each document as example um so that's kind of the mental picture I think of so you have a sequence of word maybe X1 up to XT and and this is a one example in reality what people do is that people don't really kind of like just literally look at which you know the the exam the the documents because what you are given is that you are given for example all the Wikipedia text on on Wikipedia and this is a gigantic file which is really just a sequence of words and everything is concussion together and then what people do is that you just truncate uh you just take a consecutive kind of sequence of like Words as one example maybe you take something like you know maybe 500 Words consecutive 500 words or maybe a thousand words as one example but so but the only case you know um I think you know you know if you don't care about the details you know implementation details it's just fine to think of each example as a kind of a document an important thing is that it's a sequence of words and and there's another small detail in the implementation so um when you really do this you know you sometimes you don't really operate on a granularity of words right for example one possible choice you can operate on the granularity of characters right you view each character as One X I uh people don't really do that people what people do is that people are operating on the the level called tokens and each token typically these days in the best model each token is kind of like a word but sometimes smaller than a word so basically you can think of like most of the common tokens uh oh sorry what most of the common words are a single token like the the top I think I think last time I checked this it's kind of like that the first like 20K frequent words are are all just a single token by themselves but sometimes you have longer words that just never show up very often and then you break them in some way into uh into two tokens right so a very very long word might be two tokens um this is just another small detail just in case you are implementing anything like this but you know for conceptually you just think of each word as a single x i here and you have a sequence of words keywords here that's one example so and then what you do is that you say and if let's say suppose you know you have a vocabulary so each word is uh in some like sex one two V so you have like V possible words um and each talk uh each uh example is a sequence of words and when you say language model basically people always refers to a probabilistic model probabilistic four P of the joint probability in some sense this is kind of like the same kind of like modeling methodology as with what we do with mixture of gaussians right you're modeling The Joint probability of your data of your ex but if you just directly model the probability this joint probability is kind of very difficult because this one has support this one has support size is distribution right so like how many possible sentences you can have here you can have uh support size is something like V to the T because each word can have t v choices and you have t words so this is really exponentially large uh family of possible sentences and and if you model this distribution it's kind of challenging so what people do is that people do is use chain rule so you say this challenge probability can be written as P of X1 times P of X2 given X1 times P of x3 times X1 X2 up to P of x t given X1 up to x t minus 1. and then you model on each of these p x t Little T given X1 up to x t minus 1. you model this conditional probability uh this conditional probability using some parametric form and then you learn that parameters and the the good thing about this conditional probability is that now you only require the one the problem is one word right that's the problem of that one word uh is of size V right so like sorry the the support of this probability right so like the number of choices words here is V so instead of V to the power t okay so nice question is how do you model this how do you build a parameters form for this conditional probability so I'm not going to tell you exactly how you do it if I generally you just do it with your network I'm going to tell you you know there's some kind of details I'm omitting here but roughly speaking what you do is the following so um so first the first thing you do you have to do an embedding followers meaning for every word Acts you embed this maybe for whatever word I this is a word I you invite this word into a vector e i and this Vector e i let's say is in dimension d so every word will correspond to a vector so you're going to have V vectors each Vector corresponds to word and and these vectors will be learned so these are parameters of our system so so so so after so these are the parameters and then um after I have these parameters and what I'm going to do is I'm going to um after I have this embeddings I'm going to put these embeddings into a gigantic image work and let them let the new artwork to output on the conditional probability in some sense so basically roughly speaking here you're gonna have some model which I kind of view as a black box these days people call this you use something called Transformer I'm not going to tell you exactly what Transformer do that's because it's actually pretty complicated and and it's kind of out of the scope of this course um you know if you you can take a look at PayPal or take some other Advanced courses but you know for for the first other concept concept this is a black box right action many many people don't have to up many people who use the Transformer don't have to open up the black box so so that's why I'm only telling you what's the what's the input and output of this black box so but this is a new network this is a kind of like complex Network and the the way to use this black box is the following so you just say I have some sequence of words right X1 up to x t I'm going to first encode them by the word embedding I'm going to have e of X1 up to e of x t so e sub x t so these are vectors and this Transformer takes in the sequence of vectors and then output you a sequence of vectors so so the output let's call them C1 I think let's call it I call it C2 C3 up to CT plus one so um and let's also just uh give a name for this Transformer like let's call this function V Theta so so this function V Theta after given um out of the input embeddings it outputs a sequence of vectors each of these Vector let's say is still of Dimension D even though this Dimension D you know in reality this D might be the different D from this but they are vectors okay so after you get these vectors then uh you can use these vectors to compute the conditional probability that you the the you use these vectors to predict conditional probability so basically you say that I have this let's do this I guess just so now after you got these vectors C1 up to CT so I'm going to use C T to predict P of x t given X1 up to x t minus 1. okay so what I do here is the following so what I want to predict is uh is this thing right so this is actually this probability distribution is about in sometimes it's a vector right it's a vector of Dimension V because to describe this proper distribution you have to describe P of x t is equal to one the first word give me x what up to x t minus one up to P of x t is equal to V given X1 up to x t minus 1. right so to model this probability you have to model kind of predict V numbers and these V numbers is supposed to be sum up to one and how do you do this this is kind of like you know multi-class classification um like uh so what you do is you basically you say I'm gonna have a soft Max of sun uh WT times the vector CT so let me specify here so c t is a vector of Dimension d and WT is additional parameter uh this is a parameter that you will learn so this WT is of dimension um V by D so basically WT times CT will be Dimension V so you have like multiple so for every possible choice you get a vector right and then you apply a soft Max to make them probability so this whole thing will be in dimension V so in some sense this WT times c t is just the logits and then you apply a soft Max to turn them into probability and that's your prediction for this uh for this problem so in some sense you just view each word as a class right you have V classes and how do you predict uh um something with how do you do a classification with v classes what you do is you first use a linear this is a linear hatch on top of the city and then you do a soft Max to turn them into probability um so I think the definition of soft Max I think we probably defined this in one of the early lectures right so let me just Define it again so I should use this problem So Soft Max is just like if you have soft Max Alpha Vector U is really uh something like this exponential U1 over sum of exponential UI and then exponential U so V over sum of exponential UI something like this right so you turn on the logits U into a probability Vector which sum up to one any questions [Music] so I'm going to use C2 to predict X2 oh I'm going to see it you know it's a it's a in you can index in any way I I chose this is to indicate that C2 is to use to is used to predict X2 given as well so and actually there's one thing I didn't tell you which is actually important so when you predict this probability uh this probability right this is my model to predict the probability right I need to insist that I haven't seen other words like I I have to insist that I only have seen X1 up to x t minus one right if I have seen x t already I can just output X the true value of x t I've seen right so um so so actually this Transformer uh there are multiple versions of Transformer the Transformer here I'm talking about you know on the term is called Auto regressive Transformer is this is just a name what I mean is that you you you design this Transformer in a way so that City only depends on X1 up to x t minus one so Define you design this architecture such that you have this property such that the city Only depends on X1 up to x t minus one so that when you predict x t you are not going to be able to see any other words after x t so that's why you have a you have a proper definition of the probabilistic model um but but this is like a just think of this as given the Transformers internal property ensures you to have this right how do you connect all of the neurons by ensures this property inside so this the in many of the dimensions uh I call it d right now um but the dimension is just a parameter you can you can change you can change it to anything of course it has to be some or large something like maybe a third and something like that I understand yeah how do we do we have to create as many uh that's a good question right so if you use one hot Vector probably you should use embedding of Dimension V right because you have V choices but now I think the embedding Dimension is often smaller than a vocabulary size I think the vocabulary people typically use is something on about 40K maybe 60 60k something like that um something on that level um but the dimension the dimension is probably a thousand only Matrix you mean the the mapping from the I to the e i um it's not a it's not uh uh it's learned so so basically you just learn this so so you have E1 up to EV right each word has a vector and you concussing them all optimize The Matrix and you view this as a parameter of your this is part of the parameters of your training okay so now the final step is just that after you already Define the prediction you have to Define loss function to learn the parameters right the parameters are the question was great like the parameters include E1 up to EV also include WT right and and that's it right that's all also the parameters in the Transformer which I didn't specify that's a new network which is viewed as Black Box so and now you learn you take a loss function right which contains all the W's which is a function of W's the Theta and the Eis the E the e and then under what this loss function is just the cross entropy loss of all the positions so cross entropy laws of all precision of position I and and if you really think about what's the cross entropy loss for the for the soft Max for this kind of things is really just this um but but you do necessarily have to really exactly see why is this but it's really just the minus log probability uh of pt and x t um so if you call this thing PT suppose we call this PT then basically the X teeth entry a minus log p x t actually is this is the axis entry of that Vector PT and and this is just the the the the cross-centric Plus but don't worry if you don't got this line it's really just a empirical you just have to when you implement this you just have to call the cross-centric below sync and give it to it um running a little bit late uh any questions nothing I didn't I cheated a little bit so here I'm taking so this definition I can only Define H for T is two in some sense I don't have a production model for X Y I only have the prediction for X2 given X1 X3 given X2 I didn't have a probabilistic model for X1 in priority just people just forgot about it just don't use it you know it doesn't matter that much you know you can try to fix it to make a principle but I don't think it matters that much anyway so just because you you have so many problems like a public probability test model and if you ignore one of them it's not a big deal um Okay so now let me talk about uh how to adapt um how to adapt uh this language model to to Downstream tasks so I I've erased the fine tune and the linear probe but uh if you can still do those those are very general though you can use those function and linear probe for almost anything so and for this kind of like language model the way you do find tuning in probe is just that you on it's it's just the only thing you have to decide is that which output you should use as the representation of this documents so you have so many outputs here and you which one you take is the representation of this sentence so you just take c one option is you take CT as the representation and then you add some hat W and your W transpose this is your prediction model for Downstream tasks or task and then you can choose to only fine tune W or you can choose to fine tune W and the parameters that that you use to compute CT right the parameters used to compute CTR those parameters in the Transformers those Eis those embeddings so so that's easy and and just because it's generic and for language models the interesting thing is that you can also undo this uh with uh um some other ways where you can do adaptation with other ways so one way to do this is so-called the zero shot uh learning and here is it's just very easy so basically whatever task you have you just turn it into some questions or some kind of like closed test um like you have a blank to fill in and then you just give it to the to the model and then the model to generate the next word so basically what I'm saying is that you just say for example suppose you just turn you can have X which is just the you know this model can take in your sequence forward right you just say maybe is the speed of light suppose you care about the speed of light is constantinal note and you just turn it into a question right you call this you call this um this is X1 X2 X3 up to XT and then because this model can generate maybe let's say this is x t minus one and then you use this model to generate x t because this model can do conditional generation right you can given X1 up to x t minus one you can generate x t and give maximum after x t you can generate x t plus one you can just keep generating the uh the tokens you know afterwards and you just put this into the model and let it generate and if you generate the next word is no or yes then you get the answer and maybe sometimes it generates something slightly different from just yes or no then you have to pass the answer in some way but you just write the model to generate the answer that's not that's that's one way and so basically you are using the generation power of this model that's probably the important thing right because giving this mod so basically here the important thing is that the way you have this model makes it that given some sequence of words you can generate the next token and then you can feedback this new token to the to the system to generate another one and you can keep generous so so so so so that's why this gives us opportunities to have other adaptation method which based on generation and another kind of like a even more intriguing way to to uh to adapt is the following is called in context learning so the in context learning so here you are dealing with you know a so-called fuel shot cases so you have a few examples so suppose you um and let me just give an example to there are so many flexibilities I'll just show one example so what you do is you just concatenate all your examples into a so-called prompt concatenate examples into uh in something into into actually in the language of this lecture we call it document right but but in the if you look the paper is called prompt so what I mean it just means you you can correct all of this into a sequence uh for example suppose you care about you know learning how to add up two numbers suppose you have a question Q which is something two three is what and then you do have a you do have a you have some examples right you say you know that this is five so this is your X task one and this is your white task one so you're just concaten them into a into a sequence of tokens so like this and then you just keep concatenating and you say Q six plus seven no no plus I choose do not use plus because I want to make it difficult right so something like this you're trying to learn what this symbol means right so and then this is X task 2 and Y and then you're Contracting the Y times two so you say answer is 13. and now suppose you just have two examples and you want to learn and now your question is 15 this symbol 2 is equals to what so you can cut in all of this together and you call this X1 up to x t and you give this X1 after x t the sequence of all the symbols to the model and let it generate and then you ask this sequence to generate our XT plus one so use this to generate x t plus one right or maybe even XT plus two so and so forth and and it turns out that if you give these things to the model and they will generate something reasonable for you so we will generate something for example for this case it will generate a 17. and then you got answer the answer is 70. so and you see that this is sometimes says that you learn the downstream tasks you'll learn that this symbol means addition from this um I'm not sure whether this is a little abstract but I think what I can do is the following so I think I'm this is about time and we can stop um like we can stop the class and but I can show you some uh uh some examples you know just the live"
"Stanford CS229 I Basic concepts in RL, Value iteration, Policy iteration I 2022 I Lecture 17","Stanford CS229 I Basic concepts in RL, Value iteration, Policy iteration I 2022 I Lecture 17

so I guess um let's get started um from today um we're going to talk about um just in two lectures on the topic reinforcement learning so reinforcement learning is um a pretty important sub area of machine learning but it does have a slightly different flavor um we're not going to spend a lot of time we only have just because this course has covered a lot of topics we are only going to touch on on some very basic concepts of reinforcement learning this lecture and and the the lecture the two lectures after next lecture we're going to have a guest lecture on the kind of the the broader impact of machine learning like robustness you know societal impacts and so forth and then we're gonna have the last lecture which is also going to be on reinforced learning so so this lecture is here just mostly because you know I think you need this lecture for solving well homework questions and homework four um this lecture will give you the basic concepts so that you can solve that homework question so um okay so reinforcement learning so so I think reinforcement is kind of like at least on the surface it's very different it sounds like very different from some of the other machine learning problems because there are a bunch of like a different different things so maybe just to give you a rough sense of what kind of questions we are trying to solve today so maybe the running example you can have is that you have a some kind of you're trying to let a robot learn how to navigate a certain part of like some space right so you want to control the robot to do something so so we are trying to solve this kind of like controlling tasks um so maybe you want a robot to kind of pick up some object or maybe on the robot to go to some place so and so forth so um so there are a bunch of differences between RL and super external and surprise learning so the difference is you know so so here are some differences I'm not trying to be very comprehensive and also there are intersections right so there are certain sub areas of RL which is more similar to surprise learning so here I'm just only going to you know talk about some high level differences so the first of the the first of all Ro is about sequential decision making so sequential so I guess there are two things that you need to pay attention here so the first thing is that this is about decision making so before when you talk about supervised learning you are talking about prediction right you're predicting the house price you are predicting whether um somebody has you know cancer or not you're predicting you know some wise some Target right so but here you are not really just only about prediction of what will happen in this world you also this is also about you know what you should what kind of decisions you make sometimes these decisions are made after you make the prediction right maybe before after you know that this person might have cancer then maybe you need to kind of give this person some treatment right so that's the different part where you have to make decisions based on the prediction sometimes you don't you don't predict you just make decisions you know directly that's also possible but at the end of that you are making decisions and second this is sequential decision making um so your decisions have a long-term impact right so so especially maybe you think about controlling your robot right so if you let the robot go forward at this step then and next step like a your your um uh let's do about you know the set the uh the configuration or kind of the robot will move right so so right so this the the the decision at this step does affect your decision at the next step you know think about for example treating a patients right so if you give the patient you know some kind of pill today then maybe the the patient becomes better and next day then you have to change your strategy or at least you know the the decision you made yesterday probably would affect the decision you make today and then which would also affect the decision you make tomorrow so so I think that's the two important thing about RL and you'll see that this is a kind of like why uh this is uh challenging because you have you have to consider the long-term kind of consequences our long-term kind of like consequences of a decision it's not like you can just say I just great delay choose a decision based on the current situation choose a decision that can make the next day the best right so maybe you can make a decision that makes the next day you know very good but then the day after tomorrow um you know you got into some weird situation right so so this is you know you think about you know your life decisions sometimes you shoot for short-term rewards you do something but then you miss out for example long-term investments in some other opportunities right so so greedy approach sometimes you know doesn't work very well and in many cases we just purely greedy approach doesn't work um and there are some other kind of like um um kind of things about you know another thing about decision making is the following so when you make decisions right so decisions can give you multiple on on benefits right so one thing is that you you are required to make a decision because that's the problem formulation right when you control the robot you have to make a decision how do you control it right you want to turn return left or turn right but another thing about decision is that when you decide uh what you do today you also collect information from the environment about you know what the environment will look like so so decisions also affect you in terms of the information right so you can make decisions to kind of query in sometimes the decisions part of the job is to query the environment so you collect more information maybe let's take the patient treating patient uh example as another example right so maybe your decision could be that I'm going to measure the patient in terms of certain kind of measurement right that's another decision and that decision doesn't really treat the patients but it does collect information for you so that you can treat the patients you know later better so so decisions also give you additional information so sometimes you have to think about you know what our decision uh um you know you have to trade off the different kind of like effects of decisions where sometimes the deficiency the decisions just you know directly give you some reward but sometimes the decisions give you reward uh it doesn't give you a reward but give you information so that you can use that information to um to get reward in the future so um so I use the word reward which you know I haven't really defined so that's another actual difference between this and surprise learning so here there's no supervision so um you super no um like I know our latest supervision so so what does it mean so for example if you think about controlling a robot right so one supervision could be that someone some human is telling you that how should you control the robot by some expert knows that how you can play with this robot but um the kind of questions we are solving here are those questions where you sometimes just don't even know what's the right way to control the robot where for example suppose you want to fly a helicopter right so um like a like a you know if you if you really fly a real helicopter you know there's some personal some training rights you have to be trained to be a uh to be somebody who can fly the helicopter but suppose you are developing a new helicopter that can fly automatically then you are trying to figure out what's the the decisions you have to make right so um or maybe like I think about treating a patient you have to find out the sequence of treatment you should give the patients right it's not like somebody already knows it if somebody already knows it then probably the problem is already easy right you should just use that expert you know on policy but but sometimes you are trying to figure out the best way uh to uh to make decisions so so that's why there's no uh or very little uh supervision um so um so sometimes you are not really trying to predict what's the best prediction you actually figure out a best prediction by kind of like interacting with the environment by some trials and errors um just because humans sometimes don't know the optimal decisions either um and but if you don't have any supervision how do you figure out what's the best decision so so the kind of like the way to deal with is that you have um you learn not by you know imitating some human kind of like supervisions but you are trying to learn uh from some reward function so what does it mean is that um so so humans specify the reward function so the humans specify what you want the robot to do right so if the robot pick up this object that means success then that's a reward function that means reward is high and if the robots fails to do that the reward is low so we specify the reward function and then you let the machine learning algorithm to figure out how to maximize reward function so um that's the that's not another kind of like ice pack here you know these are not completely kind of mutually exclusive so I've kind of mentioned this right so another kind of interesting feature is that you you collect more data interactively I think this is pretty much Echo what I've mentioned before the decisions also give you more data where you can make a decision to query some information or even you don't make decisions deliberately to query information they still give you additional information once you make the decision so so so because it's sequential right so you do you make some decisions and then you get some data and this data could help you in the next round to make better decisions foreign of course you know there are many different variants of the reinforcement setting where sometimes you know you can assume you have more inspiration from some experts you know can give you a demonstration um and sometimes you don't have reward function you know sometimes you know uh you cannot have interact interactive collection of the data and there are a lot of different variants right if so basically you can add the adjective before reinforcement to have um to to have a virus so you can say offline reference planning that means you don't have super you have more supervision but don't have data interaction you can have other kind of like combinations but this is the the main kind of set of features of reinforcement any questions so far I know this is very abstract but yeah little supervision Point again yeah um so basically um you don't so so what could be a supervision here because you are trying to make prediction or make decisions right one plus most information would be that you like that the expert an expert who knows how to solve the task to demonstrate how to solve the task right so for example suppose you want to control the robot and you say okay maybe you knows how to control these robots and then you just demonstrate to me how do you make the robot to solve the task and that would be your supervision right but in many cases you don't need to have this supervision sometimes you don't have the supervision because the humans don't know what are the best way to control the real box and sometimes you have the supervision but you don't need them or sometimes you have the supervision but it's kind of hard to collect supervision because you have to find an expert so um so that's the basic idea actually but but just to be fair you know recently I think in the last two or three years people are moving towards more and more like human uh supervision because it turns out that um there's trade off right if you don't use supervision you don't use humans probation that sounds great because you don't need humans to demonstrate for you but um you need to collect more data and do have more trials and errors you have to try out different kind of like ways to control your robot and see which ones succeed and which one fails and then learn from that um but as opposed to humans tell you something then you don't have to try that many times you can let the robot fall down less often right maybe without even trying on the robot you still already know how to control the robot just because you learn from the humans so so I think in the last few years I think people are moving towards you know using more and more humans foundations and sometimes you are using like imperfect human supervisions where I use all you like all you use kind of like sometimes they don't have the expert information sometimes it could be just some data from the passed away so you have seen the robots you know going around doing some tasks imperfectly in the past and use those data as supervision on to to learn some something better uh than the past data that's even possible um but we're not going to go into all of these details right so for this lecture there is no supervision just you know nothing about you know what's the right decision um but you're going to figure out the decision by trying uh uh try different strategies so basically all the algorithms will look look like this some more kind of like trials and arrows right so you start with a robot you don't know how to control it but you don't know how to use it to solve any tasks and then you try different kind of like decisions actions and then some actions you know it just happens to pick up objects for example and some other actions just happens to fail and then you try to use those actions that uh succeed uh and you know whether you succeed or fail because that's your reward function right if you succeed you have better reward and if you fail you have lower work so so you know whether the excess succeeds and then you try to kind of like a boost the chance to uh your algorithm tries to kind of like amplify or boost the chance to take those actions that can succeed in the past and you kind of do this bootstrap thing and you and and eventually you find one set of actions or one set of policies that just always succeed with you know with very high probability and that's that's how I would roughly speaking uh how it works so um okay so so basically everything is is through this reward function right so the reward function is kind of the the signal you rely on to to learn what is good and what is bad any other questions to Performance regarding the reward function so the reward is is something to collect but you can also see something more we can see for example when you manipulate the robotic arm you can see how they are where the arm moves towards right so you can also observe other things I'll formalize that as well but roughly speaking you can observe yeah basically like for example if you treat a patient you can observe something about how the patients you know behaves or or like a performance and if you if you tune a robot then you can see the how the robot kind of moves right those are additional information you can collect and sometimes this information is is viral kind of like a pixel kind of like you can collect the information as well different way sometimes it's from the camera sometimes it's from the internal like recording of the of the system sometimes it's from something else thank you yeah you can have much more reward functions in many cases but then you have to say you decide which one is more important you know how do you balance them right so um so in our lecture we are going to we are going to just have one reward function and sometimes you can also have constraints for example you can say you have one reward and a bunch of constraints which you have to satisfy um that's also a valid setup okay I think I probably have seen I have said to a lot about the high level idea um you know which is probably a little bit hard to map to the the real thing so uh so so let me try to Define uh mathematically how how does this work and I'm going to use this running example a very very trivial running example so this one example is that you are trying to control a robot navigating a 1D tape so suppose you have a tape which is like this and you have a robot which you know it says ASMR and I can move you know to the left or to the right you know you can you can take some action right to move it left or right and maybe there's a goal somewhere else maybe this is the goal so you basically want to move this robot to the goal which is kind of trivial if your human knows this task you know the person would be able to move it you just keep going moving to the goal right that's easy so um but we are gonna let the uh the algorithm to figure out what's the right uh the strategy so um and so this is my um write an example and I'm going to formulate you know on the set of problems so um this is the formulation is often called Markov decision process so let me Define a bunch of quantity uh kind of terminologies so there is something called state as and you can have a set of states and that's denoted by capitalize so so for this example a state is a state basis is the situation that the robot is in right so a stage basically is suppose you have like a say one two three four five seven six actually maybe let me just change the goal to be here to be consistent with my note maybe I say 10. so a state basically is describing what's the the current situation of the the robot right so so here the state is probably three because the robot is here right so and here you only need 10 numbers 10 there are only 10 states so that's the only thing you care about right so but if you really have a real robot maybe you have to describe the steel box in many other uh parameters or many other kind of like uh um like numerical kind of numbers right for example you may describe the robot speed the velocity the height the the center of minus one and so forth right then you can have a high dimensional state right so like you have a lot of numbers to describe the state of the um the robot and the family of the the states will so the basic state in that case will be of a high dimensional vector and the family of states will be just all the high dimension vectors that can describe the robot um so Fox you know maybe another example would be that you know if you think about you know playing goal in the state would be the the current board right so where the all the kind of like um how the how does the bar look like and then you have a lot of States because there there's like a 361 entries on the board and every entry you can have a white and black you know and nothing you know you have three choices then you have three to the power of 361 uh possible States so um so that's the this is the the concept of state and there is a concept called action sometimes typical we use a for action and and there's a site of action this is the set of actions you can you can take which is called a so so in this case basically these are all possible kind of things decisions you can make right so and it's called action technically in in this language so for example here maybe like I'll just allow the you you take two actions left and right you can just move left or move right um and when you control a real robot probably you can take other accents you can change the you can accelerate or de-accelerate or maybe you can change the um the force you know at different drones you know many other things right so so and um and if you play Gold then there are actions to really just um put something on the board right um okay so um okay so this is the set of actions and then I need something called Dynamics or transitions to describe how does the actions influence the state so this is called Dynamics and I think also sometimes it's called transition or transition probabilities so so or sometimes it's called State transition probabilities um so basically this is the um so basically you're asking about a question so I guess maybe let's define annotations this PSA this is the you're asking the question when applying when applying action a I to state as you know uh the probability distribution you're asking where I should arrive at next time so the probability distribution probability problem just a distribution of the next state and the next step often just notation wise it's often called S Prime so basically you're asking so in other words you know p as a z is the probability of X Prime is equal to Z given as an a so PSA is the probability distribution and this proper distribution is the conditional probability distribution conditional you are at currently UI State s and you play action a and you ask yourself you know what's the distribution of the possible next step and there's some Randomness here right so uh you know for robots you know sometimes you can think of as deterministic because you you do something and deterministic deterministically it moves to some other place but for many other kind of environments you know you take some action but the how does the the environment how does the the state changes you know is probabilistic there's some Randomness involved so so the randomness is trying to capture on that so so that's why in this sense you know the this um so for every ice and a it's PSA is a is a distribution so PSA if you write PSAs this you know the the probability of every state right so um suppose maybe let's say suppose you have a state s which is equals to you know one two something like this right so then um if you you can also view this as a vector you say this is PSA one this is the chance to arrive at the state one uh and this is the pic at this the last date maybe let's maybe I'll just uh I think yeah maybe maybe I'll just sorry maybe let's just uh I think I realized that maybe let's say this say m is the number of states so my my state is just to have like a m State one two three four up to n then you can list the probability to arrive at each of the states and write as a vector this is a vector that is in RN and this is the probability Vector so this this Vector itself you know all the uh the entries are some sum up to one of course you know you can also have deterministic transition Dynamics right so you can say that only one of these numbers is one and all the others are zero and there's a deterministic transition dynamics that just means that you just you always translate to the same state given the same action uh and state and same state and actually you always translate to the same next state [Applause] Okay so I think I'm gonna have an example um based on this uh robot thing um just to give your concrete uh idea so suppose you say you have this action set which is you know IO and r and this means that you push the robot you know so the left or to the right but let's say suppose you know there's some Randomness in this environment you even try to push it it doesn't necessarily always move which is only a a good chance it moves with some chance you just fail to make it move right so um so maybe then maybe let's say suppose you know so L action says say succeeds with probability let's say 0.9 then that means that if you look at or if you use this notation it means that suppose you have you had P suppose your item P um say you're either State seven and you apply the action L right so PSAs is the state and L is the action so so then you ask you know what's the chance to arrive at some other state right you write seven and you you try to move it left but you know that it only succeeds with probability 0.9 that means that with 0.9 you're going to arrive at six so this will be point nine and uh with you know 0.1 chance you're gonna arrive at seven you stayed at seven so that's point one and with zero chance you're gonna arrive at any other places for example you are not arrive at five with zero chance and you're gonna wrap other places with zero chance so that's the that's one example of the the transition Dynamics and then you can write down the transition Dynamics for other so the action maybe just a for example suppose you say P seven R so now you're asking I stay seven if I try to move it to the right um you know where it should arrive at you know we know that it should arrive at 8 with some chance with trans points say point I guess so in my example I make it a little bit complicated just to so let's say R say R actually succeed I'm just making this up just to make it interesting with probability 0.8 then it's saying that you know with probably 0.8 you're going to arrive at eight the answer eight so you get 0.8 and with probability um 0.2 you are going to stay at seven and with probability um zero you're gonna wrap at any other states so that's mean p7 um are maybe five will be zero and all the other p7 r z will be zero if Z is not equals to seven and eight and you can Define you know like a whatever transitions you know you want you know so so you can you can write you know using the same ways you can write out all the transition probabilities uh for every state action pair okay so that's my uh that's my kind of description of the environment or by the way by environment people generally refers to this kind of entire system right so how does the um like how does things change based on your action right the environment basically just means this entire system um um okay so now I have defined uh the transition probability and now I have to talk about sequential decisions right so so far I only talk about one action well how does one action affect the system so we have sequential decisions then what happens is that uh your interacting with the environment or the system um so like like the following so first of all you you say I'm going to have an initial State as zero which is the initial state and uh and let's say the initial state is given but sometimes you can also say the initial state is randomly drawn from from some distribution okay and then so what you do is that the algorithm and chooses some action a0 from the set action set so and then after the algorithm choose the action the decision or action the environment uh oh step is that you you basically sample as one from this distribution right so you basically translate your state based on this rule that we have described so you say S1 is going to be sampled from ps0 a0 right ps80 is basically you know if you apply a0 on I zero what's the chance on what's the probability to arrive at new States and you sample uh one state one concrete state from this probability distribution and then the algorithm chooses A1 and in this action I but here A1 can depend on what so A1 can depend on S1 and s0 so so ice one is considered as something that is given to the algorithm right so you observe as one after you played action You observe more information and the more information is as well and then you can play your new A1 based on S1 and I zero and then you just keep doing this right so the next round you just say I'm going to continue I'm going to say the S2 is generated from PS1 A1 and then algorithm picks are a A2 and A2 can depend on all the history right all the historical observations you have seen so um so that's the idea okay so for example you know if you really think about this you know this thing maybe you start from three and you apply you start from the state three and then you can apply R action and then with some chance you know you're gonna move arrive at four and then it's supposed to arrive at four then you can say I'm going to decide again what my action should be and you say okay my action should still be R and then I I I can observe by there it does move right so with some chance it will move and and you just keep doing this foreign that's kind of describing the decision process and there's one thing we haven't described which is the reward right so how do we decide when you succeed or not uh eventually right so um so the reward function um there's so something called reward functions so the reward function is a function that Maps uh is a function that Maps uh from Maps the state uh this family of states the state of states to real number um so basically sometimes you write it as RS right as is a state and you apply the reward function you get RS in some other cases you can also have reward function that depends on the action uh sometimes it can depend on the next state but let's follow for the purpose of this course let's say the reward function only depends on State so um so basically you know for example for this you know robot case maybe you can say um you know suppose you want to somehow have a reward function that characterized whether you achieve the goal State six maybe you can just Define your word function to be R6 to be 1.0 so so achieving sex you know give you a high reward and then you save R of s is same you know I'm making this up it doesn't really matter exactly but you say your water is very small or even active if your state is not six so suppose you'll Define a reward like this in some sense you are encouraging uh the algorithm to reach the the algorithm to reach text and not reach any other state because for other states you get Negative reward and for the six you get positive reward Okay so and but this is only a reward for one step [Music] more expensive expensive as if you take a certain model is less expensive yeah yeah you can do that so basically yeah that's what I said so your reward function can be a function of s and a as well or in many other cases but for the simplest let's just say the reward is a state is it um the mass doesn't change much it's almost almost the same yeah and sometimes actually depends on the state and action and the next state so you can depend on okay cool and this is only about one step um eventually you have to care about the the sequential decision making it's just not about one step right so the total payoff is defined to be sometimes it's got Total return um so this is defined to be you know rs0 plus r is one so and so forth um Plus rst so so this is the total reward basically just to sum up all the rewards at all the steps and and now here I didn't specify you know how many steps we can have right so if you have even number of steps then this doesn't seems to make a lot of sense because you know your reward will be sometimes going up to going to Infinity right if you have infinite number steps and each step you get a reward something like one then you're worth eventually will be Infinity um and it becomes not very informative because you cannot compare one Infinity with another Infinity so um and there are two ways to deal with this so one way to deal with it is that you say you have a discounted reward and but but you have infinite how is it by the way Horizon means how many steps you're going to play uh in this sequential game so and so basically on the discounted reward is the following so you have rs0 plus some gamma rs1 plus some gamma Square R is two so and so forth and where this comma is less than one and larger than zero is so the so-called discount Factor count vector and here you have even a horizon basically just means that you take sum to Infinity this is T but here I still have I don't know like a you can feel awake so I'm taking this like in this case all right so you know this is just a demonstration you know this is not that I just the total payoff doesn't really make a lot of sense if you really have if you just do this but you have infinite Horizon it doesn't really make out that's it's because it's going to be Infinity so this is the real definition we have infinite Horizon so you have discounted reward and the reason why you have to only have this discounted reward is that um so I guess there are several ways to think about this so one thing is that you can think of this as um um a kind of Interest kind of like it's kind of like you know in the finance where you like the the return you get the reward you get in the future doesn't work as much as as the reward you got in right now because there's a there is an interest rate or inflation rate you know something like that so so basically you say that you discount what you get in the future by a little bit exponentially right if you get some reward after tea times you're gonna have so basically here the t's term would be like this your rewards uh in the in the future by uh by some Factor governments to the poverty just like you know like in financial economics you know you you're returning the future this doesn't work as much so that's one way to think about this and another technical way to think about this is that um um if you do this then you um even you have infinite Horizon then your total reward is always bounded so so so suppose you have wait did I oh that's from last week so basically suppose your gamma is less than one and bigger than zero oh and suppose each stat a reward is bounded by minus m and n suppose you have this two then you know that your total your discounted reward discounted payoff this concrete pay off this uh is you know at the most say one plus Gamma or I'm sorry I'm the first time you can only get n and the second time you get n times gamma and the third time you get M times gamma Square so and so forth right and and this series you know will converge uh so so the total sum will be something like n over one minutes comma so basically you guarantee that at least you have a bounded return and this is the maximum return you can get um in the in the best case so so technically this make it possible for two reason about infinite Horizon because uh your return is always kind of at least bounded there's always a number uh uh kind of meaningful number for the return you have some questions I saw some oh no question okay um Okay so right um so in the if you look at the RL paper you know literature sometimes also people talk about fun and Horizon which means that you just have a hardcore thoughts for how long you're gonna play this right so that's another way to formalize the problem which we are not going to talk about in this lecture but I'm just gonna tell you that the existence of such definition so if you have final Horizon you just say you have a horizon which is called T this is basically saying that you just have to stop it start you just have to stop at t-step and then your reward will just be you don't have to have discount factors you just say I'm going to I adopt the first two steps so also this final Horizon thing on an infinite Horizon you know they don't have you know fundamental differences you know from a technical point of view if you know how to solve one you know you basically know how to solve the other of course there are some kind of dependencies if you really care about a theory um um so here you know your um Oreo kind of like dependencies depends on gamma and here your dependencies will be on T but fundamentally they don't really matter that much but the infinite Horizon case is a little bit easier to understand uh in terms of the at least for the beginning you know if you just derive the mass the mass is cleaner so that's why we do the infinite Horizon case and by the way the gamma typically empirical people do use gamma do use the discount factor and Gamma is something like probably 0.99 and sometimes in green case people even do 0.999 so so if you're in this regime in some sense the way you think about it is the following so when God says let's say gamma is 0.99 what does this really mean it means that you have to really take a t a power that is kind of on all over 100 to make this government to the priority to be somewhere different from what right so basically 199 to the power 10 this is still pretty close to one this is probably like I think this is what how to do the mask here right if you really care about it so so it's kind of like one minus Epsilon y to the power T this is close to one minus Epsilon t or something like this at least for the foreign T is small so so if you really do it uh so if you do the calculator calculation so this to the power to the 10 this is still close to one this is probably like point nine but only if you lose the power to higher so you reach the power to 100 then you're gonna have this I think this is one over e something like that it's like point to about yeah exactly so so basically this is saying that the the power has to be large enough so that you this discount Factor starts to matter right when the power is only 10 it doesn't really matter and and what exactly this power has to be I think if you if you do some colorful masks I think what happens is that um sorry I'm just reusing this part so one minus so gamma to the power of one over one minus gamma this is something like a one very roughly speaking so only your power becomes one minus more over gamma then uh you start to see the effect of the discount factor and after that it indicates pretty fast so so in some sense you know if you really want to kind of have a way to Transit between final Horizons and infinite Horizons then basically this one minus 1 over gamma it's kind of like your effective Horizon length in some sense because after when T is much much bigger than this one minus more over gamma then the discount Factor just start to be super small so then you don't even have to care about those kind of steps foreign I Define this mdp so this whole thing is called imadp so this whole formulation is called the mdp markup decision process and you can see that this Markov decision processes are defined by a bunch of kind of like Concepts so one thing is the set of States another side is another thing is a set of action and the set of transition probabilities PSA as in as a and a right so and you'll have a discount factor and you have a reward function so basically after you specify these six things or five things then you specify MVP and there's a well-formal problem and the goal of the mdp is that maybe I'll just write a go here even though it's pretty so the goal here is to maximize the so basically you want to find out a way to maximize the maximize uh the discounted payoff let's say so basically given mdp you want to figure out how to maximize this discounted payoff the payoff I think means the sum of the reward discounted sum of the reward and reward basically means one uh one step I think people sometimes also call the the sum the summation version sometimes you call it return and occasionally also people just call you reward um but I think I I like to kind of have a differentiation of the terms just to make it not too confusing okay any questions about the formulation so so far we didn't really do any derivations right so just to clarify so this all of these are definitions like how do this it's kind of like a word view how do you view this world in some sense like what's the what's the important Concepts and what are the goals [Applause] okay I'm running a little bit slow um but it's fun um [Applause] okay so now the next question is how do we um how do we solve this problem right how do we find out the best action and for the um for starters we're going to assume that this PSA are given so you know the transition probability you are just trying to find out what's the best action you should take to maximize the reward but in reality you don't know necessarily know the PSA you don't know the transition problem you have to somehow learn them from from observations so I think you know I think for this course you know we don't really talk too much about how to learn the PSAs so we in some sense you can mostly assume that the PSA they are given and the only questions to figure out what's the best action to take so um the first thing to realize is that you know there's the so-called Markov property so I didn't emphasize that but let me do that right now so the Markov property means that when you take the environment you advance the the state by change the state it only look at the previous action on the previous statement so the environment how does the environment change the state only depends on the previous state and the previous action right that's kind of described in this PSA on this framework here the PSA the next state only depends on previous state and previous action so in some sense your state there is a mark of property right so you only have to look at the previous state to decide what's next state um you don't have to look at the entire history so because of this Mark of property it means that um it means that you know you um you only have to uh when you make the decisions you only have to look at the immediate State the current state so the the optimum decision I time t often maybe that's called action just to be consistent with the terminology actually I type T only depends on the state iced tea because anyway like whatever you do is like basically after you CST you can forget about the history because the history doesn't really matter you know conditional St rather history is independent with the future conditional St after you CST you know everything about the current configuration so so you'll have to use the St to predict to to make decisions um and uh to maximize your reward so basically because of this um there's this concept called policy so policy is a function that takes in a state and output action so this so I guess technically I'm going to say this is knifing state to actions right so it's action is equals to it's kind of like action is equals to the policy applied on a state so basically you only have to look for policies instead of looking for entire trajectory of actions because either way you what the the way you make decisions is that you look at the immediate current state and then you apply some function um um on the state to to get your action right so so basically unknown thing becomes a policy instead of a sequence of states does it make some sense that's a great question so so the question was that you know whether you do this sounds like it's greedy right so um it's not in the following sense so when you decide what policy you'll use you do have to think about the long-term ramification of Your Action but it's only that but the action doesn't depend on his the I think this is more about the you don't have to care about the history so you you don't have to care about the the previous history you just heard about what currently you know conditional today right so what happens today is all that matters but I can't forget about what happens you know I don't care about how do I arrive at this situation right so so um in some sense like uh how how do I say like like for example if you have a robot right so if the robot already kind of dropped the drop the bottle you don't care about you know how the robot drop the bottle you just care about the bottle noise on the ground I have to go pick it up right so so so this is more about for guys in the history and when you but when you decide the policy you do have to think about the future you see that when we optimize the policy when you find the best policy we do think about the future a lot um yeah I'll get back to that as well yeah cool okay so this is the first thing so you only have to carry find a policy but this policy is not something it's trivial to find it right because these policy is a function right so you have to figure out what's the best action for every state so basically for every state you have to figure out the best action and that will give you the so-called policy okay so um maybe one way to think about it is that here suppose you want to move your robot to the goal then the policy problem would be that if the state is on the left of the goal your policy should be taking the right action and if the stage is here you should take the the left action so and by the way the policy can also be randomized here I'm looking at a deterministic action right so you say you have actually you have a state you you output a single action for every state you have a single action that's called your optimal policy um but in many cases the policy could be a randomized function or it could be something like conditional State as you can output the description of actions and you choose randomly from them for the purpose of this course I think we are not going to have at least for this lecture I'm not going to have random policy for the next lecture I think I'm going to talk about randomized policy um Okay so okay so now I'm going to um okay so how do I find a policy right so this sounds a little easier than finding a sequence of actions but how do I do that so um let me introduce another notion which is called the value function so let's say you have on this is a value function V Pi this is a value function of a policy this is a function that Maps the state uh to a real number R in some sense this is trying to capture the value of the state so under this policy pie so basically this is equals to so V times s is defined to be inverse the total payoff uh obtained of say executing policy pi starting from State s so basically you think you start from State s and you keep executing your policy pi just every time iteratively every time you see new state you apply a policy pi and then you collect some total payoff total discounted payoff I always have discounts you know just for the rest of lecture so I I compute the total payoff and I call that the the value of the state s right this is demonstrating how good this state as is under this policy pie because if the state s is good then you have it's a property of the both pie and ice right so if the policy is good you get better payoff if the ice is good if this is probably at a goal then you probably get better payoff because you don't have to move anything right so uh this is just the question about the schools here um what producing is that that policy that you take an action yeah I think that's a good question maybe maybe let me justify it yeah this is just the uh this is just the intuition so far um so so what does this really mean is that you so you start with us as zero is equals glass a0 is equals to Pi of s0 and then you say uh X1 is equals to uh is sampled from p i zero a0 right so you start with us and then you take action a or comes to the policy pi and then you say the environment take a step the environments draw the next stage given Ico and a0 and then you play A1 according to the observation as one pi over S1 so and so forth you played this game and then you say you look at the reward that you accumulate throughout this process and until Infinity and and you say you take this expectation the condition as zero is equals to s uh and this is definition of the value function so value function is the this is the expected total payoff expected discounted total payoff of this game right the game is you know the the process is that you start with us and then you play this policy and the environment does what it should do and you always play this policy so that's the value of this data [Music] um like all the future states are selected the environment based on as new so S1 is selected by s0 and a0 and as two is selected by you know if you have maybe let's just continue here I'm sampling as well from I'm sampling from the environment right so so if the environment is deterministic I'm just going to talk it's just fixed the environment decides it right if they if this is the environment is random then you sample one from it but eventually you take average over all the possible so basically simulate your world you know with all the possible possibilities but of course each possibility has different you know like each possible future has different probabilities some more likely to show up some less likely to show up and then you look at the rewards for every possible reward of the future and then you take average over all the possible results so basically just you apply this policy and you you you try this policy on in the real world and and you you can in the real world is starcast take something randomly happen and uh but you just try this you know out and you collect all the reward and you take expectation of the reward the total rewards this is a definition I'm trying to give you a you know how do you really do it that's a different question this is the the definition of the concept Okay so so why I'm defining this I'm defining this because there are two reasons to Define this well one reason that this is kind of capturing the the the value of the state as right so if the state s is good that means that this state is a good initial state if you start with this state you can accumulate more rewards and also this describes you know the kind of the the the goodness or the the the the the quality of the policy if the policy is good then this value would be high right if the policy is just keeping the rising you get more and more reward so um so in some sense you can say um you know you can say that actually your problem is really just trying to figure out what's the the right pie that can optimize your value function so you can reforming a problem before we are trying to find the sequence of action to maximize the reward right so now I think we can just change our perspective saying that we are trying to find out the the policy such that I can optimize my value function so basically your new goal [Applause] is that you are maximizing over all policies you maximize this V pi as zero so I'm here I'm assuming a zero is deterministic is given so um okay maybe I should just yeah so so suppose you are given some s0 right so that the question is that s0 is given and you are just basically maximizing you're trying to find out the best policies such that the value of s0 is maximized under this policy okay so um right so basically in some sense if you can see if you can find out what this function is V Pi zero for every part if you can know for every policy suppose you can figure out this vat number then you can just enumerate overall policies and see which one uh has the uh has the best best return Total return right so of course that might not be our efficient algorithm but the conceptually that's what you're trying to do right so you're trying to figure out what's the reward for every policy and then you try to pick the best policy so so so basically computing this V pi this Computing this is um is uh this is called um sometimes called policy policy evaluation you are evaluating how good this policy is and once you know how to do the policy evaluation then you can try to do the policy maximization right to maximize the power over the policy so so first thing I'm sorry is how do you do the policy evaluation so how do you do it it turns out that basically you just have to do a recursion so so to do policy evaluation you just do some recursion so what does that mean so I guess suppose they think about the policy I State s right so let's just use the definition the definition is that basically you start from State s and then right the definition is that you say this is expectation uh of this you know the total payoff discount appeal assuming that you start with State as 0 is equal to s and because s0 is equal to S right so you just say this is equals to RS R Us this is the reward that you get in the first step and then you say you have some discount Factor is you pull one gamma off because all the all the other terms has worked on there and then you say this is rs1 Plus gamma R is 2. plus gamma Square R is three so and so forth no that's the I put one gamma out right so before it was gamma square rs2 but now it becomes only one gamma right and if you look at the rest of this term this is something that that is actually uh something you have we have kind of like this is also meaningful in a sense that this term is really just the total payoff uh obtained from starting at as one right basically this is just that you you start with S1 and you apply this policy iteratively and what's the payoff you should get right without the gamma nagama is a factor right so without a gamma right this this is basically the total payoff if if you start with S1 right so so that means that this quantity is really literally just a v pi ice once so that means that you got a recursion in some sense between V and V Pi between vs and VS1 so maybe just a more formally so I'm I can write this as this so I think maybe technically I should say that without expectation this part is equal to okay I guess you know let me not to be too technical here but you kind of probably see what I mean so so here we pass one is basically the reward you get from executing from S1 but why I'm still having expectation here this is because S1 is also random right it's not like X1 is deterministic or determine determined as one is drawn from applying it's drawn from this environment right like so iceman has this distribution so S1 is drawn from p as zero a0 and a0 is is pi of I zero so this is p of a zero Pi of I zero any questions so far so maybe just to be more expressive basically this is equals to R of s plus if you write out this expectation you can write it as a sum so basically you draw S1 from this distribution that means that you it means that you just say for every possible S1 in a set s you look at the density of S1 this is the chance that you see S1 in the next step and then you times V pi as one that's that's just how I expanded it the definition of expectation I think uh maybe people typically when sometimes you know it doesn't really matter what variables I use for exponent where I can use any variable so maybe let's just use as Prime just to be consistent with the typical notation so basically you you Loop over all possible X Prime all possible next state and you first say I'm looking at what's the chance to arrive at that State as Prime and then I multiply that with the value function at as Prime so this is just equivalent to this uh um expectation about okay so why this is useful so first of all let me say this is called Velma equation this is an equation about the value of function V pi and it's often called Belmont equation so and also this is you know technically I think if you don't really want to have a sometimes this is called biome equation for v pipe because they're going to be another bioma equation which has exactly the same name people also call it Battlement inflation for some other quantities I'm going to Define in a moment so so this Palma equation Y is useful it's useful because um this is a linear um this is a linear function in V in V pi x right so you can think of it as maybe I'll use here so you can think of V Pi 1 up to V Pi n we call that m is the number of states I defined so you can think of this all of this as the the variables as M variables and the Belmont equation gives an equations about these variables right why there are M equations because for every s this is true for every eyes right forever as it has the equation that involves these variables Pi of V pile of us and v-pal fast part in a linear weight so so about my equation is the six of linear system equations in this variable we have one up to V pattern so um so so to figure out what is V part V Pi I you just have to solve the system equations I'm not sure whether this make is this too abstract um so sometimes maybe just just give your concrete example right so so um for this concrete example here if you uh write out this balance equation what will happen is the following right so so you probably for the for the concrete example you have maybe something like V pi I'm just plugging in some concrete thing like maybe six so you you are trying to figure out what's the equation for v pass Six v56 if you think about the equation in your first off thing is the R6 this is the reward you get in the first step and then you time gamma times the reward again in the future right so so where you have this sum right sum S Prime in s PSA on something like ice Prime V Prime X Prime right and you plug in all of these numbers here and then you get an equation which depends on V pass 6 and all of the other V5 as Prime and but this is a linear equation right so think of this as a variable all of these are variables this is a equation with a bunch of variables but they are linearly they are linear in those variables and you can write this for every everything right you can say this is R5 plus and you have the system equations right each equation is a linear equation in the variables so so basically this means that you're just computing V pi s for every s by some linear equation solver by solving the linear equations and because they are linear you can use the efficient solver like um just uh how do you solve linear equations you can um I guess one way to do it is do some inverse of the Matrix you know maybe the other ways to do a linear equation um but that's that's a sub module that you can just invoke you can evoke some off-the-shelf algorithm to solve the linear equation any questions [Applause] is it possible that there are infinite of solutions so um I think um I think in this case it's just not possible um why it's not possible it's probably not super obvious to see um and sometimes you know like you have at least you know it is it it passes the the trivials need to check but if you count how many equations how many variables they are exactly the same so so typically you probably should have a reasonable you have a unique solution and I think in this case you can prove that there is a unique solution like uh just uh because this this set of equations has some special properties um maybe like not get into that too much okay okay great so so we know how to so basically we know how to evaluate V pi x maybe you can see it again a PSA this one there will be different depending on the initial state no no no this PS um PSA this PSAs Prime this p is is the Dynamics right it's the transition probability which is uh uh which is global which is kind of like what you it's a given property of the mdp to the second one would be just r o uh six minus 0 to 5. if you subtract these two yeah oh no because all right rice that's a good point Sorry I think uh this is an a I think I only partially replace this this will be six yeah yeah yeah yes oh that is that is that's what you're asking okay okay cool cool sure so I think uh I think this will be yeah that's a that's a great question so this should be six and also Pi of six so and then if you write this then you have gamma 5 pi over 5. X Prime V pi x Prime so all the coefficients are different um for different equations different lines thanks okay cool cool so we have completed the V Pi but V Pi but the next question is how do you so basically we have solved the policy evaluation now we try we need to figure out how do we maximize the policy part how do we find out the best policy so um let me find out some places it turns out that you can use a similar technique to find out the best policy um so here is the some definition so first of all is Define V Star as to be Max pie V pies so what what does this mean this means that you are looking at all the possible policies that you can use starting from us you look at it you basically try all different policies starting from X and you you you're asking which policy give me the best reward total payoff and and the the value of the total payoff will be the uh the the V Stars so V stars is the intrinsic value of this data aspect so you're saying that the state has just how how valuable the state ass is right and how valuable is measured by using the best possible policy right vitalize what you know depends on both the pi and X right like um you know if you use a bad policy V pass may be low but we've start is saying that what's the value of this stage if you use the best possible policy in the future steps and then you can also Define the so-called Pi star this is the so-called Optimum policy this is equals to the arc Max over pi V pies right this is asking you know what is the basically you are just doing exactly the uh the arguments of the previous thing right so so the best policy that achieves the um uh the maximizer is defined to be the pi star this is the optimal policy we are trying to find out okay so um with this no two notations I think I can so I'm going to first to find out the V stars and during the pi star is you know will be relatively easy to do because um as you will see so so the first question will answer is how do we find out what's the V Star of us what's the intrinsic value of each of the state so it turns out that you can do a similar type of bioma equation or as the V Pi but just the the whole thing involves a lot of Max operators um so here's what I mean so if you think about the V Star s again you try to get a recursion for V stars because V stars is kind of complicated because it depends on all the future States so you want to have a recursion so I'm going to be a little bit the exact mask here you know I think if you want to make a rigorous you have to justify uh more formally but I'm not going to deal with that so I'm going to be slightly sloppy here just for Simplicity so you take an arc Max over Pi RS Plus gamma so so this here I'm using about my equation I just derived right so this is about my equation I just derived so because V part s is equal to this right and so let's think about this right so first of all your maximizing over Pi so this one doesn't depend on Pi even right so you can just take it out so you just say this is R of s Plus [Applause] okay so now you look at this thing so Pi shows you want to maximize this and Pi shows up in several different places right Pi shows up here and Pi shows up here right so so the pie shows up here in a sense that if you use different pie you're gonna arrive you're gonna have a different transition probability so that you can arrive a different set of State as part in with different probabilities right and and this Pi so here is trying to capture what happens after this type right so after you already see X Prime what's the future reward right this V prime minister is basically telling you know if you arrive at this S Prime what's the future possible payoff uh after X Prime so I'm going to um um so so I'm going to be a little sloppy here but let's say suppose we optimize this first V pi as you know first right this occurrence of Pi's right so if you want to choose the pie such that this is the the best and what you should do so what you should do is you should try to uh figure out you know you should try to make the pie of ice give you the best action right so basically what I'm saying is that this is equals to maybe let me write it down then it's easier to expand what I want to prove so basically I'm saying that I'm going to choose the pile of eyes to be the action a that maximize this right so pi over 5 is some action right so I just write a here and let's say I try to choose the best a such that such a pi of X is equal to a and I still want to maximize this however you know I apply affects two things right Pi also affects what happens in the next right so then I need to also try to make sure the pi uh makes the the future steps the biggest so and then the kind of the nice thing about this is that this term is something I already defined which I can it's a recursion right this is just V Star as Prime so basically if you look at the final equation it's kind of like you are trying to say you are trying to try all multiple action AIDS you take at least that way that's why you take Max right so you try all possible A's and use oh I guess sorry my bad guys [Applause] for any possible A's you have a probability arrive at its prime right and then you say that um after rough added S Prime you you after that you use the optimal policy V Star X Prime starting from that and this is the basically this part is the is the reward the best reward you can get if you apply action a like this type right if you're basically if you apply action as this step you know what's the reward you can get you're gonna have some transition probabilities to run X Prime and then after a rough at S Prime you have some Maximum possible reward V Star Express so that's why the sum is the the best possible reward you can get if you apply action a at this time at this step and then you Max over a and that's the that's the best thing you can do I guess it's similar to earlier when you are finding out the entire policy or to find the optimal policy again we have equations we have variables yep okay so that's the next step that's a good question so but maybe just any other questions before we move on to the next step okay so good okay great so together equation and okay let's see what equations right you have for every eyes you have an equation and so you have the same number of variables and the same number of equations right the the variables and I'm equations but the problem is that now the equations are not linear anymore so they are not linear equations so you still have a you have a non-linear equations that involves M variables and you have time of these equations so uh that's the challenge right so there's no any off-the-shelf solver you can use to solve these sets of equations that are non-linear so um that's why we need to um introduce this uh um so-called value iteration so how do we solve this equations to solve the equations by the so-called value iteration so first of all let's just um think of this V Star let's just Define notation V Star to be V Star one up to V Star I this is a vector in RN okay so um I view this function as a vector because I only have n possible inputs I can view this function as a vector of dimension okay so and I'm going to say that so and then my equations can be written as this right so it's kind of like V Star equals to one is equals to something like you know maybe let's just say of this equation is like this right R1 plus some Max something like this right and V Star is 2 is equals to R2 plus Max conduct this way that's my system equations that I I have so many equations each equation is look like this um so and I can abstractly write this as the following so I can I've circled this as this whole Vector let's call it V Star and the right hand side is something that involves Vista rbn right so you call this whole thing B of B Star so so basically this is just a function of v start right so I'm just abstracting abstractly recognize a function of V Star which gives you a vector this is also black so then if I do this then basically my equation can be written as V Star is equals to B of B Star I'm not doing anything deep it's just rewriting the the thing with a very abstract notation right so this is my the form of my equation V Star is equal to B of V Star and bo3 star is just the right hand side of the development equation okay so and so what I'm going to do is that um the algorithm is very simple to find out the equation so so this is kind of taking inspiration from the so-called fixed Point problem in math you know if if you have if you haven't heard of it that don't matter doesn't matter it doesn't matter um don't worry it doesn't matter so but roughly speaking the kind of the thing is that you think of this B as some kind of operation right so you say that this V Star is a fixed point of this operation you apply the this operation on a V Star it you get arrive at the same thing so so that's the connection to the so-called fixed power problem but you know but if you don't know the the connection basically the somehow there's some Theory Mass which says that if you want to solve this config spawn problem you just have to iterate until it converges so what does that mean that really just means that you have the circle value iteration so what you do is you say you um you initialize some V in this Dimension R to the n um maybe you can just do v0 that's I think that's fine you just initialize you know some randomly or maybe just interest to zero and then you just have a for Loop so you have a loop such that at every time you say V is updated to be P of V and you just skip iterate and there's a guarantee that it will converge to the to the fixed point the fixed point will satisfy V is equal to V of v and that's the the V Star and this update really just means what this last Matrix really means that you say V as is equals to r as plus basically the right Max right hand side of the the development equation so this is what really means right when you really Implement algorithm you say you compute the right hand side of the value of my equation with the hypothetical V right and then you you you give this value to the new value of field but you you update a new value of V by the right hand side of the bottom equation here I'm using this right it means like I compute this value and then I give this value to the to the vs so I think you you yeah you can guarantee that there is a there is a unique one and you can convert to it uh in a certain amount of time I think that's the homework question yep so to do that you have to I think the homework question has some hints on it right so you basically compare the distance between this V and a 2v and you can see that the distance between this V is working V with the true V Star is kind of shrinking uh iteratively okay I think I'm running quite um yeah so there's one other algorithm which is called policy iteration which is very similar um but I think I'll just leave that to um to you for reading on the lecture notes like it's it's basically kind of like a um it's also kind of not required for homework so so just optional you can read it if you're interested okay thanks"
Stanford CS229 I Societal impact of ML (Guest lecture by Prof. James Zou) I 2022 I Lecture 18,"Stanford CS229 I Societal impact of ML (Guest lecture by Prof. James Zou) I 2022 I Lecture 18

so I'll be telling you about some of the applications of machine learning especially in healthcare settings so I'm assistant professor here at Stanford my name is James so and a lot of my group works on actually developing and deploying such the AI systems for biomedical and for healthcare applications right so feel free to stop me if you have questions at any point so what I want to do today is to just firstly give you a few examples right a few case studies of like how what kind of AI systems are we using and deploying in healthcare settings and also talk a bit about then what are the some of the challenges in actually building AI systems for healthcare applications right um so the first example I want to give is actually based on this paper that we published a couple years ago it's on a computer vision system that we developed for assessing heart conditions right so the idea here is that you know there are these kind of ultrasound videos so if you go to the Stanford Hospital right or most of the hospitals they'll take a lot of these kind of ultrasound videos which is looking at the human heart um and we developed the system to basically read these ultrasound videos right and based on these videos to assess the different cardiac conditions of the patient and the system is also now being uh we develop the system as published and then we spent a lot much of the last two years in trying to actually deploy this at different parts of Stanford for example that's a setup that we have using the sets the emergency department here at Stanford okay so so how does this work right um so if you think about these ultrasounds videos right this car cardiac ultrasound also called echocardiogram so an example of this is shown on the left here right um so if you think about like the hearts as like a power pump right so the standard way to estimate like how much power the heart is generating especially by looking at these ultrasound videos right so there are actually millions of these videos are being collected every year around the US and the current workflow is that the cardiologist The Physician would actually look at these videos and they would try to identify you know which frame of the video where the heart this chamber of the heart is actually the most expanded where it's the largest and also try to identify the frame where the chamber is the smallest right and by looking at how much the volume of the heart changes from going from the largest to the smallest then they can get an estimate of how much power the heart is producing right if you think of the hardest like a pump so um so as you can imagine that process can be quite um labor intensive because it's quite manual right because they have to go through an entire video by hand and then once they find the frame they have to actually Trace out the boundaries of the chamber right to figure out its volume of the on the of the Chamber of the hearts uh all of those steps currently is down basically with sort of manual annotations um so this is where we thought like these kind of machine learning applications especially computer vision can be very useful right so we developed a system called Echo net right and what econaut does is to basically mimic this clinical workflow right so it takes its input the same kind of cardiac ultrasound videos like the one we see here right so it produces like a real-time segmentation of the relevant chamber of the heart which is the one that's shown in blue right in addition in addition to doing this real-time segmentation of the heart chamber it also produces like a real-time assessment right of how much power the heart is producing which is technically called this ejection fraction so that's the first thing is shown here uh so by doing this is sort of really simplifies and automates these different manual parts of the clinical workflow oh so no I wouldn't go into too much detail of how the algorithm the details of the algorithm but it shares like a high level overview of what it's doing right so it's basically taking input these videos right um and they're basically two sort of components right of the algorithms it's like the top arm and then the bottom arm so the top arm is basically looking through these videos using a spatial con spatial temporal convolutional Network right so it's because it's a video so we have both the spatial information right for in the given frame how big the chamber is there's also the temporal dynamical information right so this is sort of a modification of the standard kinds of confidence you probably typically have seen on imagenet type applications by adding an additional Dimension corresponds to this to capture the temporal Dynamics okay so so it's sort of like a three-dimensional convolution in that sense right so it's going through that in the top enter extract the features in the bottom is basically doing this real-time segmentation right so it's basically producing like a segmentation of this chamber of the heart that was colored in blue right and these two arms will come together at the end right to make like a actual real-time assessment for how much power or the ejection fraction of the hearts for Every Beat right because once you have the segmentation for every you know every beat right so you can actually then sort of assess the power so after it does this assessment right so there's a final classification layer where it's actually trying to predict all sorts of relevant and interesting clinical interesting um cardiac phenotypes right so there's like the probability of heart failure or you can predict that you can also assess the ejection fraction which again so basically how much power the heart is producing and so it turns out that we can also use the same kind of layer to predict all sorts of other functions like liver or kidney function because it turns out that see once you know how the heart is doing that you can actually learn a lot about the rest of the body Yeah question I guess he wants me to do with sequence we do something like that resonance but sometimes how do you decide yeah so it's a great question um so here if you look at these videos right so the heart is actually pretty repetitive right you know you know roughly about every once a second where the heart will expand and contract so there's actually a lot of repetitive spatial information right which actually makes it quite well suitable for these kind of more convolutional architectures which are looking for these the spatial patterns or you know or temporal patterns and here in particular we basically are looking at the time scale of basically once every second right so the we get maybe around say I think 60 frames per second right um and then so for every second then we make like a one assessment of of like the ejection fracture and also the assessment of the heart condition for every individual beat which is about once a second and then what happens in the end is that you know you have a for every bit of the hard drive we get an assessment of okay so how much power is that heart producing and How likely the hardest have different diseases right the video itself actually could have multiple seconds right you can actually capture many beats right so at the end we actually do like aggregation across all of those speeds to say uh holistically for the patient then what is the status of the patient thank you cool other questions great um so this is the system now that's actually used and developed here using Stanford and data from Stanford and we also test the system both at Stanford and at other hospitals right so one of the places we tested especially in the hospital in Los Angeles Cedar Sinai um and you know we just took the algorithm without any modification and then just shipped it to Cedars right and then to see how what did it do right and we're actually quite happy to see that even without any modification right on a different hospital where they had different ways of collecting these images collecting videos and data right the algorithm actually had very similar performance as it did at Stanford right so the AUC is quite high it's about Point uh 0.96 all right okay so that's the first example um any any questions about that okay so the second example I want to briefly mention right is an application of AI more for a telemedicine or Telehealth applications so what is Telehealth right so that in normal settings or usually if uh you know if if you have some sickness or if the patient has some illness right they'll actually come in to visit the doctors in person right um but recently last few years there's been uh explosion of the need for visiting and having patient doctor interactions not in person but through uh you know digital formats right especially without having to digit number start having the patients needing to leave their homes as you can imagine that really the need for these kind of Telehealth or telemedicine applications have really expanded especially during the pandemic right so for example this at Stanford in Stanford hospital so just over the last two years there's actually something like a 50 fold increase the number of these digital or televisits right compared to about two years ago so one of the you know so telemedicine could potentially be really transformative for healthcare right it means imagine like not having to actually leave your home and drive an hour to come to Stanford right much easier to see doctors and also make appointments um one big challenge right was telemedicine in general is the idea that you know can you actually get sufficient information without having the doctor seeing the patient in person right um and in particular like oftentimes right lots of information that the doctor gets is by you know sort of visual interactions with the patient if I actually see you face to face then you can often examine them quite closely and that's difficult to do with you know when you're doing this on on Zoom where you know through some other video visits so and that's really one of the big challenges Telehealth is in getting high quality images right from the patient to the doctors and for example at Stanford there are actually a large number of visits that are wasted right so patients will set up a visit with a doctor but then the doctor is not able to get sufficiently good quality images from the patient right so so then they can't really make it informed recommendation or informed diagnosis right and then they have to sort of reschedule when we turn that back into like in-person visits so there's actually a large number of hours both by the patient and Physicians that's wasted because of this lack of quality images so what we want to do is to basically um you know see can we actually use machine learning and social especially about using computer vision to improve the quality right of these images for specifically for this Telehealth type applications right because it turns out that you know people are very good at taking photos for their Instagrams and for their Facebook but maybe they're not so good at taking photos that are clinical quality with our eie like of that are informative for this clinical decision making process right so the idea is can we actually use machine learning to guide people and help people to take more clinically informative images that's a lot of the motivation behind this um algorithm that we developed it's called True Image right which has also been sort of commercialized through Stanford's um and so the motivation is quite intuitive right so similar to how like online check check deposit works right so idea is that you know we want something that's very simple it can be run on you know on people's phones and then it would automatically tell people like you know is the image that you're taking maybe off your skin Legion right is that sufficiently high quality for your dermatologist right for your clinical visits if it's not so good quality then the algorithm actually provides real-time feedback right guidance to the patient on how to improve the quality right maybe it says that you want to zoom in a bit more or you want to move to the closer to the window to get better lighting right so provide these real-time guidance until they get sufficiently high quality right um so yeah so basically the True Image algorithm is you know it's uh so it basically runs right it's designed to be run sort of at the patient's facing side right so maybe like so the patients could be taking a photo right of their you know of their skin and then they want to use that to send it to their dermatologist for their televisions right and the outputs obviously was decide like if the photo is sufficiently good quality right if it's sufficiently good quality then that's fine right so the image just goes through as normal if it's not so good quality right the algorithm would decide okay how does give a recommendation to the patient right say how do you actually improve the quality of your photo in this case maybe it says that's okay you can move to a brighter lighting right and the patient will retake that uh and if it's good enough then no it's surf passed through the algorithm it's a setup an application clear to people okay um so a little bit more about under the um okay I'll just jumped into the sort of the how do we how well does this work right so the algorithm we actually conducted like a prospective study here at Stanford right um perspective study means like there's actually like a real-time study where we recruit the patients they use these tools it's almost like a clinical trial right so this is done in the dermatology clinics that Stanford operates um and we tested this on maybe about 100 patients right um and it was actually quite effective right so by the patients using this algorithm they were able to basically filter out but 80 percent right of the poor quality photos which are IE like photos that are um they would have sent to the clinician but otherwise would have been sort of useless by the clinician because there are not sufficient high quality to actually make a meaningful informed diagnosis right um it's also nice that this is actually very fast right so on average it takes actually like less than a minute for a patient right to generate a high quality image by going using the stream image algorithm um and this is the sort of an example of the kinds of improvements that you see here right so maybe like this it's like an initial image that someone would actually really would take and send it to to their doctors for these Telehealth applications right for these one of these Telehealth visits um and you know that True Image algorithm identify that though this image has the following issues with the blur and the lighting it makes a recommendation and then after using the algorithm right the patient actually have much better image that um now would facilitate their televisit so this is actually being now uh it was tested at Stanford in dermatology settings right and it's also being integrated now into the Stanford Medical Records cool any questions about this before we move on yes this is pretty useful easy just based on it yeah but beyond that are there any other like readily convertible fields in this exactly like oftentimes you need to be a doctor assistant looking at your throat or something interesting things yeah yeah so it's a good question so I think Dermatology is probably the the most immediate application of this right um there are also a few others or from like Primary Care settings right where often the doctors actually get more information just by inspecting what the patients look like and how they're how they behave right so this is where it could also be useful for other settings where if it actually requires taking a biopsy of the patient right for more like pathology or you know cancer diagnosis Beyond skin cancer then um then the patient will still have to come in to the visit yeah so it's a good question so uh so what the algorithm does is that it actually does incorporate quite a bit of domain knowledge so one of the developers on our team for doing this she's actually one of my postdoc with the dermatologist so she sees patients one day a week and we actually did our initial piloting of this in her Dermatology Clinic all right so for example the kinds of domain knowledge that comes in will be uh first we actually you know take the image and we first segment out this just the skin part of the image right because if you take an image there could be all sorts of background right maybe our Furnitures and chairs and we don't really care about the quality of those backgrounds so if you segment out to the relevant human skin um and to identify and after we stick about the human skin right and then we also try to map onto what we think are the likely issues towards the image if there are issues right so oftentimes the things that come in would be like is the image does it have enough lighting right so the kinds of lighting that's required for a dermatologist is actually quite different from the lighting that's you know for on the standard photos so that's actually one place where often people make mistakes right and another place where the expert analogy is very useful is in terms of how much Zoom is needed right sometimes if people zoom in too much that's actually not so good because then they lose the context of the surrounding like if you just zoom in Only onto your legion then you lose the context of the uh this neighboring parts of the skin or if it's too zoomed out you also don't have enough information right so there's like an optimal Zoom which we get basically actually by so the way that we train two images is actually by having dermatologists uh generating annotations about what is like the optimal Zoom from like a database of of images cool yeah good questions um it's like another example it's very quickly mentioned it's um we've also been developing these algorithms for uh using machine learning to improve clinical trials right so clinical trials are like the most expensive parts of of medicine right each trial could actually cost hundreds of millions of dollars to run right and sort of like they're really the bottleneck of this entire biomedical translation process right so one place where we found that where machine learning can be very useful is in helping these clinical trial or helping the drug companies to decide what are the a good sets of patients to enroll in a given clinical trial because you want the patients to be that they're diverse right so they really cover diverse populations and also that's the drugs likely to be safe and effective right for that set of patients right so this is basically a tool that we developed called trial Pathfinder and you know for helping to guide the designs of the clinical trial specifically the designs of the which cohorts of patients are eligible to participate in a clinical trial and this is being piloted Now by some of our collaborators and Partners at Roche Genentech which is the largest biopharma company and if you're interested the more details are described in this paper good so now that we've talked about a few examples right of where machine learning can be used in healthcare settings and where in case having a substantial impact I would also like to discuss some of the challenges and opportunities that arise when we actually think about deploying machine learning in practice right because you know in in 229 we talked a lot about actually how to build these algorithms right and there's also a lot of interesting challenges you know I've rebuilt them thinking about how do we actually really deploy and use them in practice so just to set the stage I want to give you like a concrete example with sort of like a little detective story right so here's the story right so um you know I mentioned this Dermatology aif applications right so the Dermatology is actually one area where there's been sort of the most intense interests and investments in developing AI algorithms right precisely because the data there is relatively easy to collect and oftentimes these algorithm also work as follows right you take your photo then maybe of a legion right you can take your phone and take that photo and then behind the scene here there will be some sort of often like a convolutional neural network right this looks through this photo and try to classify it's just likely to be cancer or not right so in this case it actually predicts that it's likely to be melanoma so it's skin cancer and the recommendation is that the patient the user should go visit the dermatologist as soon as possible okay so the reason why this is useful right is that there are actually several millions of people every year who have skin cancer but they're not diagnosed until it's too late our skin cancer if you diagnosed it early on then it's actually very treatable but if it's too late then it's deadly right and potentially many of these people right they actually could have made earlier diagnosis uh no because they all have many of them have access to not be able to take these photos so that's the reason why there's a lot of interest now both by academic groups and also by commercial companies like Google again pushing out like these kind of AI for Dermatology applications so of course oh yes go ahead one question so do you what's the target consumers this is like the ordinary patient or it's actually the doctor because I mean if there's something growing on your skin I mean like I think your status to actually show a doctor or something like that so what what is your target base yeah yes good question so there are algorithms there people are putting out that are consumer facing right there also alcoholisms that are more clinician-facing so most of these ones here are actually more consumer facing and the reason is that to actually make an appointment to see a dermatologist could be like a three months or six month wait wait time yeah uh whereas uh maybe people just they don't want to make a visit every time they see something because they don't know if it's like to be serious or not so basically for that kind of applications where a lot of these consumer facing algorithms are being put out yes just focusing because we all do we don't want people that actually has the cancer signal but be classified as like uh now yeah so that from here probably like what what if the absence is that's possible that is anything successful Hardware produced Maybe yes yes it's a good question um so here I'm Tony showing like sort of the AUC of these algorithms from their original Publications and papers and in addition to AUC people also care quite a bit about like the sensitivity and specificity which I also mentioned in a bit right and in particular I think the sensitivity is probably the important thing here a sensitivity here means that if the patient actually has skin cancer how often would algorithm say that they actually do have skin cancer right so that's actually really the important part if you you know if patient doesn't have skin cancer and the opposition says you have skin cancer that's not great but it's actually not too bad right because maybe they'll get a check then they say it's it's okay but if you actually miss the diagnosis and that can be um more potentially like more more more damaging to the health Okay so you know given that there's a lot of interest in algorithms certainly we're interested in thinking about how to potentially use them and deploy them here at Stanford right um so we actually took three of these state-of-art Dermatology AI models right they're all solving this task of you know given photo predict whether it's malignant or benign right skin cancer or healthy um and we tried trading out here at Stanford right so if you look at the original algorithms they all have very good performance right so AUC is very high um however when we tested them at Stanford on real Stanford patients right the performance is suddenly dropped off quite a bit right it's much worse as the aec dropped from like 0.9 to 0.6 okay so let's sort of the the setting of this little detective story right so what we want to understand is okay so why did this happen right why did these algorithms performance perform so importantly on Stanford patients because we really need to understand that if we really want to be able to use this in a responsible way in practice and just to be clear here right so these are actually just you know images from real Stanford patients right there's no sort of adversarial perturbations or attacks down on top of these images so before I tell you what we found right um what's this people actually like to guess like what do you think are the some potential reasons why the algorithms performance dropped off so much when they're applied on the real patients any ideas a lot of times so like when it was in real patients it was 0.93 and my life wasn't Stanford equations it was 0.6 for this was uh is that our Stanford Creations were the real patient information so the 0.93 was the performance of these models on their original test data so the original test data also came from you know real patients that these companies or these groups have collected that 0.6 is the performance of this algorithms right when you apply them to actually stand for patients [Music] was a test data taken from a different types of different types okay so that so that could be one fossil Factor like is there some differences like temporal differences in data sets oh yeah go ahead [Music] okay [Music] foreign [Music] okay so that's also a good idea so maybe there's like some age differences between the original test patients and the Stanford test patients yes for both people okay okay so that's also a good idea so maybe there's some changes in the location which drives different distributions of diseases right skin diseases that are more common here these are good ideas um there's a yeah any other suggestions yeah okay okay so that's also a good idea so maybe there's some uh differences in like what kinds of cameras were used or how the quality of the images across these the original test data and the data here yeah so these are all good ideas about there's actually a couple of really big factors that are haven't been talked about yet people want to say more days [Music] okay okay so maybe there's some caution but whether the models are being overfit or not into the original test data okay yeah so these are all excellent suggestions so you know we actually did sort of a systematic analysis like an audit to try to figure out what happened here um and the goal of this audit right mathematically is that we really want to explain this drop off the performance right so we see about today let's travel from 0.93 to this point six I will want to understand what are the factors that statistically explain right this difference in the model's Behavior so I actually think of one of the the biggest single factor is actually label mistakes right uh so what does that mean so it if you look at the original test data right the data that was used to evaluate these original algorithms right in their initial Publications so it turned out that there is no test data had a lot of mistakes in their annotations all right so what happened was that's the test data were generated by having these Dermatology images right and then they would have dermatologists to visually look at the image and say okay is this benign or is this malignant because that's relatively easy to collect right however just even having expressed dermatologists visually look at images can also lead to a lot of mistakes right so the actual ground truth uh come from you take a biopsy of the patient and then do a pathology test to say does this patient have skin cancer or not all right so that's what ground truth from the biopsy is basically the the labels that we have here at Stanford right but the labels from the original test data actually had a lot of noise in them because they were just coming from these visual inspections right so and this actually explains quite a bit of the drop of the model's performance and this is maybe not something that's um that's sort of the first thing that comes to mind if you think about like somebody build a test data set and they evaluate it we're oftentimes in machine learning right we just assume that's okay the test data should be pretty clean should be good right but in practice uh the quality of the label itself in the test data can often be highly variable depends on the real world applications all right so the first question that we should always ask is okay so how good is the quality of the test labels right how good is the quality of the test data so a second big factor which people mentioned right is that there is actually a distribution shift in the different types of diseases right so the original test data o has relatively common kind of skin diseases again because it's relatively easy to collect the data here we have at Stanford has both common and also less common diseases right because all sorts of people come to Stanford to get treatment right and the algorithms perform worse on the less common diseases right and because of this distribution shift it's also explains to some of the drop-off in the model's performance so the third factor is that actually turned out that these algorithms um had significantly worse performance when applied to images from darker skin patients right so specifically if you look at the actual the sensitivity death model which as we said is what we really care about if a patient has skin cancer How likely is it to find those skin cancer right this the so the sensitivity is actually much lower when the algorithms are applied to images that come from dark skin patients and you know we dug deeper into this it turns out that that's the original training and test data sets had very few and in some cases zero images that come from darker skin individuals right and no that's just I think the overall takeaway here is that oftentimes when we look at the application of machine learning in real world in practice right um it's very difficult to interpret the performance level model right so if someone just tell you their AUC right it's very it's almost meaningless unless you really know the context of what is the data that's used to evaluate that model so here I talk about you know the Dermatology settings but we actually did sort of similar kinds of audits of all of the medical AI algorithms that are approved by the FDA so so as of last year right there's like over 100 medical AI systems that were approved by the FDA so they can be used in on patients so each symbol here corresponds to you know one of these algorithms right and here I'm just stratifying them by which body part they apply to right so some of them apply to you know to the chest or to the heart Etc so there's a bunch of interesting findings we have from other things algorithms but I just wanted to highlight maybe a couple of the Salient points just for today right so the most interesting things may just look at the color right so I colored each of these algorithms uh blue if the algorithm actually I reported evaluation performance across multiple locations maybe from multiple hospitals or otherwise it's colored Gray so it's already quite clear right that most of these algorithms are over 90 up to 130 did not report or couldn't find the evaluation performance across multiple locations right we only see how well does it work at one site okay in addition to that right so four out of these only four of these 130 devices were tested using a prospective study right by perspective I mean more like a human in the loop study right so they had the algorithm and they tested how well does this work you know uh in a real setting was was maybe doctors were as patients so the remaining the 126 of 130 were only tested on for retrospective data right and that means that's now somebody collected like a benchmark data set beforehand and then they apply the algorithm to that Benchmark data sets so retrospective Benchmark data set could actually have come from the same hospital where the algorithm was being trained or developed since we saw from the previous example in dermatology setting right so if you only have data from one location that's collected retrospectively right that can potentially mask substantial limitations for biases in these models okay any questions yourself yeah it's a good question um this is actually something that we are working together with the FDA on right so the FDA has a quite rigorous process for evaluating drugs right for example like for the covet vaccine to be approved at the FDA they have to run like a very large scale randomized clinical trial right and to show that the drugs are safe and effective the evolution standards for medical AI algorithms by the FDA is actually quite different compared to like drugs right so for example these algorithms they do not have to go through like a prospective clinical trial right it doesn't have to be randomized in a clinical control right so that's why many of them were tested on these just retrospectively collected like Benchmark data sets right so one of the interesting challenge going forward is to figure out what's that feel like what's the proper way to evaluate and to monitor these algorithms in practice okay so now given that these challenges that we we saw here right um so I just want to quickly go through a few lessons that we've learned uh or recommendations that we have for how do we improve the reliability right or trustworthiness of these AIS especially as they're being deployed in healthcare or biomedical context right now based on some of our experiences from both building deploying and also evaluating these models so I'll say a little bit a couple of slides about each of these points right so the first one is that I think as we saw that there needs to be a much greater amount of transparency about what data is actually used to Benchmark or test each of these algorithms right um so for example just to give you a concrete visualization of this so we actually did a survey analysis of what are all the different types of data that are used to Benchmark Dermatology AI models right so now each Square here corresponds to one of these Dermatology AI models right this is where people have published the paper on this right and so that's the square so that are the models for the papers right so the circles are the data sets okay so the size of a circle corresponds to how big that data set is and there are two colors right so the red circles are basically the private data sets which means that these are data set that someone could be a company or academic group that has curated but they're private so that nobody really has access to it and then the blue ones um circles corresponds to the public data set so these are like the the openly available Benchmark data sets right so for example one here it would be like a relatively large public data said it's often used by many of these algorithms for benchmarking and what's quite interesting is that there's actually a large number of these algorithms maybe about half of them right that's um were mostly tested or entirely tested um relatively small actually private data sets right basically like all of those ones on the on the top right all right and those are the ones where it's actually could be potentially problematic right because then it's actually very hard for people to audit and to understand like what's going on enough data and what's going on in these algorithms so you know so we've been very keen to try to uh release much more like publicly available data sets right so no I mentioned that we built this thermal uh this Cardiology video algorithm and actually as a part of that paper we had created and released I think one of the maybe the largest publicly available Medical Data medical video data sets right so it has basically all of the videos that we trained and tested on right so we released all of those there's over 10 000 videos along with the patient's information and annotations right so this is all publicly available so that people can use this for additional research I think this is still maybe one of the largest public data set of medical videos okay so in addition to understanding what data goes to developing the models right so we're also very interested in thinking about you know more quantitative ways to understand how to different types of data contribute to the performance or to biases of the models right so what does that mean right so from a machine learning or statistical perspective right oftentimes you have your training data right here you have these different each symbol could be like one of these sources of training Dynamic data from a particular Hospital right and then you have your favorite learning algorithm so we're a model agnostic or it could be a deep learning model or could be XG boost random Forest right and you have whatever performance metric that you care about in deployment it could be accuracy or some sort of loss or F1 score and let's say if your model actually gets you know 80 accuracy right so ideally what I want to do is to be able to partition that 80 accuracy back to my individual training you know individual data sources of my training set right so I want to say that oh how much each of the data points for Insurance data sources contribute to the model's performance and the reason why that's useful is that you know if the mod or if the model actually makes mistakes right in deployment or if exhibits biases in deployment as we saw right then we also want to be able to say very quantitatively like what specific training image or training Source actually are responsible for introducing those biases or mistakes in the model's Behavior right so if we can actually do this end to end from the data to the model and then going back to the data right so then this will make the whole system much more accountable and more transparent okay so you know in a bunch of Works um you know with my students we have actually developed sort of approaches for doing this right for exactly trying to do this data valuation uh it's based on these ideas we're calling sort of data chaply scores so the idea here is that we're able to sort of compute a score for each training point right could be a training image and the score will actually quantify how much how much does that image contribute to the model's Behavior either positively or negatively in deployment right so this is for example if we use our Dermatology as our running example right so the training set could be quite heterogeneous could be quite noisy as we saw right and the models trained on this could have relatively poor performance when it's deployed in clinical settings so data Shuffle score that we propose actually would just be like a number a score for each of my training image right the score could be negative right if that image is somehow not informative or contains some misadmentations or introduce some sort of outliers or bias to the model right so the model if it's trimmed down that image actually does worse and the positive scores right indicate that these are the images the training points that are informative such that when the models train down those images they actually learn and do better in deployment it's actually capturing some informative signals right so these Japanese scores can then be computed throughout the efficiently on these different data on individual data instances and this is actually quite useful also for improving the model's reliability right because one thing that we can do now is to you know weight my training data by the Japanese scores right so a simple idea that we after we complete the sharpiece course a simple experiment that we can do is to just take the original model and just retrain the model on the same data set except now I'm waiting each data point by their shafly scores so this has the effects of encouraging the model to pay more attention to data points that have high chapty scores right which are the again the data points that we believe are the more informative uh or have better annotations right and by doing this this action actually can substantially improve how well the model Works in deployment settings right and the benefit of this approach is that it's quite data efficient right because we just still have the same data set we didn't have to go out and collect a new data set right we'll have the same data sets and actually the same model architecture the only difference now is that the same model architecture is now being trained on a weighted version of the data rather than the vanilla kind of training okay so so those stars are you know two I think quite complementary ideas right once we want to be much more transparent about what the data is coming from right and what's the data Shuffle scores this helps us to understand how do the different types of data really quantitatively contribute to the model's behavior and by understanding that contribution let's also give us ways to quickly improve the model's performance um so the third lesson we learned actually quite important is actually really useful to try to really understand why does the model make specific mistakes right because as a general message if we want to ensure that the AI systems that we deploy are safe or responsible it's actually the mistakes that are the most revealing right we want to look at because by looking at the mistakes we can actually understand what are the potential blind spots where the weaknesses what the limitations of the model um so we developed a bunch of algorithms to try to basically provide more high-level natural language explanations for why does the model make specific mistakes as a way to teach us about blind spots of these machine learning models so here's one example right so let's say if you've actually put in this image so image the true label is uh zebra but if you put it in this image whereas a lot of the outputs almost some of them will make a mistake and predict this to be a crocodile rather than zebra right and in this case we'd like to understand why okay why did that happen and this to work so the explanation we provide using this tool that we call sort of uh you know conceptual explanation of mistakes it's actually automatically generates a reason for why the model made the mistake in this context right so in this case it's because there's not actually too much water in the image right so in other words if the image this image has less water and maybe more field right then it would actually have gotten the model could have gotten the correct prediction of zebra rather than crocodile right so you can view this conceptual this mistake explainer sort of like a raptor around in a different computer vision you know AI systems right so it takes one of these AI systems right and looks at the mistakes the models make and then it provides like this high level natural language explanation for why did the model make that mistake on that input so this is quite useful because then we can apply our AI explainer right this mistake explainer to also sort of explain you know why did some of these Dermatology models make mistakes on these different users on these different kind of patient image um so here are actually Four example inputs right where the original Dermatology AI classifier made wrong predictions so the correct diagnosis the correct label is written on top and what models the predict is written on the second line and in each of these examples right so our mistake explainers or automatically provides uh sort of a reasoning of why the model actually made that mistake so for example in the first example right so we learned that the reason why this model actually made the wrong prediction here is actually directly because of the skin tone right so it's it's um uh because of the darkness of the skin tone so in other words right if the skin actually had been lighter right or else being equal then the model would have actually gotten the correct prediction in the second example right so explainer learned that it's really because of the blur in the image that led to the model to make that mistake right and if the image at the same image had been sharper right the explainer learned that then the model would have actually gotten the correct prediction which is on top so in the third example it's because of the zoom right so it turned out that it's actually too zoomed out and that's really the reason why the the classifier made those mistakes and the fourth example is because of there's like two it's too much hair in the image right and despite actually understanding why the models made these mistakes in each of these specific instances this actually gave us quite a lot of insights into potential limitations and blind spots of the model right here we've already learned that and potentially it doesn't really work well on dark skins uh it really needs to have pretty Crisp Images it can't be blurry and there's certain level of Zoom that it needs in order to make this diagnosis and also if there's too much hair in the image then that doesn't the model doesn't really work well right and this insights are actually pretty actionable right for example you can then take these insights and then as a guideline to help the users to improve their image quality right so that's maybe you actually tell the human users like we did with True Image right you want to zoom in a bit more or you want to um you know take sharper images right it could also give us insights on you know if we want to collect additional data right what additional data should we collect in order to improve the model's Behavior across these different and improve their weaknesses right maybe we want to collect you know more diverse images across different skin tones we also want to maybe collect more training data with more like hair in it foreign that we need to have much more human Loop analysis and testing of these AI models right because if you think about how Machinery is often developed right um I think it's often optimizing for the wrong objectives because most of the time right you have a data set that's fixed the static data sets right then algorithms optimized you know with SGD try to optimize for its performance on on that static data set that's actually not what you really care about right it's what you really care about is actually how well the algorithm really works in the real world applications which oftentimes it's not really in isolation right but it was some team of human users especially in healthcare setting like most of these outwards on the top right here right they're not just working isolation but there's often some clinician who takes the input from the takes the recommendation from the algorithm and then make some final decisions right so in the ideal setting what you would really like to optimize over is maybe some to like an SGD right to optimize the model's performance directly for their final uh usage right rather than under accuracy on the static Benchmark data set um but that's actually challenging to do right so to address this challenge what we developed um these platforms called radio which actually makes it very easy to collect real-time feedback right from users at all sorts of different stages of my model development right so basically it was one line of python code right we can basically create a wrapper around any machine learning model and this wrapper also creates like a interactive user interface right which can then be shared like as a URL with any user right so if they open up that URL then they can just interact with the model in real time on their browser without having to download any any data or having to download or write any code right and by doing this right that makes it very easy for even for non-computer scientists to be able to interactively engage with the model right to test it out and provide feedback on you know of the sort that we discovered that we discussed before okay so creative is actually now um now is recently acquired by hugging face but still public right and it's also being used by basically all of the larger major attack companies and the many thousands of machine learning teams it's also what we use to power some of our own like deployments here at Stanford so just to summarize right so I think these are sort of the four main key lessons that we learned from our experiences in applying in building and deploying and auditing these AI models right uh no I talk a lot of here about applications in healthcare settings but I think many of these have lessons and applications also apply more broadly in other other domains where machine learning is being used okay and all of the papers and the sort of the the algorithms I mentioned are all available on my website and here again are the different references and I also want to thank the students a lot of these works so maybe we'll pause here and then see if people have any more questions [Applause] paper button yeah yeah good so the hell of ideas that we want to estimate the impact of individual data points right and we do this by basically adding and moving into this data points across different contacts right so in each context I basically have a different subset of my data right and then say okay so what's the impact of adding this particular data point to that subset right if I add this point that's an improve my model's performance before after adding a compared to before adding it uh and we do this basically across a lot of different scenarios each scenario corresponds to a different random subset of my training data and the reason why we do this across many scenarios is to really capture potential different interactions between different data points and then back then we finally aggregate across all these different scenarios to get one single score the data shopping score for each individual training point the snc you have to retrain the model multiple times with different subsets of the data and I'd evaluate them on your test to see how they detect an individual yeah so in principle if you want to do this exactly then you need to retrain the model many times so we actually came up with a bunch of different efficient more efficient algorithms that enables us to estimate the sharpiece score so without having to retrain the models right so for example in some other applications we can actually come up with sort of analytic mathematical formulas right to either exactly or approximately compute the shaftly values without having to retrain the models yes I have a review party um that's right yeah so the original ideas for these kind of shafly values actually came from economics from Game Theory where they're the people are interested in ideas of how to allocate credits among different users or among different participants in the game right so imagine if all of us if we solve we do a course projects and we get some bonus and how do you split about that bonus about each of the individual participants so that people don't complain people they're happy with their bonus so it's developed in that context of more like Game Theory credit allocation um and we extended that idea to the context of the data right now instead of having individual workers or participants now I've based everybody brings their own data sets so data is what works together to train the machine learning model basically the performance now is basically how what is the bonuses how well does the model perform right so then we can see okay so how do you allocate or attribute the performance the model back to the individual data sources or individual data sets cool any other questions yeah um [Music] did it tell you are those pre-provided by the team or are those like generally attached Me by that novel yeah it's a good question so what we have is basically we have a library of Concepts right uh like what we call like a concept bank so basically we can look at all the sorts of common visual concepts like the concept of water or concept of stripes or concept of zoom and color right so those are OB Concepts right so I'm going to create like a pretty large library of hundreds of these Concepts right and then those then that's basically the input into the explainer as the explainers try to see okay so what of these Concepts would actually be able to explain the model's mistakes in cases where the concept that we provide are not complete but maybe there's some texture information right the that actually leads to the mistake but it's not in our concept Bank we'll have some additional ways to try to automatically learn Concepts right in a more unsupervised way directly from the data for most of the concepts we use are actually you know from these large concept banks that we can just learn ahead of time okay great yeah and then we can wrap up here"
"Stanford CS229 Machine Learning I Model-based RL, Value function approximator I 2022 I Lecture 20","Stanford CS229 Machine Learning I Model-based RL, Value function approximator I 2022 I Lecture 20

uh so I guess so today we're going to um on the last lecture on reinforcement learning and I'll probably spend like five minutes to briefly wrap up the whole course but mostly we're gonna talk about reinforcement learning so um so this is supposed to be a more kind of like introduction into introductory lecture on some of the a little more advanced topics in reinforcement learning so I wouldn't talk about a lot about details but mostly I'm going to Define some terms and so that it's easier for you to kind of like either take another course on reinforcement learning or read some of the literatures yourself so I guess last time we have introduced the basic concept concept of mdp a Markov decision process that's the main language that people use to think about reinforcement questions I'm going to start by just reviewing some of the the key idea to recall that you have mdp Markov decision process this is a kind of like um it's described by a few important Concepts so one thing is the state space you have to specify the state space to specify the mdp you have to specify the action space and the mdp has this transition probability which is called PSA for every s and every a H ice is is in the um ice is the state is the action for every I said a you have this so-called transition uh probability Matrix or transition probabilities which is to describe what is the probability that if you start with a state s and take action a what is the probability to arrive at a new state so um and there is the so-called discount Factor gamma and a reward function r so after specifying these five quantities you get mdp and we also talked about the concept of a policy so policy is something you are trying to learn from interacting with the system the environment um you're trying to learn the so-called policy which is a function that maps from the state and action so this policy tells you what you do what which action do you take when you see your state s um so Pi of ice is the action you take when you are either State s um and we also introduced these two concept uh two type of value function so the first type of value function is the value function of the state s so this is the value of the state s and then the policy Pi so this is the expected reward uh expected future payoff um of uh executing the policy pi from say the S right so you keep update you keep um you keep taking action from the policy Pi you start with State s and then you cut you you compute what is the total future payoff in expectation and that's V pile of x so and we also discuss this so-called V Star of us this is oblivious to the policy Pi this is asking you know what's the maximum possible reward you can get from starting from State s right so you maximize over all possible policies and you look at you maximize the V pile phase um and the maximizer of this process the arc Max of this is kind of the optional policy you care about you want to find out what is the optimal policy all right so I think we probably didn't have time to Define this you know um uh formula last time so the optimal policy Pi star is the so-called uh uh the arc Max hi and this this is actually um there's a unique Pi star that maximized the vpa is for every X just because the policy itself is already a function of us right so you're finding a policy that Maps the function as to the action you already can take different actions for different status so um and this is one way to this Define this and another way to define this is the following so this is another way to define optional policies you say that this is the greedy policy with respect to V Star so so this is alternative definition of the optimal policy which is defined to be Pi star of iris is defined to be the the best action you take such that you maximize your future payoff what's your future payoff the future payoff is equals to R of s plus the payoff you get from the future steps which is gamma times um this PSA as Prime this is sum over S Prime this is the probability that you see your X Prime after you take action a which is the variable here and then you times V Star X Prime right so this part is the expected reward you take you you the best reward you can get after you take policy you after you take action 8 right you take action a you have some chance to wrap at S Prime and you multiply the best payoff you can get after your after starting from X Prime and and this so that's why this is the expected the best possible expect payoff after you take action and this is the payoff you get at State s so the total thing is the best payoff including the current payout the current return the best future payoff if you take action and you maximize over a and that's the the best policy the best that's that's the definition of the optimal policy right so because this is already of the optimal Choice you're already thinking about the optional choice for all the future steps and then you take the action a that is the optimal um for this step taking into account the future steps and that's the optimal policy for the status any questions I mean sometimes this is the way to find out the optimal policy because if you find out what's the the V Star then you can find out the opt-in policy because you just take the greedy policy with respect to P star and we also introduced this important concept of bellman equation which is the main tool that we use to find out to compute V pi and V Star so for V Pi if you're given Pi then about my equation for v Pi uh Development question for V Pi is equals is this so V path is equal to RS plus gamma times this right so so so most of the Belmont equations you can pretty much verify intuitive for yourself right so because what is the the reward the payoff when you when you write State s accessing policy Pi you first look at what's the current reward for this step and then UK what's the future reward the future reward involved can be computed by considering all different possible outcomes of executing pile files right so if you apply a pile of us then you can you have some problems around S Prime and that's you multiply that probability with the the the the payoff you get after arriving at State as per okay and and this is uh and one of the important thing here was that this is actually linear uh in the variable we pass so linear in uh in this um in a variable V Pi one up to V pi uh I I think I used M as the number of total States so so this is a linear equation of these variables so you can solve the linear equation by any linear system solver and we also introduced this uh um equation for v star which is of a familiar form but the difference is that now you don't have the pi you are you have to maximize uh the action so you have RS plus a Max you take the Max Plus the best possible action that maximize the future reward right so so now this is not a linear equal system equations in terms of V Star but you can use the so-called on the the algorithm we introduced last time was this iterative algorithm that you do the Bell map update iteratively uh you can do this iterative algorithm to find out restart and the question so far this is basically a review of the last lecture so um okay so so far we have deal with you know in the last lecture we have to deal with no Dynamics right so in all of this right so we have described the algorithm is so and so forth everything was you know was kind of like under the assumption that the algorithm so basically the algorithm to solve the V Pi or the algorithm cell V Star the iterative algorithm I guess I'm not sure whether you still remember the algorithm the algorithm here was just something very simple so you take a loop and you just say I'm going to update V I have a working memory for v i update V uh like a vs updated to be something like RS plus Max but you just compute the right hand side and then with the the V plugging in here and then you update the the left hand side with itself so this is called value iteration algorithmic cloud value iteration so both the value iteration algorithm and the algorithm that solves the linear system equation in this case both of these two algorithms are assuming you have a known Dynamics so the PSA is known the all the family of PSA is known um um like in both of these two algorithms because you have to compute PSA right so so in in the arrow language you are saying that this means that you have the known transition Dynamics or the known um like uh environment that's how people refer to this kind of settings so but in reality what happens is that this PSA is not known anymore so for example thinking you know sometimes you do know it right for example suppose you consider this is a game like playing goal right you are playing the goal or chess so you do know the rules of the game right you know what happens if you play an action a what will happen next right so you're gonna move this the each of the the piece you know in some way right so you know the root the rules then then in those cases PSA is not but in many other cases the transition Dynamics is not known right so for example when you control the robot right so in some cases you probably know a little bit about you if you control this the robot would move forward but sometimes if you're doing a low level control you are changing the joint of the robotic arm or the the hand your body hand um you don't exactly know how the the all the all everything moves exactly you probably have some rough sense but you never you are never able to model them exactly actually this is a challenge so so this is actually the reason why kind of like now people are using more and more learning techniques so I think in the in the early days I think for example there was a company called Boston Dynamics so what they do is they basically just use rule-based so basically they build this PSA they try to figure out from physics what's exactly this dynamical systems should it look like and then they they build their policies based on on that Dynamic ecosystem so but um but these days I think people are kind of like at least trying to apply more learning techniques because there is no way you can figure out exactly what PSA is on just just from the physical rules you have to use some kind of like learning based technique sometimes also it involves interaction with environments for example suppose I have a robot that is moving on this carpet then the speed will be different from the robot moving on the on the hardwood floor right so so so the so the the other environment is also part so like the floor the other things also part of the environment so then you can never model everything perfectly where you would never know the texture of the the floor exactly so so that's why we have to learn the Dynamics to some extent so so I think that's the um that's what that's the that's the in some sense that's the real uh problem in reinforcement learning where the Dynamics is unknown so when you don't have the Dynamics right so uh what what can you do so um you have to know something right you have to somehow have some information about Dynamics so the typical assumption which is is that um so the the the PSA this is I know but you have the but given a state s and action a uh we can sample as Prime from this transition Dynamics basically you can just try try the robot in the real world we can just say I'm going to try my action a in a real world and see what's the next S Prime is right the X Prime will involve a little stochasticity you'll do some Randomness but you're going to observe one random sample from this transition Dynamics um so so so so so so so that's why when people call sample complex it means that how many times you have to try this where for how many state interaction you have to try to see ice Prime um by just try it in a real world um so so that's how people generally learn so learn by interacting with the environment by trying all these actions and and and you somehow learn the Dynamics in some way so that's the basic assumption and then there are two type of algorithms in reinforcement learning that are um the most popular I think most of the algorithm can be either categorized into one of this group so so one one type of algorithm is called Model based RL so here the model means the dynamical the model means the dynamic mode or the transition Dynamics so as the name someone suggests you know basically model based arrow means that you explicitly uh learn the transition Dynamics how do you learn it that you know there are multiple variants you know it depends on the situations depends on your how complex your Dynamic is but you know the transition dynamics of the transition probabilities on probabilities I guess they mean exactly the same thing for for you know they already they probably just always mean exactly the same thing um but sometimes people have different terms so so basically learn this PSA or especially you build a model to describe this PSA um or like and you learn this model uh from the the samples the samples are the data you learn from and and then you build some approximate PSA from the samples and that's um that's the typical kind of like types of um um type of like a the table types of kind of like uh model based R algorithm so from samples Okay so and there's another type of type of algorithm which is called model 3rl you know I don't think there is a really a a precise definition of any of this but in some sense the model 3 are I would just say is the negation of this right so you don't explicitly learn a transition probability so so just the the doesn't learn the transition so it sounds a little bit kind of like um um you know if you don't have any contacts you know this sounds might be a little bit tricky because how come you don't learn the Dynamics but still learn the policy so it's possible for example sometimes you can just directly optimize um optimize this without learning the Dynamics right so you can probably um just uh um you use some kind of like a stock so so they are there you know there are ways to to to note um learn expressive dynamics of course you know eventually any algorithm needs to somehow have some understanding about Dynamics internally in some sense but you don't have to expressly build one um right so for example one possible option is just that you optimize this function V pass over over the policy Pi suppose you can somehow take really descent over policy pi then you can um you can avoid dealing with the Dynamics where you don't use the about my equation you just somehow it'd be you just somehow compute the derivative of this respect five so some of the passwords you know are like some some of the algorithms Q learning is one type of algorithm and another type of algorithm is called policy gradients so I don't think we have time to discuss any of this model 3 algorithm I just want to write them down here just the buzzword so that you know if if you happen to kind of like came across them you know they are model 3 algorithm you know in some of the quarters we do cover this some of the course have well two more lectures like depending on how many holidays they are so um okay so and another kind of like a um some of the kind of the definition of terms people use here is that so um there is something called when you say tabular case or tablet RL this means that you have decreased this space discrete uh action and state space so on the other words the size of the state space is finance and and the size of the action space is also finite and this and actually impresses you are in some sense whether it's final or not probably it's not the most important thing the real important thing is that the state and action space are not too big they are not like a billion or something they are something that um that is reasonable like in the last lecture we are we are basically assuming this right we'll assume that the state space is like has an anxious right and and we didn't talk that much about action space but we implicitly say that we actually all the algorithms required at the action space to be somewhat confined and and you can see like this uh for example this linear system solver algorithm if you want to solve the linear system equations what are the variables the variables are the V Pi one up to V Pi n where m is the number of states so if m is infinite then you cannot solve this linear system equations where you have infinite variables even if m is not infinite even if m is something like super big say like exponentially back or something like a billion then you cannot really afford time to solve this system equations so so in some sense the most important thing is that you really don't want to have like if you are a type blockage you are implicitly assuming that the state space is not huge um but you know as you can see you know sometimes the state space is just have to be infinite or very big because you have the state space is continuous right so um so so that's another case where you have continuous continuous database sometimes you have continuous action space as well so so for example the state space is something like Rd so you have a d dimensional Vector to describe the state and your action typically the action space is smaller than a state space well maybe the action space is something like RK where K is smaller than D sometimes D can be like 100 or maybe sometimes even more than 100 action space typically if it's continuous then K problem would be 5 10 or something like that um and sometimes you have the combination right so you have continuous State space but find an action space that's also possible um for example if you for example one table case where you have continuous State space but a final action space is a target game right you play this entire game and you'll stay space is this pixel space where you see the what like the the the the the pixels right so that that is showing to you and the actions but actually are actually fine and you just have a few buttons and maybe like some kind of like handle you can you can you can choose to play um so the action space is somewhat final um all right so so these are just some kind of like terms just in case they are useful uh and when you read some of the the other books or literatures um okay so then in the next um 20 minutes I'm going to discuss you know how do you do the model maybe in the next 30 minutes how do you do the model based RL follow tabular case right so so basically you can see that their model based model 3 and tabular continuous way is kind of like you have two like four combinations and we're going to do the model based Plus tabular so and this is actually not very hard so so what you really want to do here is that um basically model based tabular you want to learn expressive an exclusive model um on that kind of like a somewhat simulator two model PSA so what you have is that you have um a collection so so suppose we have a collection of trajectories and forgot whether I Define trajectory spectrojectors I just mean a sequence of like State actions um so suppose you say you you have some state as zero uh maybe let's say we start from the state as zero and you take some action maybe a0 and you arrive at S1 and you take some action A2 A1 and you arrive at S2 so and so forth right so this is the one introduction right a sequence of State actions right so and so so far I'm not uh I didn't really tell you how I got this reaction but I'm I'm gonna talk about how do you learn the the transition probability from some game introductions right so suppose I give one should actually and so often you need more than one production so then you say after I take a bunch of steps maybe I take two steps I reset so I reset and then I get another traction that's called maybe let's call this should actually ice zero one a01 after super scriptures to indicate that this is the first Factory and then I restart I get a new initial State and I call this as zero two and then apply a0 to S1 to a12 so forth so I got a and maybe I can get more right I can get a bunch of three actors here the subscript is indexing the time and the superscript is indexing which structure you are in how do you ask me ask the transition Dynamics recall that all of these State and actions are discrete variables because I'm in a typical case so so basically I just have to estimate PSA as Prime right what's the chance to start with us and and take action a to an arrive at Express so this is kind of like a in some sense the problem is the the same as the uh some of this like I think we discussed this generative learning algorithm where we have this event model right where everything is kind of content based so um so basically you can you can compute the Max mo likelihood of the of this transition you view this as a parameter right so this is something you want to learn right this is a parameter and and then you try to find out the maximum likelihood estimate for this parameter and it turns out that it's just a as usual it's the most kind of Natural Choice basically you just count on the the frequency to see this right you look you say in the denominator you say I look at how many times uh like a uh we let's say we took action hey I'd stay nice so you basically look at all the cases will take action a and state s and then in the numerator you count how many times we took action a at okay I guess maybe I'm a little too worthy here so so basically a number of times if you take action a and say I State us and you arrive at I Supply so so the denominator is the total number of times you take the action I State us and the numerator is how many among all of this occurrences of ice and a how many times you indeed arrive as S5 and um and then that's that's your transition probability that's that's your estimated transition probability this is the estimate for PSA um right so any questions so far and once you have this it's it's just accounting based algorithm you just count how many fractions you really arrive at S Prime among all the sa how many fractions of those you know really arrive at S Prime and and that's the the empirical frequencies is your estimate for the transition probability yourself okay I'm using the right click I should use the black color okay so and once you have this uh a tool to estimate the transition probability then um you can you can have a model based R algorithm I'll still use the right if it's okay I think the the black one doesn't seem to be very good um so the model bits are algorithm is doing the following so it's pretty intuitive so first of all you initialize uh some policy pi maybe randomly let's say and then um you have some data set initially you have no no data so there's a d is empty I'm just defining a notation basically I'm going to use this data set D some way so and once you have this uh and then what you really do is that you say I'm going to I have two steps the first step is that I'm going to estimate I'm going to collect some data so so collect data by executing policy pi [Music] um so so if you actually policy Pi in a real environment you're going to get some samples right that's that's our assumption our assumption is that you are able to get samples from the Real Environment you don't know the PSA but you can get samples from the pic so so you're actually a policy pie to get your environment so basically what you do is you get a family of the environment of sorry you actually you actually your policy pie to get some samples unless the samples you got let's say they are denoted by S01 um something and you apply a01 and then you get that's one one so and so forth and you have X1 and zero two right this is the same kind of set of samples here so you get some samples and and then you add all of this reaction right on the two lectures the trajectories 2D so D is a kind of like a set of data and then I'm using the bar spacing your you know awkward way how do I I think I'll just I have to so let's say I'm going to so you got this data um this kind of data right so so this is a set of data that is that is kind of like this okay so and then uh you add them okay so on the Second Step this is the first step you collect some data and then in the second step uh your estimate you estimate the the PSA using data indeed Okay so uh and let's still use PSE as the estimator right so suppose you get some PSA um um that is supposed to be estimated for the real transition Dynamics um and then in the step C um so you you can use for example is value iteration um all could it could be also part of situation I guess we didn't have time to discuss pulse iteration in the last lecture but if you are interested you can read lecture notes to see what's the policy iteration but let's suppose to use value iteration to get to get V Star the value function um for the estimated PSA the estimates you know Dynamics so just pretend that the isometrics is the real dynamics and then you solve the best policy um the boss the the optimal value function uh for data Dynamics and then you take Pi star uh to be you take Pi on be the optimal policy for the same for the isometrics foreign Okay so so are we done so it sounds like we're done right because we estimate some it's kind of everything simple right you collect some data you ask me the the the the the Dynamics and then you get the best policy of on the isometrics but actually what you really have to do is you have to take it you have to have another loop outer loop that repeat this process so what you really should need to do is you take have to take a loop and so that after you you have some policy pie you want to collect some more data using the current policy Pi initially the policy Pi was random and you collect data from the random policy and after you get some policy Pi then you should take another loop to collect more data and then we estimate your transition Dynamics and then recompute your policy and then you keep doing this you probably don't have to do a lot of Loops but but you have to do sub iterations so the question is why why you need this Loop uh Auto Loop right why we can't just go with the first Dynamics right we have estimated right so if you ask me one Dynamics you know if it's accurate enough then why we cannot go with that the reason is that there is the in RL there's this problem with the this kind of like so-called exploration exploitation trade-off um which I'm going to elaborate so so the immediate problem is the following so it's possible that in the first round when you collect data your data is not very good it's very bad low quality data right so for example suppose you want to control a robot and and you initialize people pause randomly you you just do some random you just push the random buttons or control the robot in a random way then the data you collect are are basically just some kind of like a on the nobody just kind of like wiggling around a little bit right it doesn't really move much so the data you clock is actually very very bad and then even you have a lot of data you see your data cloud is not good enough just because the the robot doesn't really do much and then your ultimate Dynamics is also not going to be good enough and then um uh your policy is also good not going to be good enough so what you really want is that you want to do this iterative so the next round when your policies is kind of like reasonably okay you collect some higher order higher quality data and then you do this again and then your pores becomes even better and then you get even better higher quality data so so that's why all you want to have this Loop another example you know um is the following so suppose you have this is another example suppose for example um let's say I guess you probably all some of you have used this kind of like a automatic robot to clean the to do the cleaning for the for the house where you have this small um like vacuum that can like a robotic vacuum that you can that can move in your house right so if you haven't used that I think how it works is that it first kind of like explore your whole whole room and to try to figure out what your room looked like and then the next round is it's gonna kind of like do some kind of like a take some introductions to kind of clean your room you know so that it covers every part right so so something like that um however but if you think about this right suppose you have said for example say something like you have a like a a big room and then you have a small room adjacent to it right so you have some robots um that kind of tries to clean the room and navigate through the room right so what if you at the beginning your robot only kind of goes to this part right so you just in the first round your policy just only look at this room then your Dynamic model will only be able to only is only accurate for this room right you only know what's what's happening in this room what's the chairs what's the what's the on the stairs or other kind of like so so far so and so forth right so um and but you don't know anything about this one so so so the so that's why you cannot um so that's the that's a typical situation where the data of the quality of the data is not good enough because your your quality of your data doesn't even cover some part of the room so then if you don't do anything special then your robot wouldn't have incentive to go to this room because the robot doesn't even know the existence of this room in some sense so um so this is actually even more challenging the case because here even you just do this Loop you wouldn't be able to necessarily wouldn't be able to kind of like figure out uh this small room because begin at the beginning you just you just only see the large room and you never look at a small room and then then you figure out of the policy to clean a large room and you still don't know the existence of the small room and you just keep doing this eventually use the only kind of clean the large room and you just never notice the existence of the small room so so in this kind of cases um you need even something more than this kind of algorithm on to on to to be able to to to work so um and typically this is called um this is a phenomenal exploitation versus exploration so exportation basically means that you believe your current transition Dynamics you just strongly believe their current transition Dynamics um or your current kind of understanding about the world in the environment right and you just try to find an optimal policy for that uh for the current understanding of about the world and exploration means that you try to explore different kind of strategies to see whether you missed anything in this world right so in this case you know maybe you missed the small room so so so you want to do exploration to figure out the existence of the small rule and exploitation really just means that you just basically do the best thing for the current map um so so as you can see you know you need some exploration to kind of at least cover the entire kind of like the entire world so you know like the existence of any other options so so typically if you really run this kind of reinforcement learning algorithm you have to add some Randomness in the policy pie so that you can have some exploration right so you don't want to just always in every round you just always collect data from the policy Pi that is optimal for the for the for the current environment you also want to have some extraordinary exploration by adding some Randomness so so basically what you really do is that when you collect data you add some uh by asking file with some randomness so in this case you know you have some small chance to go into the small room so that you can see the small room and then you figure out you should actually kind of clean that small room you know uh with some kind of like a trajectory uh with some actions um so maybe another example is that for example suppose you can figure out where where which restaurant you want to go right in in Palo Alto downtown right so suppose you just um suppose so far you know two restaurants and you know one of them is better than the other for you and and the exploitation would mean that you just always go to the better restaurant for your taste right and then you just keep go to that by the restaurant uh for you however um you may also consider some exploration because you know there are many other restaurants in Palo Alto and you don't know the existence of them even or you don't know whether they are good on or bad you don't know their their taste whether their taste fits you so so exploration means that you should try some of the other restaurants even at a risk that those restaurants are not as good as the one you have known but you want to try more uh with those uh try to kind of explore those restaurants and exploitation means that you just believe that okay there's no uh nothing I should explore anymore I just I believe in the my current evaluation of all the restaurants I just take the best one I need to keep going to that best on that one so um and there's a trade-off because you know if you keep doing exploration then uh you keep trying all different restaurants then you all inevitably you're gonna find some restaurants that are not very good right so and you're going to suffer and you're gonna say okay this doesn't work the money you're gonna you're gonna like have some um bad energy in some sense um but um but uh but on the other hand um if you're only doing exportation you can miss other opportunities so so it really depends on and if you really go into the the RO literature there are different ways to trade off these two things right depending on how confident you are with each of the uh the choices uh you may you may decide to exploit more or you want you might decide to explore more um okay but I think we are not going to have enough time to go into the details there's a huge literature uh here like even just when you talk about this going to restaurant thing right which doesn't have a sequential aspect you just go to the restaurant and and have lunch or dinner there's no sequential decision you still have to think about the decision the exportation and the exploration trade-off so is there kind of like a to do it optimally you have to be on some kind of careful [Music] um okay any any questions so far so like like do we know that we we really we are we are coming back and it's learning about the world we are here and how we are just trying I'm random yeah so so I guess um maybe you're asking about you know what uh how do we do expiration right so what's the kind of the guiding principle for for doing and it seems to suggest that one principle could be that you only want to explore when you can get collect more information uh yes I think that's that's that basically uh it's pretty much like what you said um but sometimes random actually can serve that need so if you just do random perturbation of your current option typically you get some you get a actual good amount of information but of course in some other cases you have to directly go for those kind of um in certain places actually actually you are exactly right so for for the tabular role case typically what people do is not just random exploration what people do is you say you own you take those actions that are most uncertain for you so suppose you have some action that you don't know what outcome would be you have no idea what the outcome would be or you have very little idea you have very huge uncertainty about what outcome would be you try those actions more uh as exploration strategy but for the continuous State space right so I think it turns out that people most most of the algorithm works are kind of like local randomized exploration uh you don't try some crazy option you try in your neighborhood I think that's what I should probably make a lot of sense in many in in those cases where you have so many different actions right so um for example if you if you think about your career planning like like if you really have suppose you like in theory you have really really a lot of actions actions you can try you can instead of being at Stanford you can try to be a professional soccer player right so or you have you can be a musician you can be a many different there could be many things I mean you you are very uncertain about some of those problems right so like I wouldn't know what if I try to be a musician what would happen so um but but I think we have so many actions when you talk about technical videos but typically we have so many actions I think you somehow the algorithms most of the working algorithms tries to explore locally so so for example I'm a student here at Stanford and then I try something somewhat similar maybe going to intern at Google or maybe like maybe trying to try and graduate school or something like that but I wouldn't try something completely kind of different um um you know there's no we don't have like a really a strong Theory to to say exactly what you should do um so so these are just it's a mixture of like some theoretical explanation and some kind of like empirical observations questions okay so I got some um so I'm going to use the rest of the the 40 minutes or 30 35 minutes to talk about continuous State space so I've talked about the model based Arrow right so now that I'm going to talk about model based plus continuous State space um and the the the the idea is pretty much similar uh it's just that you have to um somehow there is a you have to deal with the continuous database you'll see there's some challenge so foreign like at least when I kind of like after I learned it um I feel like it has a lot of things to do with your kind of like life decision as well like of course you can only 100 trust the algorithm like it's not like you should Implement an algorithm in your life decisions um but but there are some kind of like inside theory that is useful for um for the general kind of like this you know this is a you can model your life as a reinforcement algorithms it's just that okay one difference is that um in a real life you have much more information uh like so like in the in the theoretical formulation you're only collecting information from the samples right so you know nothing about the environment at the beginning any anything you have to you have to try it out right like if you want to know anything about PSA you have to try it out um and sometimes you can share more and sometimes you can travel less but you still have to try um in life decisions in many cases you don't have to try you can already predict to some extent uh what's the outcome is um other than that I feel it's pretty similar just just my two cents um anyway so um I guess so so what you do with continuous State space so um so one easy case just to start with is when D is two suppose you have the state space is only Dimension two I think when this in this case basically your state space is a two-dimensional um pen you have like um maybe like a so two two axis and one way to do it is that you just discretize your this state space into discrete variables right so before what each of the state is like two real number but then you say I'm going to discretize the state space there's some boundary of course because your state cannot be too big and then you discretize something like this and then for every cell you see all the states in there is the number of states in this each of the cell but you say all the states in that cell is going to be con treated as one state just because they're all pretty much similar so as long as you have like a fine enough granularity here then you can basically treat every cell as a single state so and suppose you have like granularity up soon then you are going to have one over absolute Square well of absolute choices for the first one well absolute choices for the second one so you have one of absolute Square choices of states um and you can probably take absence to be something like 0.01 I don't know exactly but you you're gonna get a ref at a reasonable number of states maybe maybe quite a lot but not like maybe a thousands or something like ten thousands but but it wouldn't be too too bad so that's the that's easy easy Choice um however this doesn't really work for when your dimension is higher than two I think when dimensions three it already becomes tricky because if you think about you know uh your display is in the three-dimensional space then say suppose these three and you do this round of apps on kind of like uh so each of these um size of each of this cell is absolutely you need one over absolute so the power three states I guess it depends on what option you choose but if you choose absom to be something at point zero one then you have like a hundred to the power three that's like a million which is already a lot right so so maybe sometimes it's still okay but generally when these three is already kind of tricky and then you can see this doesn't scale very well because if you have these for any D this would be one of the absolute power D and and when D is five basically it's kind of and impossible completely impossible so so we need an other approach for but actually just to just to clarify so when these two actually this is actually pretty typically a pretty good idea because it's simple and clean and when you don't have to deal with any other complications the only thing is that you have one for absolute Square um States but there's no any other complications um it's actually a pretty good solution I think you know it probably should work in most of the cases so but when D is more than three um it's going to be a problem so um so basically what we do is we're gonna we're gonna redesign everything that we have discussed you know with continuous State space in mind so so I guess there are two questions one question is how to learn how to learn PSA for continuous State space and then question two is that how do you uh do the value iteration of both for continuous database so um so for question one so let's discuss each of them so uh and the basic idea is that you try to kind of like extend what you have done to the continuous State space in some way um so regarding question one so how do you do it the first question probably is that how do you even represent the PSA right so now you have infinite number of us here so before for every ice and a you have a vector to represent right and the the right so forever I say this PS is a distribution and it's basically the vector over I'm possible choices if I miss number of states but now you have infinite of s here or maybe exponential number of ads there and for every PSA this Vector is actually a very high dimensional Vector maybe a exponential dimensional vector or infinimental vector so how do you even represent this so um the idea is that you can change the way to represent it you don't represent this uh uh you you you can do the following there are many ways but this is one way that is probably common learning on Dynamics foreign so what you do is you first say I'm going to model this process as prime example from PSA by assuming as Prime is equals to F Asa plus some noise this is one option not not all not the only option but one option is you say you assume that ice Prime is computed by applying some functional ion a and then add some noise that's my that's my way to sample as Prime from this two solution so this is a way to define a distribution the distribution basically has mean FSA and some gaussian has some variance you know um like a cosine like like the same as the cosine so so this will give you a random variable S Prime given as an a so and this FSA let's say maybe maybe f as a is deterministic this is just some function we want to learn and this part is the noise I'll give you the stochasticity or maybe like a Kasai is maybe it's from some zero with some covalent Sigma so sometimes you just I guess probably have seen I've seen that this is kind of almost the same as what we do with um like uh surprise learning I will just treat this as a super streaming problem so S Prime is my label as an a are my inputs so I'm just trying to predict my label from the inputs and the way I model this is I model this by some kind of like some function plus noise right and then you can do Max molecular Hood and the max molecular hood is just the the square loss so and you learn that you learn this model f by some Square loss so for example so so you have to introduce some parametrization for us so for example F could be a linear model suppose you say you have some parameter A and B and your f it could be just a s plus b either way right so this could be the F which is parametricized by A and B um so A and B are parameters s and there are inputs so you just have this linear model uh I think this is called linear dynamical model so this is a linear model and another option is that you say I'm going to say FSA is equals to uh maybe you can you can make you can use the the same idea as the feature uh like the when you do the kernel you can say this is like something like a times V of us Plus B times I don't know I forgot what is this is called This is called you just have some other feature something like this so both of this two fees these are some some features right so this is um like what we did with kernel method right so you introduce some features and then you are linear in a feature space of course you can also say that on FSA um so I guess here department is a and b as well uh you can also say my FSA if you have parametric specimens Theta is a new network neural network applied on ice and a um something like this and this new network is maybe say parameter as bad title so so you just say s and a are concatenated as inputs of a network and you apply this network with parameter Theta and the output will be my FSA and in each of this case you can model your S Prime like this and then you can try to uh find out the uh and this becomes a surprise problem so what I mean is that once you have Department transition right so then you can say you can the learning loss function is that suppose you have some data suppose data are something like I guess I've written this several times um S01 a01 is 1 1 so on and so forth so you have a bunch of trajectories sorry my massive index so you have these three actually and then you you you break these structures into uh three tuples so you mean that you view this you on the eyes a collection of three tuples right so you say you have I zero one a01 comma S11 so this is the first three things here and you view this as the input and this as the so-called label or output and then you say oh I'm going to have for these three things which is ice one one a uh one one and S12 right so this again is the input and this is a label and you do this for every three typos you know in every introduction so you get the basically you get a sequence of like three tuples um so you guys eventually you guys you know as say p minus one and is the number of triggers uh 80 minus one and I guess the a lot of indexing here but basically just the view every three concatenate three consecutive kind of like numbers as a tuple and you view the first as an AST the input and the the outcome is the output um something like this okay so you have a data side of this right so this is a set of size n times t a is the number of two actually T is the length of the trajectory um and then you do a supervised learning so you have you just use the surprise learning so you do some kind of regression let's say you say I'm going to minimize over my parameter maybe let's call it Theta and I minimize this over I minimize the laws over all data points so what is the loss the loss so there is a i there's also a t I is indexing on the num the trajectories and T's in the second induction timestamp and every time you want to predict what you want to predict as t plus one in the iot directory using this function f Theta as t i t i so this is the input of this Supply saving problem you apply the model and then you try to match it with the um with the output a label of this problem and here I'm using a L2 Norm because the label is a vector right so in many of the cases you see this parenthesis is square because in those cases when you do the typical super string problem your label is a real number right house price prediction right so the label is the house price you know which is a real number but here the label is a vector it's the same you just to do the L2 knob score sometimes you can change the loss function you don't have to always use L2 num Square maybe you don't even need Square sometimes so you can use other thoughts I want some touch so um and once you do this you get the Theta and this I will say that will be on model any questions so so I think the benefit here is that before you have to specify PSA for every ice and a right as a vector right so now you you learn this function as Theta so the number of parameters is Theta and then for every ice and a you can compute FSA um if you know the Theta so that's the that's how you do the model based RL part uh sorry that's how you do the model estimation part how you ask me the Dynamics and then uh you also need to deal with the value or iteration how do you do the value iteration for continuous database space uh I don't think I have the the math there so when you do the value iteration you know before you are trying to kind of like update the value function for every state or every time right so not that's not possible anymore because you have so many states you cannot update the value function for every state actually it's not even clear how do you even uh how do you even describe a value function because before the way you describe a value of function is you say for every V Star as for every result I have a number right so but before you describe it by listing all the ads right for every us you have a number that's how you describe V Star but now you cannot really describe it like that so what you do is you say I'm going to parametrize the value function by again kind of liking before by some kind of like new network or linear model right so you can say I parameterize I write my vrs as on um like something like um maybe one choice is that you can say this is say that transpose times some feature of s so this is a feature and this is the parameter that's my option and another option is you say my V of us is a new network with parameter Theta applied on a state s right so this data is a description of of the value function and you need to learn of course in the other ways to do this for example one way to do it is that you can um for example design some right uh the right features using physical intuitions sometimes you know that some coordinates you know only is only meaningful when it combines with other currents in some meaningful way so so I think some Mitch of this could also work because you could design some features and then use these features as input to a new network um but generally you just want to have a parametress form okay so that so we have done so so after we have the representation of the value function and the next question is how do you do the update right so before what we did was that so recall the update was something like v as is other to be something like RS Plus gamma Max a all right that's what we did right so we come we have some working value for V and we compute the right hand side of the development equation and we update the V with the right hand side and then we repeat so um but you cannot do this because you don't have before we do it for every eyes but that's not possible anymore so so what we do is we try to make this is true for the eyes we have seen so basically for continuous State space so for the continuous case we just try to uh Ensure about my equation for States uh that we have seen you don't you show this for every state as anymore you just ensure it for the states you have seen and if you if you just want to do that then you can do some kind of loss function to ensure that so um let me elaborate on what what do I mean here so so you first the first step is the following so your estimate the right hand side of the Bowman equation four um first States say S1 up to s so I haven't told you exactly how I got these states right so suppose I got some states that I have seen right in like in the algorithm and I want to only ensure the spell my equation you turn this to be equal to this or someone encouraged it they are the same only for this kind of choice of States s is equal to one of this that's my compromise because I cannot do it for every state so um so what I do is that I I try to First Complete the right hand side so how do I get the right hand side for every state s so what I do is I just um so so basically I want to compute RSI plus this Max thing but the problem is that you know here I have a sum over S Prime again that's again a lot of different choices for us so what I do is I'm going to first turn this into uh an expectation we write this as expectation of v as Prime and X Prime is sampled from p s i hey so after you have the expectation you can use the empirical sample to estimate this quantity so basically what you do is you say so for every by the way I think I forgot to mention one thing which is that now I'm talking about um I'm talking about uh continuous uh State space but fine and action space just so this is a slightly simpler question than both of them are continues um or just because I don't want to overconfect too much so so the con so it's continuous State space and final action space so um what I do is I first estimate this by sampling some X Prime and then I take the max because you have a final number of states action space actions you can just take the max so what you do is you say for every a uh you get K samples uh let's call it S1 Prime up to s k Prime these are sampled from this transition Dynamics PSA PS uh s i right so you understand as I you try the action a and you see what is the outcome you have a bunch of random outcome um and then you still Define q a to be um RSI plus 1 over k times this average right you use the empirical average basically right the empirical average of PV ice SJ Prime so this is supposed to estimate this is supposed to estimate RS uh I think I'm missing a gamma here sorry it's supposed to ask me RS Plus RSI plus gamma expectation v s i so I'm supposed to use this to estimate this without a max and then I take the max I call y I to be the max of this QA so then this is supposed to ask me this one so y i is supposed to estimate this quantity and for every eye I have estimate right so for every eye I have estimated for the right hand side of this bioma equation and then I ensure that this development equation to be somewhat correct for all the eyes um that I have considered so step so step B I need to enforce um like v as I to be close to y i right because you know recall that we have spent so much effort to complete this y i it's why it's supposed to be the right side of the bowel make right hand side of the biome equation evaluate at is I and I want this to be close to the left hand side and how do I do that I do that by saying I'm Computing Theta to be Arc Max of this loss function that encourages this and the loss function is simply just some L2 loss say I take the average over all the possible ice I's and let's say v um my vsi is parametrized by some saying right work like this way so let's don't talk about the right works but it works for anything right so say this is a neural network you're not with Theta applied on x i minus y i Square so basically you want this new network to Output the right value function that match the target the target is the right hand side of the bioma equation actually you know in the theoretical in the in the RL papers you do call this target so this is the target you want to kind of match and and you want to choose the the updated such that your left hand side which is this new network on SI to match the target and that's your data that you got in this step and again you have to do it's a value iteration you have to iterate because if you get stated this way you have to iterate to recompute your your right hand side and then after the left hand side where you call it when you do development the value iteration you complete the right hand side and you up to the right hand side you do this iteratively so um So eventually you also have to have iteration which is you say you let's see so you each between these two steps so you say you have a loop um but I think there's a little bit to that specify everything yes I think I specify okay so this is the loop maybe maybe sorry let's uh okay yes so um I'm a little bit I should save some space before so let's say this is a and I have a loop here right so but this is not still not enough because this Loop is doing what this Loop is only doing the value iteration for this Iceman after s n right I haven't told you even how do you get asthma after I sent right this is just a bunch of states that you have seen so and also even you you like you can update you can do this for every us or value iteration is not enough because our model based algorithm has something even up on the outer layer of the value iteration right so you're asking the model and you do value iteration and then you ask the model again you do value iteration now you have to do another outer loop to update your uh it's your samples right so because when you do the model based algorithm you either have a loop outside the value iteration so this is the look for Value iteration and then we have another loop outside it um which is supposed to do the uh to iteratively update the model so basically you have another look and in this Loop you collect some data um say S1 up to SN you collect this data from current policy right you collect the data and then you do the value iteration on this data and then on and then you collect it again and you do value iteration so this is how it imitates the model based our algorithm for Tableau case right because for the typical case we also have this Auto Loop where you you collect data and you do value iteration and you connected you to evaluate iteration so okay so I think I'm missing even more more step you collect data you collect data and also you need to also in this also Loop you also have to estimate Dynamics collect data my bad so collect data Maybe one and two you estimate Dynamics and then three you do the value tuition and the value iteration consists of another loop which is this um any questions I see some confusing faces so so basically I think I erased that oh actually it was part of party here right so this was the part that we deal with the tablet case right so we have to the the two steps I think maybe I should actually rename them I just want two and three and there was a one step that that was erased which is the the first stack which is the clock data right so before when you do the tabular case you repeat between the three you alternate between the three steps collect data asthma Dynamics validation and now for the uh for the uh The Continuous State space number one is do the same number two was done by this part of the board where where we learn this you know with the supervised learning and number three is done by here this part of the board where we do this A and B stuff the A and B to kind of like uh to do the value iteration so so number three is the is this part where we do the value iteration and and eventually you have to do a loop for this one two three on top of this one two three okay I guess that's all for today and for this whole course I guess you know we have covered you know surprise learning and surprise learning and reinforcement learning um yeah and we try to kind of like cover more and more deep learning uh these days but um but also like some of these maths are included mostly because I think these are um kind of the foundations of machine learning so um I hope you um you had some fun with the course thanks foreign"
